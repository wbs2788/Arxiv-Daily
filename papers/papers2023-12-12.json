[
    {
        "title": "Photorealistic Video Generation with Diffusion Models",
        "authors": [
            "Agrim Gupta",
            "Lijun Yu",
            "Kihyuk Sohn",
            "Xiuye Gu",
            "Meera Hahn",
            "Li Fei-Fei",
            "Irfan Essa",
            "Lu Jiang",
            "José Lezama"
        ],
        "abstract": "We present W.A.L.T, a transformer-based approach for photorealistic video\ngeneration via diffusion modeling. Our approach has two key design decisions.\nFirst, we use a causal encoder to jointly compress images and videos within a\nunified latent space, enabling training and generation across modalities.\nSecond, for memory and training efficiency, we use a window attention\narchitecture tailored for joint spatial and spatiotemporal generative modeling.\nTaken together these design decisions enable us to achieve state-of-the-art\nperformance on established video (UCF-101 and Kinetics-600) and image\n(ImageNet) generation benchmarks without using classifier free guidance.\nFinally, we also train a cascade of three models for the task of text-to-video\ngeneration consisting of a base latent video diffusion model, and two video\nsuper-resolution diffusion models to generate videos of $512 \\times 896$\nresolution at $8$ frames per second.",
        "date": "2023-12-11T18:59:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06662v1"
    },
    {
        "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior",
        "authors": [
            "Fangfu Liu",
            "Diankun Wu",
            "Yi Wei",
            "Yongming Rao",
            "Yueqi Duan"
        ],
        "abstract": "Recently, 3D content creation from text prompts has demonstrated remarkable\nprogress by utilizing 2D and 3D diffusion models. While 3D diffusion models\nensure great multi-view consistency, their ability to generate high-quality and\ndiverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion\nmodels find a distillation approach that achieves excellent generalization and\nrich details without any 3D data. However, 2D lifting methods suffer from\ninherent view-agnostic ambiguity thereby leading to serious multi-face Janus\nissues, where text prompts fail to provide sufficient guidance to learn\ncoherent 3D results. Instead of retraining a costly viewpoint-aware model, we\nstudy how to fully exploit easily accessible coarse 3D knowledge to enhance the\nprompts and guide 2D lifting optimization for refinement. In this paper, we\npropose Sherpa3D, a new text-to-3D framework that achieves high-fidelity,\ngeneralizability, and geometric consistency simultaneously. Specifically, we\ndesign a pair of guiding strategies derived from the coarse 3D prior generated\nby the 3D diffusion model: a structural guidance for geometric fidelity and a\nsemantic guidance for 3D coherence. Employing the two types of guidance, the 2D\ndiffusion model enriches the 3D content with diversified and high-quality\nresults. Extensive experiments show the superiority of our Sherpa3D over the\nstate-of-the-art text-to-3D methods in terms of quality and 3D consistency.",
        "date": "2023-12-11T18:59:18+00:00",
        "link": "http://arxiv.org/pdf/2312.06655v1"
    },
    {
        "title": "LightSim: Neural Lighting Simulation for Urban Scenes",
        "authors": [
            "Ava Pun",
            "Gary Sun",
            "Jingkang Wang",
            "Yun Chen",
            "Ze Yang",
            "Sivabalan Manivasagam",
            "Wei-Chiu Ma",
            "Raquel Urtasun"
        ],
        "abstract": "Different outdoor illumination conditions drastically alter the appearance of\nurban scenes, and they can harm the performance of image-based robot perception\nsystems if not seen during training. Camera simulation provides a\ncost-effective solution to create a large dataset of images captured under\ndifferent lighting conditions. Towards this goal, we propose LightSim, a neural\nlighting camera simulation system that enables diverse, realistic, and\ncontrollable data generation. LightSim automatically builds lighting-aware\ndigital twins at scale from collected raw sensor data and decomposes the scene\ninto dynamic actors and static background with accurate geometry, appearance,\nand estimated scene lighting. These digital twins enable actor insertion,\nmodification, removal, and rendering from a new viewpoint, all in a\nlighting-aware manner. LightSim then combines physically-based and learnable\ndeferred rendering to perform realistic relighting of modified scenes, such as\naltering the sun location and modifying the shadows or changing the sun\nbrightness, producing spatially- and temporally-consistent camera videos. Our\nexperiments show that LightSim generates more realistic relighting results than\nprior work. Importantly, training perception models on data generated by\nLightSim can significantly improve their performance.",
        "date": "2023-12-11T18:59:13+00:00",
        "link": "http://arxiv.org/pdf/2312.06654v1"
    },
    {
        "title": "4M: Massively Multimodal Masked Modeling",
        "authors": [
            "David Mizrahi",
            "Roman Bachmann",
            "Oğuzhan Fatih Kar",
            "Teresa Yeo",
            "Mingfei Gao",
            "Afshin Dehghan",
            "Amir Zamir"
        ],
        "abstract": "Current machine learning models for vision are often highly specialized and\nlimited to a single modality and task. In contrast, recent large language\nmodels exhibit a wide range of capabilities, hinting at a possibility for\nsimilarly versatile models in computer vision. In this paper, we take a step in\nthis direction and propose a multimodal training scheme called 4M. It consists\nof training a single unified Transformer encoder-decoder using a masked\nmodeling objective across a wide range of input/output modalities - including\ntext, images, geometric, and semantic modalities, as well as neural network\nfeature maps. 4M achieves scalability by unifying the representation space of\nall modalities through mapping them into discrete tokens and performing\nmultimodal masked modeling on a small randomized subset of tokens.\n  4M leads to models that exhibit several key capabilities: (1) they can\nperform a diverse set of vision tasks out of the box, (2) they excel when\nfine-tuned for unseen downstream tasks or new input modalities, and (3) they\ncan function as a generative model that can be conditioned on arbitrary\nmodalities, enabling a wide variety of expressive multimodal editing\ncapabilities with remarkable flexibility.\n  Through experimental analyses, we demonstrate the potential of 4M for\ntraining versatile and scalable foundation models for vision tasks, setting the\nstage for further exploration in multimodal learning for vision and other\ndomains.",
        "date": "2023-12-11T18:57:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06647v1"
    },
    {
        "title": "Online Decision Making with History-Average Dependent Costs (Extended)",
        "authors": [
            "Vijeth Hebbar",
            "Cedric Langbort"
        ],
        "abstract": "In many online sequential decision-making scenarios, a learner's choices\naffect not just their current costs but also the future ones. In this work, we\nlook at one particular case of such a situation where the costs depend on the\ntime average of past decisions over a history horizon. We first recast this\nproblem with history dependent costs as a problem of decision making under\nstage-wise constraints. To tackle this, we then propose the novel\nFollow-The-Adaptively-Regularized-Leader (FTARL) algorithm. Our innovative\nalgorithm incorporates adaptive regularizers that depend explicitly on past\ndecisions, allowing us to enforce stage-wise constraints while simultaneously\nenabling us to establish tight regret bounds. We also discuss the implications\nof the length of history horizon on design of no-regret algorithms for our\nproblem and present impossibility results when it is the full learning horizon.",
        "date": "2023-12-11T18:55:03+00:00",
        "link": "http://arxiv.org/pdf/2312.06641v1"
    },
    {
        "title": "Harmonic Mobile Manipulation",
        "authors": [
            "Ruihan Yang",
            "Yejin Kim",
            "Aniruddha Kembhavi",
            "Xiaolong Wang",
            "Kiana Ehsani"
        ],
        "abstract": "Recent advancements in robotics have enabled robots to navigate complex\nscenes or manipulate diverse objects independently. However, robots are still\nimpotent in many household tasks requiring coordinated behaviors such as\nopening doors. The factorization of navigation and manipulation, while\neffective for some tasks, fails in scenarios requiring coordinated actions. To\naddress this challenge, we introduce, HarmonicMM, an end-to-end learning method\nthat optimizes both navigation and manipulation, showing notable improvement\nover existing techniques in everyday tasks. This approach is validated in\nsimulated and real-world environments and adapts to novel unseen settings\nwithout additional tuning. Our contributions include a new benchmark for mobile\nmanipulation and the successful deployment in a real unseen apartment,\ndemonstrating the potential for practical indoor robot deployment in daily\nlife. More results are on our project site:\nhttps://rchalyang.github.io/HarmonicMM/",
        "date": "2023-12-11T18:54:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06639v1"
    },
    {
        "title": "SurvBeNIM: The Beran-Based Neural Importance Model for Explaining the Survival Models",
        "authors": [
            "Lev V. Utkin",
            "Danila Y. Eremenko",
            "Andrei V. Konstantinov"
        ],
        "abstract": "A new method called the Survival Beran-based Neural Importance Model\n(SurvBeNIM) is proposed. It aims to explain predictions of machine learning\nsurvival models, which are in the form of survival or cumulative hazard\nfunctions. The main idea behind SurvBeNIM is to extend the Beran estimator by\nincorporating the importance functions into its kernels and by implementing\nthese importance functions as a set of neural networks which are jointly\ntrained in an end-to-end manner. Two strategies of using and training the whole\nneural network implementing SurvBeNIM are proposed. The first one explains a\nsingle instance, and the neural network is trained for each explained instance.\nAccording to the second strategy, the neural network only learns once on all\ninstances from the dataset and on all generated instances. Then the neural\nnetwork is used to explain any instance in a dataset domain. Various numerical\nexperiments compare the method with different existing explanation methods. A\ncode implementing the proposed method is publicly available.",
        "date": "2023-12-11T18:54:26+00:00",
        "link": "http://arxiv.org/pdf/2312.06638v1"
    },
    {
        "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
        "authors": [
            "Songlin Yang",
            "Bailin Wang",
            "Yikang Shen",
            "Rameswar Panda",
            "Yoon Kim"
        ],
        "abstract": "Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear (with respect to output length) inference\ncomplexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM\n(Qin et al., 2023a) observe that adding a global decay term to the additive RNN\nupdate rule greatly improves performance, sometimes outperforming standard\nTransformers with softmax attention when trained at scale. In this work we show\nthat adding a data-dependent gating mechanism further improves performance. We\nderive a parallel form of this gated linear attention layer that enables\nefficient training. However, a straightforward, numerically stable\nimplementation of this parallel form requires generalized matrix\nmultiplications in log-space for numerical stability, and thus cannot take\nadvantage of tensor cores on modern GPUs which are optimized for standard\nmatrix multiplications. We develop a hardware-efficient version of the parallel\nform that can still make use of tensor cores through block-parallel\ncomputations over sequence chunks. Experiments on moderate-scale language\nmodeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models\ntrained on 100B tokens) show that gated linear attention (GLA) Transformers\nperform competitively against a strong LLaMA-architecture Transformer baseline\n(Touvron et al., 2023) as well as Mamba (Gu & Dao, 2023), a recently introduced\nstate-space model with a data-dependent state transition mechanism. For\ntraining speed, our Triton-based implementation performs comparably to\nCUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training\nlength setting, while outperforming FlashAttention-2 when training on longer\nsequences beyond 4096.",
        "date": "2023-12-11T18:51:59+00:00",
        "link": "http://arxiv.org/pdf/2312.06635v1"
    },
    {
        "title": "Information theory for model reduction in stochastic dynamical systems",
        "authors": [
            "Matthew S. Schmitt",
            "Maciej Koch-Janusz",
            "Michel Fruchart",
            "Daniel S. Seara",
            "Vincenzo Vitelli"
        ],
        "abstract": "Model reduction is the construction of simple yet predictive descriptions of\nthe dynamics of many-body systems in terms of a few relevant variables. A\nprerequisite to model reduction is the identification of these relevant\nvariables, a task for which no general method exists. Here, we develop a\nsystematic approach based on the information bottleneck to identify the\nrelevant variables, defined as those most predictive of the future. We\nelucidate analytically the relation between these relevant variables and the\neigenfunctions of the transfer operator describing the dynamics. Further, we\nshow that in the limit of high compression, the relevant variables are directly\ndetermined by the slowest-decaying eigenfunctions. Our information-based\napproach indicates when to optimally stop increasing the complexity of the\nreduced model. Further, it provides a firm foundation to construct\ninterpretable deep learning tools that perform model reduction. We illustrate\nhow these tools work on benchmark dynamical systems and deploy them on\nuncurated datasets, such as satellite movies of atmospheric flows downloaded\ndirectly from YouTube.",
        "date": "2023-12-11T18:39:05+00:00",
        "link": "http://arxiv.org/pdf/2312.06608v1"
    },
    {
        "title": "Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops",
        "authors": [
            "Aditya Prakash",
            "Arjun Gupta",
            "Saurabh Gupta"
        ],
        "abstract": "Objects undergo varying amounts of perspective distortion as they move across\na camera's field of view. Models for predicting 3D from a single image often\nwork with crops around the object of interest and ignore the location of the\nobject in the camera's field of view. We note that ignoring this location\ninformation further exaggerates the inherent ambiguity in making 3D inferences\nfrom 2D images and can prevent models from even fitting to the training data.\nTo mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding\n(KPE), which incorporates information about the location of crops in the image\nand camera intrinsics. Experiments on three popular 3D-from-a-single-image\nbenchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes,\nand predicting 3D shapes of articulated objects on ARCTIC, show the benefits of\nKPE.",
        "date": "2023-12-11T18:28:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06594v1"
    },
    {
        "title": "Concurrent Density Estimation with Wasserstein Autoencoders: Some Statistical Insights",
        "authors": [
            "Anish Chakrabarty",
            "Arkaprabha Basu",
            "Swagatam Das"
        ],
        "abstract": "Variational Autoencoders (VAEs) have been a pioneering force in the realm of\ndeep generative models. Amongst its legions of progenies, Wasserstein\nAutoencoders (WAEs) stand out in particular due to the dual offering of\nheightened generative quality and a strong theoretical backbone. WAEs consist\nof an encoding and a decoding network forming a bottleneck with the prime\nobjective of generating new samples resembling the ones it was catered to. In\nthe process, they aim to achieve a target latent representation of the encoded\ndata. Our work is an attempt to offer a theoretical understanding of the\nmachinery behind WAEs. From a statistical viewpoint, we pose the problem as\nconcurrent density estimation tasks based on neural network-induced\ntransformations. This allows us to establish deterministic upper bounds on the\nrealized errors WAEs commit. We also analyze the propagation of these\nstochastic errors in the presence of adversaries. As a result, both the large\nsample properties of the reconstructed distribution and the resilience of WAE\nmodels are explored.",
        "date": "2023-12-11T18:27:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06591v1"
    },
    {
        "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models",
        "authors": [
            "Avi Singh",
            "John D. Co-Reyes",
            "Rishabh Agarwal",
            "Ankesh Anand",
            "Piyush Patil",
            "Peter J. Liu",
            "James Harrison",
            "Jaehoon Lee",
            "Kelvin Xu",
            "Aaron Parisi",
            "Abhishek Kumar",
            "Alex Alemi",
            "Alex Rizkowsky",
            "Azade Nova",
            "Ben Adlam",
            "Bernd Bohnet",
            "Hanie Sedghi",
            "Igor Mordatch",
            "Isabelle Simpson",
            "Izzeddin Gur",
            "Jasper Snoek",
            "Jeffrey Pennington",
            "Jiri Hron",
            "Kathleen Kenealy",
            "Kevin Swersky",
            "Kshiteej Mahajan",
            "Laura Culp",
            "Lechao Xiao",
            "Maxwell L. Bileschi",
            "Noah Constant",
            "Roman Novak",
            "Rosanne Liu",
            "Tris Warkentin",
            "Yundi Qian",
            "Ethan Dyer",
            "Behnam Neyshabur",
            "Jascha Sohl-Dickstein",
            "Noah Fiedel"
        ],
        "abstract": "Fine-tuning language models~(LMs) on human-generated data remains a prevalent\npractice. However, the performance of such models is often limited by the\nquantity and diversity of high-quality human data. In this paper, we explore\nwhether we can go beyond human data on tasks where we have access to scalar\nfeedback, for example, on math problems where one can verify correctness. To do\nso, we investigate a simple self-training method based on\nexpectation-maximization, which we call ReST$^{EM}$, where we (1) generate\nsamples from the model and filter them using binary feedback, (2) fine-tune the\nmodel on these samples, and (3) repeat this process a few times. Testing on\nadvanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find\nthat ReST$^{EM}$ scales favorably with model size and significantly surpasses\nfine-tuning only on human data. Overall, our findings suggest self-training\nwith feedback can substantially reduce dependence on human-generated data.",
        "date": "2023-12-11T18:17:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06585v1"
    },
    {
        "title": "3D Hand Pose Estimation in Egocentric Images in the Wild",
        "authors": [
            "Aditya Prakash",
            "Ruisen Tu",
            "Matthew Chang",
            "Saurabh Gupta"
        ],
        "abstract": "We present WildHands, a method for 3D hand pose estimation in egocentric\nimages in the wild. This is challenging due to (a) lack of 3D hand pose\nannotations for images in the wild, and (b) a form of perspective\ndistortion-induced shape ambiguity that arises in the analysis of crops around\nhands. For the former, we use auxiliary supervision on in-the-wild data in the\nform of segmentation masks & grasp labels in addition to 3D supervision\navailable in lab datasets. For the latter, we provide spatial cues about the\nlocation of the hand crop in the camera's field of view. Our approach achieves\nthe best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, a\npopular and robust approach for estimating hand pose in the wild, by 45.3% when\nevaluated on 2D hand pose on our EPIC-HandKps dataset.",
        "date": "2023-12-11T18:15:47+00:00",
        "link": "http://arxiv.org/pdf/2312.06583v1"
    },
    {
        "title": "Grokking Group Multiplication with Cosets",
        "authors": [
            "Dashiell Stander",
            "Qinan Yu",
            "Honglu Fan",
            "Stella Biderman"
        ],
        "abstract": "We use the group Fourier transform over the symmetric group $S_n$ to reverse\nengineer a 1-layer feedforward network that has \"grokked\" the multiplication of\n$S_5$ and $S_6$. Each model discovers the true subgroup structure of the full\ngroup and converges on circuits that decompose the group multiplication into\nthe multiplication of the group's conjugate subgroups. We demonstrate the value\nof using the symmetries of the data and models to understand their mechanisms\nand hold up the ``coset circuit'' that the model uses as a fascinating example\nof the way neural networks implement computations. We also draw attention to\ncurrent challenges in conducting mechanistic interpretability research by\ncomparing our work to Chughtai et al. [6] which alleges to find a different\nalgorithm for this same problem.",
        "date": "2023-12-11T18:12:18+00:00",
        "link": "http://arxiv.org/pdf/2312.06581v1"
    },
    {
        "title": "Multi-class Support Vector Machine with Maximizing Minimum Margin",
        "authors": [
            "Feiping Nie",
            "Zhezheng Hao",
            "Rong Wang"
        ],
        "abstract": "Support Vector Machine (SVM) stands out as a prominent machine learning\ntechnique widely applied in practical pattern recognition tasks. It achieves\nbinary classification by maximizing the \"margin\", which represents the minimum\ndistance between instances and the decision boundary. Although many efforts\nhave been dedicated to expanding SVM for multi-class case through strategies\nsuch as one versus one and one versus the rest, satisfactory solutions remain\nto be developed. In this paper, we propose a novel method for multi-class SVM\nthat incorporates pairwise class loss considerations and maximizes the minimum\nmargin. Adhering to this concept, we embrace a new formulation that imparts\nheightened flexibility to multi-class SVM. Furthermore, the correlations\nbetween the proposed method and multiple forms of multi-class SVM are analyzed.\nThe proposed regularizer, akin to the concept of \"margin\", can serve as a\nseamless enhancement over the softmax in deep learning, providing guidance for\nnetwork parameter learning. Empirical evaluations demonstrate the effectiveness\nand superiority of our proposed method over existing multi-classification\nmethods.Code is available at https://github.com/zz-haooo/M3SVM.",
        "date": "2023-12-11T18:09:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06578v1"
    },
    {
        "title": "HyPE-GT: where Graph Transformers meet Hyperbolic Positional Encodings",
        "authors": [
            "Kushal Bose",
            "Swagatam Das"
        ],
        "abstract": "Graph Transformers (GTs) facilitate the comprehension of graph-structured\ndata by calculating the self-attention of node pairs without considering node\nposition information. To address this limitation, we introduce an innovative\nand efficient framework that introduces Positional Encodings (PEs) into the\nTransformer, generating a set of learnable positional encodings in the\nhyperbolic space, a non-Euclidean domain. This approach empowers us to explore\ndiverse options for optimal selection of PEs for specific downstream tasks,\nleveraging hyperbolic neural networks or hyperbolic graph convolutional\nnetworks. Additionally, we repurpose these positional encodings to mitigate the\nimpact of over-smoothing in deep Graph Neural Networks (GNNs). Comprehensive\nexperiments on molecular benchmark datasets, co-author, and co-purchase\nnetworks substantiate the effectiveness of hyperbolic positional encodings in\nenhancing the performance of deep GNNs.",
        "date": "2023-12-11T18:00:27+00:00",
        "link": "http://arxiv.org/pdf/2312.06576v1"
    },
    {
        "title": "Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets",
        "authors": [
            "Subhajit Dutta Chowdhury",
            "Zhiyu Ni",
            "Qingyuan Peng",
            "Souvik Kundu",
            "Pierluigi Nuzzo"
        ],
        "abstract": "Graph Lottery Tickets (GLTs), comprising a sparse adjacency matrix and a\nsparse graph neural network (GNN), can significantly reduce the inference\nlatency and compute footprint compared to their dense counterparts. Despite\nthese benefits, their performance against adversarial structure perturbations\nremains to be fully explored. In this work, we first investigate the resilience\nof GLTs against different structure perturbation attacks and observe that they\nare highly vulnerable and show a large drop in classification accuracy. Based\non this observation, we then present an adversarially robust graph\nsparsification (ARGS) framework that prunes the adjacency matrix and the GNN\nweights by optimizing a novel loss function capturing the graph homophily\nproperty and information associated with both the true labels of the train\nnodes and the pseudo labels of the test nodes. By iteratively applying ARGS to\nprune both the perturbed graph adjacency matrix and the GNN model weights, we\ncan find adversarially robust graph lottery tickets that are highly sparse yet\nachieve competitive performance under different untargeted training-time\nstructure attacks. Evaluations conducted on various benchmarks, considering\ndifferent poisoning structure attacks, namely, PGD, MetaAttack, Meta-PGD, and\nPR-BCD demonstrate that the GLTs generated by ARGS can significantly improve\nthe robustness, even when subjected to high levels of sparsity.",
        "date": "2023-12-11T17:52:46+00:00",
        "link": "http://arxiv.org/pdf/2312.06568v1"
    },
    {
        "title": "Promoting Counterfactual Robustness through Diversity",
        "authors": [
            "Francesco Leofante",
            "Nico Potyka"
        ],
        "abstract": "Counterfactual explanations shed light on the decisions of black-box models\nby explaining how an input can be altered to obtain a favourable decision from\nthe model (e.g., when a loan application has been rejected). However, as noted\nrecently, counterfactual explainers may lack robustness in the sense that a\nminor change in the input can cause a major change in the explanation. This can\ncause confusion on the user side and open the door for adversarial attacks. In\nthis paper, we study some sources of non-robustness. While there are\nfundamental reasons for why an explainer that returns a single counterfactual\ncannot be robust in all instances, we show that some interesting robustness\nguarantees can be given by reporting multiple rather than a single\ncounterfactual. Unfortunately, the number of counterfactuals that need to be\nreported for the theoretical guarantees to hold can be prohibitively large. We\ntherefore propose an approximation algorithm that uses a diversity criterion to\nselect a feasible number of most relevant explanations and study its robustness\nempirically. Our experiments indicate that our method improves the\nstate-of-the-art in generating robust explanations, while maintaining other\ndesirable properties and providing competitive computational performance.",
        "date": "2023-12-11T17:49:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06564v1"
    },
    {
        "title": "On Meta-Prompting",
        "authors": [
            "Adrian de Wynter",
            "Xun Wang",
            "Qilong Gu",
            "Si-Qing Chen"
        ],
        "abstract": "Certain statistical models are capable of interpreting input strings as\ninstructions, or prompts, and carry out tasks based on them. Many approaches to\nprompting and pre-training these models involve the automated generation of\nthese prompts. We call these approaches meta-prompting, or prompting to obtain\nprompts. We propose a theoretical framework based on category theory to\ngeneralize and describe them. This framework is flexible enough to account for\nLLM stochasticity; and allows us to obtain formal results around task\nagnosticity and equivalence of various meta-prompting approaches. We experiment\nwith meta-prompting in two active areas of model research: creativity and\nideation. We find that user preference favors (p < 0.01) the prompts generated\nunder meta-prompting, as well as their corresponding outputs, over a series of\nhardcoded baseline prompts that include the original task prompt. Using our\nframework, we argue that meta-prompting is more effective than basic prompting\nat generating desirable outputs.",
        "date": "2023-12-11T17:46:44+00:00",
        "link": "http://arxiv.org/pdf/2312.06562v1"
    },
    {
        "title": "Automatic Regularization for Linear MMSE Filters",
        "authors": [
            "Daniel Gomes de Pinho Zanco",
            "Leszek Szczecinski",
            "Jacob Benesty"
        ],
        "abstract": "In this work, we consider the problem of regularization in minimum\nmean-squared error (MMSE) linear filters. Exploiting the relationship with\nstatistical machine learning methods, the regularization parameter is found\nfrom the observed signals in a simple and automatic manner. The proposed\napproach is illustrated through system identification examples, where the\nautomatic regularization yields near-optimal results.",
        "date": "2023-12-11T17:45:10+00:00",
        "link": "http://arxiv.org/pdf/2312.06560v1"
    },
    {
        "title": "Robust Graph Neural Network based on Graph Denoising",
        "authors": [
            "Victor M. Tenorio",
            "Samuel Rey",
            "Antonio G. Marques"
        ],
        "abstract": "Graph Neural Networks (GNNs) have emerged as a notorious alternative to\naddress learning problems dealing with non-Euclidean datasets. However,\nalthough most works assume that the graph is perfectly known, the observed\ntopology is prone to errors stemming from observational noise, graph-learning\nlimitations, or adversarial attacks. If ignored, these perturbations may\ndrastically hinder the performance of GNNs. To address this limitation, this\nwork proposes a robust implementation of GNNs that explicitly accounts for the\npresence of perturbations in the observed topology. For any task involving\nGNNs, our core idea is to i) solve an optimization problem not only over the\nlearnable parameters of the GNN but also over the true graph, and ii) augment\nthe fitting cost with a term accounting for discrepancies on the graph.\nSpecifically, we consider a convolutional GNN based on graph filters and follow\nan alternating optimization approach to handle the (non-differentiable and\nconstrained) optimization problem by combining gradient descent and projected\nproximal updates. The resulting algorithm is not limited to a particular type\nof graph and is amenable to incorporating prior information about the\nperturbations. Finally, we assess the performance of the proposed method\nthrough several numerical experiments.",
        "date": "2023-12-11T17:43:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06557v1"
    },
    {
        "title": "LLM360: Towards Fully Transparent Open-Source LLMs",
        "authors": [
            "Zhengzhong Liu",
            "Aurick Qiao",
            "Willie Neiswanger",
            "Hongyi Wang",
            "Bowen Tan",
            "Tianhua Tao",
            "Junbo Li",
            "Yuqi Wang",
            "Suqi Sun",
            "Omkar Pangarkar",
            "Richard Fan",
            "Yi Gu",
            "Victor Miller",
            "Yonghao Zhuang",
            "Guowei He",
            "Haonan Li",
            "Fajri Koto",
            "Liping Tang",
            "Nikhil Ranjan",
            "Zhiqiang Shen",
            "Xuguang Ren",
            "Roberto Iriondo",
            "Cun Mu",
            "Zhiting Hu",
            "Mark Schulze",
            "Preslav Nakov",
            "Tim Baldwin",
            "Eric P. Xing"
        ],
        "abstract": "The recent surge in open-source Large Language Models (LLMs), such as LLaMA,\nFalcon, and Mistral, provides diverse options for AI practitioners and\nresearchers. However, most LLMs have only released partial artifacts, such as\nthe final model weights or inference code, and technical reports increasingly\nlimit their scope to high-level design choices and surface statistics. These\nchoices hinder progress in the field by degrading transparency into the\ntraining of LLMs and forcing teams to rediscover many details in the training\nprocess. We present LLM360, an initiative to fully open-source LLMs, which\nadvocates for all training code and data, model checkpoints, and intermediate\nresults to be made available to the community. The goal of LLM360 is to support\nopen and collaborative AI research by making the end-to-end LLM training\nprocess transparent and reproducible by everyone. As a first step of LLM360, we\nrelease two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder,\nincluding their training code, data, intermediate checkpoints, and analyses (at\nhttps://www.llm360.ai). We are committed to continually pushing the boundaries\nof LLMs through this open-source effort. More large-scale and stronger models\nare underway and will be released in the future.",
        "date": "2023-12-11T17:39:00+00:00",
        "link": "http://arxiv.org/pdf/2312.06550v1"
    },
    {
        "title": "KF-PLS: Optimizing Kernel Partial Least-Squares (K-PLS) with Kernel Flows",
        "authors": [
            "Zina-Sabrina Duma",
            "Jouni Susiluoto",
            "Otto Lamminpää",
            "Tuomas Sihvonen",
            "Satu-Pia Reinikainen",
            "Heikki Haario"
        ],
        "abstract": "Partial Least-Squares (PLS) Regression is a widely used tool in chemometrics\nfor performing multivariate regression. PLS is a bi-linear method that has a\nlimited capacity of modelling non-linear relations between the predictor\nvariables and the response. Kernel PLS (K-PLS) has been introduced for\nmodelling non-linear predictor-response relations. In K-PLS, the input data is\nmapped via a kernel function to a Reproducing Kernel Hilbert space (RKH), where\nthe dependencies between the response and the input matrix are assumed to be\nlinear. K-PLS is performed in the RKH space between the kernel matrix and the\ndependent variable. Most available studies use fixed kernel parameters. Only a\nfew studies have been conducted on optimizing the kernel parameters for K-PLS.\nIn this article, we propose a methodology for the kernel function optimization\nbased on Kernel Flows (KF), a technique developed for Gaussian process\nregression (GPR). The results are illustrated with four case studies. The case\nstudies represent both numerical examples and real data used in classification\nand regression tasks. K-PLS optimized with KF, called KF-PLS in this study, is\nshown to yield good results in all illustrated scenarios. The paper presents\ncross-validation studies and hyperparameter analysis of the KF methodology when\napplied to K-PLS.",
        "date": "2023-12-11T17:32:36+00:00",
        "link": "http://arxiv.org/pdf/2312.06547v1"
    },
    {
        "title": "Uncertainty quantification in automated valuation models with locally weighted conformal prediction",
        "authors": [
            "Anders Hjort",
            "Gudmund Horn Hermansen",
            "Johan Pensar",
            "Jonathan P. Williams"
        ],
        "abstract": "Non-parametric machine learning models, such as random forests and gradient\nboosted trees, are frequently used to estimate house prices due to their\npredictive accuracy, but such methods are often limited in their ability to\nquantify prediction uncertainty. Conformal Prediction (CP) is a model-agnostic\nframework for constructing confidence sets around machine learning prediction\nmodels with minimal assumptions. However, due to the spatial dependencies\nobserved in house prices, direct application of CP leads to confidence sets\nthat are not calibrated everywhere, i.e., too large of confidence sets in\ncertain geographical regions and too small in others. We survey various\napproaches to adjust the CP confidence set to account for this and demonstrate\ntheir performance on a data set from the housing market in Oslo, Norway. Our\nfindings indicate that calibrating the confidence sets on a \\textit{locally\nweighted} version of the non-conformity scores makes the coverage more\nconsistently calibrated in different geographical regions. We also perform a\nsimulation study on synthetically generated sale prices to empirically explore\nthe performance of CP on housing market data under idealized conditions with\nknown data-generating mechanisms.",
        "date": "2023-12-11T17:09:12+00:00",
        "link": "http://arxiv.org/pdf/2312.06531v1"
    },
    {
        "title": "Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context",
        "authors": [
            "Xiang Cheng",
            "Yuxin Chen",
            "Suvrit Sra"
        ],
        "abstract": "Many neural network architectures have been shown to be Turing Complete, and\ncan thus implement arbitrary algorithms. However, Transformers are unique in\nthat they can implement gradient-based learning algorithms \\emph{under simple\nparameter configurations}. A line of recent work shows that linear Transformers\nnaturally learn to implement gradient descent (GD) when trained on a linear\nregression in-context learning task. But the linearity assumption (either in\nthe Transformer architecture or in the learning task) is far from realistic\nsettings where non-linear activations crucially enable Transformers to learn\ncomplicated non-linear functions. In this paper, we provide theoretical and\nempirical evidence that non-linear Transformers can, and \\emph{in fact do},\nlearn to implement learning algorithms to learn non-linear functions in\ncontext. Our results apply to a broad class of combinations of non-linear\narchitectures, and non-linear in-context learning tasks. Interestingly, we show\nthat the optimal choice of non-linear activation depends in a natural way on\nthe non-linearity of the learning task.",
        "date": "2023-12-11T17:05:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06528v1"
    },
    {
        "title": "Label Smoothing for Enhanced Text Sentiment Classification",
        "authors": [
            "Yijie Gao",
            "Shijing Si"
        ],
        "abstract": "Label smoothing is a widely used technique in various domains, such as image\nclassification and speech recognition, known for effectively combating model\noverfitting. However, there is few research on its application to text\nsentiment classification. To fill in the gap, this study investigates the\nimplementation of label smoothing for sentiment classification by utilizing\ndifferent levels of smoothing. The primary objective is to enhance sentiment\nclassification accuracy by transforming discrete labels into smoothed label\ndistributions. Through extensive experiments, we demonstrate the superior\nperformance of label smoothing in text sentiment classification tasks across\neight diverse datasets and deep learning architectures: TextCNN, BERT, and\nRoBERTa, under two learning schemes: training from scratch and fine-tuning.",
        "date": "2023-12-11T17:00:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06522v1"
    },
    {
        "title": "A GAN Approach for Node Embedding in Heterogeneous Graphs Using Subgraph Sampling",
        "authors": [
            "Hung Chun Hsu",
            "Bo-Jun Wu",
            "Ming-Yi Hong",
            "Che Lin",
            "Chih-Yu Wang"
        ],
        "abstract": "Our research addresses class imbalance issues in heterogeneous graphs using\ngraph neural networks (GNNs). We propose a novel method combining the strengths\nof Generative Adversarial Networks (GANs) with GNNs, creating synthetic nodes\nand edges that effectively balance the dataset. This approach directly targets\nand rectifies imbalances at the data level. The proposed framework resolves\nissues such as neglecting graph structures during data generation and creating\nsynthetic structures usable with GNN-based classifiers in downstream tasks. It\nprocesses node and edge information concurrently, improving edge balance\nthrough node augmentation and subgraph sampling. Additionally, our framework\nintegrates a threshold strategy, aiding in determining optimal edge thresholds\nduring training without time-consuming parameter adjustments. Experiments on\nthe Amazon and Yelp Review datasets highlight the effectiveness of the\nframework we proposed, especially in minority node identification, where it\nconsistently outperforms baseline models across key performance metrics,\ndemonstrating its potential in the field.",
        "date": "2023-12-11T16:52:20+00:00",
        "link": "http://arxiv.org/pdf/2312.06519v1"
    },
    {
        "title": "Decoupling Meta-Reinforcement Learning with Gaussian Task Contexts and Skills",
        "authors": [
            "Hongcai He",
            "Anjie Zhu",
            "Shuang Liang",
            "Feiyu Chen",
            "Jie Shao"
        ],
        "abstract": "Offline meta-reinforcement learning (meta-RL) methods, which adapt to unseen\ntarget tasks with prior experience, are essential in robot control tasks.\nCurrent methods typically utilize task contexts and skills as prior experience,\nwhere task contexts are related to the information within each task and skills\nrepresent a set of temporally extended actions for solving subtasks. However,\nthese methods still suffer from limited performance when adapting to unseen\ntarget tasks, mainly because the learned prior experience lacks generalization,\ni.e., they are unable to extract effective prior experience from meta-training\ntasks by exploration and learning of continuous latent spaces. We propose a\nframework called decoupled meta-reinforcement learning (DCMRL), which (1)\ncontrastively restricts the learning of task contexts through pulling in\nsimilar task contexts within the same task and pushing away different task\ncontexts of different tasks, and (2) utilizes a Gaussian quantization\nvariational autoencoder (GQ-VAE) for clustering the Gaussian distributions of\nthe task contexts and skills respectively, and decoupling the exploration and\nlearning processes of their spaces. These cluster centers which serve as\nrepresentative and discrete distributions of task context and skill are stored\nin task context codebook and skill codebook, respectively. DCMRL can acquire\ngeneralizable prior experience and achieve effective adaptation to unseen\ntarget tasks during the meta-testing phase. Experiments in the navigation and\nrobot manipulation continuous control tasks show that DCMRL is more effective\nthan previous meta-RL methods with more generalizable prior experience.",
        "date": "2023-12-11T16:50:14+00:00",
        "link": "http://arxiv.org/pdf/2312.06518v1"
    },
    {
        "title": "Asynchronous Distributed Optimization with Delay-free Parameters",
        "authors": [
            "Xuyang Wu",
            "Changxin Liu",
            "Sindri Magnusson",
            "Mikael Johansson"
        ],
        "abstract": "Existing asynchronous distributed optimization algorithms often use\ndiminishing step-sizes that cause slow practical convergence, or use fixed\nstep-sizes that depend on and decrease with an upper bound of the delays. Not\nonly are such delay bounds hard to obtain in advance, but they also tend to be\nlarge and rarely attained, resulting in unnecessarily slow convergence. This\npaper develops asynchronous versions of two distributed algorithms, Prox-DGD\nand DGD-ATC, for solving consensus optimization problems over undirected\nnetworks. In contrast to alternatives, our algorithms can converge to the fixed\npoint set of their synchronous counterparts using step-sizes that are\nindependent of the delays. We establish convergence guarantees for strongly and\nweakly convex problems under both partial and total asynchrony. We also show\nthat the convergence speed of the two asynchronous methods adapts to the actual\nlevel of asynchrony rather than being constrained by the worst-case. Numerical\nexperiments demonstrate a strong practical performance of our asynchronous\nalgorithms.",
        "date": "2023-12-11T16:33:38+00:00",
        "link": "http://arxiv.org/pdf/2312.06508v1"
    },
    {
        "title": "Aligning brain functions boosts the decoding of visual semantics in novel subjects",
        "authors": [
            "Alexis Thual",
            "Yohann Benchetrit",
            "Felix Geilert",
            "Jérémy Rapin",
            "Iurii Makarov",
            "Hubert Banville",
            "Jean-Rémi King"
        ],
        "abstract": "Deep learning is leading to major advances in the realm of brain decoding\nfrom functional Magnetic Resonance Imaging (fMRI). However, the large\ninter-subject variability in brain characteristics has limited most studies to\ntrain models on one subject at a time. Consequently, this approach hampers the\ntraining of deep learning models, which typically requires very large datasets.\nHere, we propose to boost brain decoding by aligning brain responses to videos\nand static images across subjects. Compared to the anatomically-aligned\nbaseline, our method improves out-of-subject decoding performance by up to 75%.\nMoreover, it also outperforms classical single-subject approaches when fewer\nthan 100 minutes of data is available for the tested subject. Furthermore, we\npropose a new multi-subject alignment method, which obtains comparable results\nto that of classical single-subject approaches while improving out-of-subject\ngeneralization. Finally, we show that this method aligns neural representations\nin accordance with brain anatomy. Overall, this study lays the foundations for\nleveraging extensive neuroimaging datasets and enhancing the decoding of\nindividuals with a limited amount of brain recordings.",
        "date": "2023-12-11T15:55:20+00:00",
        "link": "http://arxiv.org/pdf/2312.06467v1"
    },
    {
        "title": "Variational Auto-Encoder Based Deep Learning Technique For Filling Gaps in Reacting PIV Data",
        "authors": [
            "Shashank Yellapantula"
        ],
        "abstract": "In this study, a deep learning based conditional density estimation technique\nknown as conditional variational auto-encoder (CVAE) is used to fill gaps\ntypically observed in particle image velocimetry (PIV) measurements in\ncombustion systems. The proposed CVAE technique is trained using time resolved\ngappy PIV fields, typically observed in industrially relevant combustors.\nStereo-PIV (SPIV) data from a swirl combustor with very a high vector yield is\nused to showcase the accuracy of the proposed CVAE technique. Various error\nmetrics evaluated on the reconstructed velocity field in the gaps are presented\nfrom data sets corresponding to three sets of combustor operating conditions.\nIn addition to accurate data reproduction, the proposed CVAE technique offers\ndata compression by reducing the latent space dimension, enabling the efficient\nprocessing of large-scale PIV data.",
        "date": "2023-12-11T15:50:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06461v1"
    },
    {
        "title": "Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images",
        "authors": [
            "Bao Li",
            "Zhenyu Liu",
            "Lizhi Shao",
            "Bensheng Qiu",
            "Hong Bu",
            "Jie Tian"
        ],
        "abstract": "Directly predicting human epidermal growth factor receptor 2 (HER2) status\nfrom widely available hematoxylin and eosin (HE)-stained whole slide images\n(WSIs) can reduce technical costs and expedite treatment selection. Accurately\npredicting HER2 requires large collections of multi-site WSIs. Federated\nlearning enables collaborative training of these WSIs without gigabyte-size\nWSIs transportation and data privacy concerns. However, federated learning\nencounters challenges in addressing label imbalance in multi-site WSIs from the\nreal world. Moreover, existing WSI classification methods cannot simultaneously\nexploit local context information and long-range dependencies in the site-end\nfeature representation of federated learning. To address these issues, we\npresent a point transformer with federated learning for multi-site HER2 status\nprediction from HE-stained WSIs. Our approach incorporates two novel designs.\nWe propose a dynamic label distribution strategy and an auxiliary classifier,\nwhich helps to establish a well-initialized model and mitigate label\ndistribution variations across sites. Additionally, we propose a farthest\ncosine sampling based on cosine distance. It can sample the most distinctive\nfeatures and capture the long-range dependencies. Extensive experiments and\nanalysis show that our method achieves state-of-the-art performance at four\nsites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can\ngeneralize to two unseen sites with 229 WSIs.",
        "date": "2023-12-11T15:41:05+00:00",
        "link": "http://arxiv.org/pdf/2312.06454v1"
    },
    {
        "title": "Experimental demonstration of a robust training method for strongly defective neuromorphic hardware",
        "authors": [
            "William A. Borders",
            "Advait Madhavan",
            "Matthew W. Daniels",
            "Vasileia Georgiou",
            "Martin Lueker-Boden",
            "Tiffany S. Santos",
            "Patrick M. Braganca",
            "Mark D. Stiles",
            "Jabez J. McClelland",
            "Brian D. Hoskins"
        ],
        "abstract": "The increasing scale of neural networks needed to support more complex\napplications has led to an increasing requirement for area- and\nenergy-efficient hardware. One route to meeting the budget for these\napplications is to circumvent the von Neumann bottleneck by performing\ncomputation in or near memory. An inevitability of transferring neural networks\nonto hardware is that non-idealities such as device-to-device variations or\npoor device yield impact performance. Methods such as hardware-aware training,\nwhere substrate non-idealities are incorporated during network training, are\none way to recover performance at the cost of solution generality. In this\nwork, we demonstrate inference on hardware neural networks consisting of 20,000\nmagnetic tunnel junction arrays integrated on a complementary\nmetal-oxide-semiconductor chips that closely resembles market-ready spin\ntransfer-torque magnetoresistive random access memory technology. Using 36\ndies, each containing a crossbar array with its own non-idealities, we show\nthat even a small number of defects in physically mapped networks significantly\ndegrades the performance of networks trained without defects and show that, at\nthe cost of generality, hardware-aware training accounting for specific defects\non each die can recover to comparable performance with ideal networks. We then\ndemonstrate a robust training method that extends hardware-aware training to\nstatistics-aware training, producing network weights that perform well on most\ndefective dies regardless of their specific defect locations. When evaluated on\nthe 36 physical dies, statistics-aware trained solutions can achieve a mean\nmisclassification error on the MNIST dataset that differs from the\nsoftware-baseline by only 2 %. This statistics-aware training method could be\ngeneralized to networks with many layers that are mapped to hardware suited for\nindustry-ready applications.",
        "date": "2023-12-11T15:28:47+00:00",
        "link": "http://arxiv.org/pdf/2312.06446v1"
    },
    {
        "title": "Revisiting Graph-based Fraud Detection in Sight of Heterophily and Spectrum",
        "authors": [
            "Fan Xu",
            "Nan Wang",
            "Hao Wu",
            "Xuezhi Wen",
            "Xibin Zhao"
        ],
        "abstract": "Graph-based fraud detection (GFD) can be regarded as a challenging\nsemi-supervised node binary classification task. In recent years, Graph Neural\nNetworks(GNN) have been widely applied to GFD, characterizing the anomalous\npossibility of a node by aggregating neighbor information. However, fraud\ngraphs are inherently heterophilic, thus most of GNNs perform poorly due to\ntheir assumption of homophily. In addition, due to the existence of heterophily\nand class imbalance problem, the existing models do not fully utilize the\nprecious node label information. To address the above issues, this paper\nproposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector\nincludes a hybrid filtering module and a local environmental constraint module,\nthe two modules are utilized to solve heterophily and label utilization problem\nrespectively. The first module starts from the perspective of the spectral\ndomain, and solves the heterophily problem to a certain extent. Specifically,\nit divides the spectrum into multiple mixed frequency bands according to the\ncorrelation between spectrum energy distribution and heterophily. Then in order\nto make full use of the node label information, a local environmental\nconstraint module is adaptively designed. The comprehensive experimental\nresults on four real-world fraud detection datasets show that SEC-GFD\noutperforms other competitive graph-based fraud detectors.",
        "date": "2023-12-11T15:18:51+00:00",
        "link": "http://arxiv.org/pdf/2312.06441v1"
    },
    {
        "title": "Towards A Flexible Accuracy-Oriented Deep Learning Module Inference Latency Prediction Framework for Adaptive Optimization Algorithms",
        "authors": [
            "Jingran Shen",
            "Nikos Tziritas",
            "Georgios Theodoropoulos"
        ],
        "abstract": "With the rapid development of Deep Learning, more and more applications on\nthe cloud and edge tend to utilize large DNN (Deep Neural Network) models for\nimproved task execution efficiency as well as decision-making quality. Due to\nmemory constraints, models are commonly optimized using compression, pruning,\nand partitioning algorithms to become deployable onto resource-constrained\ndevices. As the conditions in the computational platform change dynamically,\nthe deployed optimization algorithms should accordingly adapt their solutions.\nTo perform frequent evaluations of these solutions in a timely fashion, RMs\n(Regression Models) are commonly trained to predict the relevant solution\nquality metrics, such as the resulted DNN module inference latency, which is\nthe focus of this paper. Existing prediction frameworks specify different RM\ntraining workflows, but none of them allow flexible configurations of the input\nparameters (e.g., batch size, device utilization rate) and of the selected RMs\nfor different modules. In this paper, a deep learning module inference latency\nprediction framework is proposed, which i) hosts a set of customizable input\nparameters to train multiple different RMs per DNN module (e.g., convolutional\nlayer) with self-generated datasets, and ii) automatically selects a set of\ntrained RMs leading to the highest possible overall prediction accuracy, while\nkeeping the prediction time / space consumption as low as possible.\nFurthermore, a new RM, namely MEDN (Multi-task Encoder-Decoder Network), is\nproposed as an alternative solution. Comprehensive experiment results show that\nMEDN is fast and lightweight, and capable of achieving the highest overall\nprediction accuracy and R-squared value. The Time/Space-efficient\nAuto-selection algorithm also manages to improve the overall accuracy by 2.5%\nand R-squared by 0.39%, compared to the MEDN single-selection scheme.",
        "date": "2023-12-11T15:15:48+00:00",
        "link": "http://arxiv.org/pdf/2312.06440v1"
    },
    {
        "title": "Reward Certification for Policy Smoothed Reinforcement Learning",
        "authors": [
            "Ronghui Mu",
            "Leandro Soriano Marcolino",
            "Tianle Zhang",
            "Yanghao Zhang",
            "Xiaowei Huang",
            "Wenjie Ruan"
        ],
        "abstract": "Reinforcement Learning (RL) has achieved remarkable success in\nsafety-critical areas, but it can be weakened by adversarial attacks. Recent\nstudies have introduced \"smoothed policies\" in order to enhance its robustness.\nYet, it is still challenging to establish a provable guarantee to certify the\nbound of its total reward. Prior methods relied primarily on computing bounds\nusing Lipschitz continuity or calculating the probability of cumulative reward\nabove specific thresholds. However, these techniques are only suited for\ncontinuous perturbations on the RL agent's observations and are restricted to\nperturbations bounded by the l_2-norm. To address these limitations, this paper\nproposes a general black-box certification method capable of directly\ncertifying the cumulative reward of the smoothed policy under various\n$l_p$-norm bounded perturbations. Furthermore, we extend our methodology to\ncertify perturbations on action spaces. Our approach leverages f-divergence to\nmeasure the distinction between the original distribution and the perturbed\ndistribution, subsequently determining the certification bound by solving a\nconvex optimisation problem. We provide a comprehensive theoretical analysis\nand run sufficient experiments in multiple environments. Our results show that\nour method not only improves the certified lower bound of mean cumulative\nreward but also demonstrates better efficiency than state-of-the-art\ntechniques.",
        "date": "2023-12-11T15:07:58+00:00",
        "link": "http://arxiv.org/pdf/2312.06436v1"
    },
    {
        "title": "VisionTraj: A Noise-Robust Trajectory Recovery Framework based on Large-scale Camera Network",
        "authors": [
            "Zhishuai Li",
            "Ziyue Li",
            "Xiaoru Hu",
            "Guoqing Du",
            "Yunhao Nie",
            "Feng Zhu",
            "Lei Bai",
            "Rui Zhao"
        ],
        "abstract": "Trajectory recovery based on the snapshots from the city-wide multi-camera\nnetwork facilitates urban mobility sensing and driveway optimization. The\nstate-of-the-art solutions devoted to such a vision-based scheme typically\nincorporate predefined rules or unsupervised iterative feedback, struggling\nwith multi-fold challenges such as lack of open-source datasets for training\nthe whole pipeline, and the vulnerability to the noises from visual inputs. In\nresponse to the dilemma, this paper proposes VisionTraj, the first\nlearning-based model that reconstructs vehicle trajectories from snapshots\nrecorded by road network cameras. Coupled with it, we elaborate on two rational\nvision-trajectory datasets, which produce extensive trajectory data along with\ncorresponding visual snapshots, enabling supervised vision-trajectory interplay\nextraction. Following the data creation, based on the results from the\noff-the-shelf multi-modal vehicle clustering, we first re-formulate the\ntrajectory recovery problem as a generative task and introduce the canonical\nTransformer as the autoregressive backbone. Then, to identify clustering noises\n(e.g., false positives) with the bound on the snapshots' spatiotemporal\ndependencies, a GCN-based soft-denoising module is conducted based on the fine-\nand coarse-grained Re-ID clusters. Additionally, we harness strong semantic\ninformation extracted from the tracklet to provide detailed insights into the\nvehicle's entry and exit actions during trajectory recovery. The denoising and\ntracklet components can also act as plug-and-play modules to boost baselines.\nExperimental results on the two hand-crafted datasets show that the proposed\nVisionTraj achieves a maximum +11.5% improvement against the sub-best model.",
        "date": "2023-12-11T14:52:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06428v1"
    },
    {
        "title": "MalPurifier: Enhancing Android Malware Detection with Adversarial Purification against Evasion Attacks",
        "authors": [
            "Yuyang Zhou",
            "Guang Cheng",
            "Zongyao Chen",
            "Shui Yu"
        ],
        "abstract": "Machine learning (ML) has gained significant adoption in Android malware\ndetection to address the escalating threats posed by the rapid proliferation of\nmalware attacks. However, recent studies have revealed the inherent\nvulnerabilities of ML-based detection systems to evasion attacks. While efforts\nhave been made to address this critical issue, many of the existing defensive\nmethods encounter challenges such as lower effectiveness or reduced\ngeneralization capabilities. In this paper, we introduce a novel Android\nmalware detection method, MalPurifier, which exploits adversarial purification\nto eliminate perturbations independently, resulting in attack mitigation in a\nlight and flexible way. Specifically, MalPurifier employs a Denoising\nAutoEncoder (DAE)-based purification model to preprocess input samples,\nremoving potential perturbations from them and then leading to correct\nclassification. To enhance defense effectiveness, we propose a diversified\nadversarial perturbation mechanism that strengthens the purification model\nagainst different manipulations from various evasion attacks. We also\nincorporate randomized \"protective noises\" onto benign samples to prevent\nexcessive purification. Furthermore, we customize a loss function for improving\nthe DAE model, combining reconstruction loss and prediction loss, to enhance\nfeature representation learning, resulting in accurate reconstruction and\nclassification. Experimental results on two Android malware datasets\ndemonstrate that MalPurifier outperforms the state-of-the-art defenses, and it\nsignificantly strengthens the vulnerable malware detector against 37 evasion\nattacks, achieving accuracies over 90.91%. Notably, MalPurifier demonstrates\neasy scalability to other detectors, offering flexibility and robustness in its\nimplementation.",
        "date": "2023-12-11T14:48:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06423v1"
    },
    {
        "title": "DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics",
        "authors": [
            "Zhiao Huang",
            "Feng Chen",
            "Yewen Pu",
            "Chunru Lin",
            "Hao Su",
            "Chuang Gan"
        ],
        "abstract": "Combining gradient-based trajectory optimization with differentiable physics\nsimulation is an efficient technique for solving soft-body manipulation\nproblems. Using a well-crafted optimization objective, the solver can quickly\nconverge onto a valid trajectory. However, writing the appropriate objective\nfunctions requires expert knowledge, making it difficult to collect a large set\nof naturalistic problems from non-expert users. We introduce DiffVL, a method\nthat enables non-expert users to communicate soft-body manipulation tasks -- a\ncombination of vision and natural language, given in multiple stages -- that\ncan be readily leveraged by a differential physics solver. We have developed\nGUI tools that enable non-expert users to specify 100 tasks inspired by\nreal-life soft-body manipulations from online videos, which we'll make public.\nWe leverage large language models to translate task descriptions into\nmachine-interpretable optimization objectives. The optimization objectives can\nhelp differentiable physics solvers to solve these long-horizon multistage\ntasks that are challenging for previous baselines.",
        "date": "2023-12-11T14:29:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06408v1"
    },
    {
        "title": "Debiased Machine Learning and Network Cohesion for Doubly-Robust Differential Reward Models in Contextual Bandits",
        "authors": [
            "Easton K. Huch",
            "Jieru Shi",
            "Madeline R. Abbott",
            "Jessica R. Golbus",
            "Alexander Moreno",
            "Walter H. Dempsey"
        ],
        "abstract": "A common approach to learning mobile health (mHealth) intervention policies\nis linear Thompson sampling. Two desirable mHealth policy features are (1)\npooling information across individuals and time and (2) incorporating a\ntime-varying baseline reward. Previous approaches pooled information across\nindividuals but not time, failing to capture trends in treatment effects over\ntime. In addition, these approaches did not explicitly model the baseline\nreward, which limited the ability to precisely estimate the parameters in the\ndifferential reward model. In this paper, we propose a novel Thompson sampling\nalgorithm, termed ''DML-TS-NNR'' that leverages (1) nearest-neighbors to\nefficiently pool information on the differential reward function across users\nand time and (2) the Double Machine Learning (DML) framework to explicitly\nmodel baseline rewards and stay agnostic to the supervised learning algorithms\nused. By explicitly modeling baseline rewards, we obtain smaller confidence\nsets for the differential reward parameters. We offer theoretical guarantees on\nthe pseudo-regret, which are supported by empirical results. Importantly, the\nDML-TS-NNR algorithm demonstrates robustness to potential misspecifications in\nthe baseline reward model.",
        "date": "2023-12-11T14:24:24+00:00",
        "link": "http://arxiv.org/pdf/2312.06403v1"
    },
    {
        "title": "DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers",
        "authors": [
            "Aaron Mir",
            "Eduardo Alonso",
            "Esther Mondragón"
        ],
        "abstract": "We propose a novel talking head synthesis pipeline called \"DiT-Head\", which\nis based on diffusion transformers and uses audio as a condition to drive the\ndenoising process of a diffusion model. Our method is scalable and can\ngeneralise to multiple identities while producing high-quality results. We\ntrain and evaluate our proposed approach and compare it against existing\nmethods of talking head synthesis. We show that our model can compete with\nthese methods in terms of visual quality and lip-sync accuracy. Our results\nhighlight the potential of our proposed approach to be used for a wide range of\napplications, including virtual assistants, entertainment, and education. For a\nvideo demonstration of the results and our user study, please refer to our\nsupplementary material.",
        "date": "2023-12-11T14:09:56+00:00",
        "link": "http://arxiv.org/pdf/2312.06400v1"
    },
    {
        "title": "NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos",
        "authors": [
            "Jinxi Li",
            "Ziyang Song",
            "Bo Yang"
        ],
        "abstract": "In this paper, we aim to model 3D scene dynamics from multi-view videos.\nUnlike the majority of existing works which usually focus on the common task of\nnovel view synthesis within the training time period, we propose to\nsimultaneously learn the geometry, appearance, and physical velocity of 3D\nscenes only from video frames, such that multiple desirable applications can be\nsupported, including future frame extrapolation, unsupervised 3D semantic scene\ndecomposition, and dynamic motion transfer. Our method consists of three major\ncomponents, 1) the keyframe dynamic radiance field, 2) the interframe velocity\nfield, and 3) a joint keyframe and interframe optimization module which is the\ncore of our framework to effectively train both networks. To validate our\nmethod, we further introduce two dynamic 3D datasets: 1) Dynamic Object\ndataset, and 2) Dynamic Indoor Scene dataset. We conduct extensive experiments\non multiple datasets, demonstrating the superior performance of our method over\nall baselines, particularly in the critical tasks of future frame extrapolation\nand unsupervised 3D semantic scene decomposition.",
        "date": "2023-12-11T14:07:31+00:00",
        "link": "http://arxiv.org/pdf/2312.06398v1"
    },
    {
        "title": "ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation",
        "authors": [
            "Cédric Rommel",
            "Victor Letzelter",
            "Nermin Samet",
            "Renaud Marlet",
            "Matthieu Cord",
            "Patrick Pérez",
            "Eduardo Valle"
        ],
        "abstract": "Monocular 3D human pose estimation (3D-HPE) is an inherently ambiguous task,\nas a 2D pose in an image might originate from different possible 3D poses. Yet,\nmost 3D-HPE methods rely on regression models, which assume a one-to-one\nmapping between inputs and outputs. In this work, we provide theoretical and\nempirical evidence that, because of this ambiguity, common regression models\nare bound to predict topologically inconsistent poses, and that traditional\nevaluation metrics, such as the MPJPE, P-MPJPE and PCK, are insufficient to\nassess this aspect. As a solution, we propose ManiPose, a novel\nmanifold-constrained multi-hypothesis model capable of proposing multiple\ncandidate 3D poses for each 2D input, together with their corresponding\nplausibility. Unlike previous multi-hypothesis approaches, our solution is\ncompletely supervised and does not rely on complex generative models, thus\ngreatly facilitating its training and usage. Furthermore, by constraining our\nmodel to lie within the human pose manifold, we can guarantee the consistency\nof all hypothetical poses predicted with our approach, which was not possible\nin previous works. We illustrate the usefulness of ManiPose in a synthetic\n1D-to-2D lifting setting and demonstrate on real-world datasets that it\noutperforms state-of-the-art models in pose consistency by a large margin,\nwhile still reaching competitive MPJPE performance.",
        "date": "2023-12-11T13:50:10+00:00",
        "link": "http://arxiv.org/pdf/2312.06386v1"
    },
    {
        "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
        "authors": [
            "Tao Chen",
            "Enwei Zhang",
            "Yuting Gao",
            "Ke Li",
            "Xing Sun",
            "Yan Zhang",
            "Hui Li"
        ],
        "abstract": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input.",
        "date": "2023-12-11T13:11:04+00:00",
        "link": "http://arxiv.org/pdf/2312.06363v1"
    },
    {
        "title": "Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes",
        "authors": [
            "Zhen Qin",
            "Daoyuan Chen",
            "Bingchen Qian",
            "Bolin Ding",
            "Yaliang Li",
            "Shuiguang Deng"
        ],
        "abstract": "Pre-trained large language models (LLMs) require fine-tuning to improve their\nresponsiveness to natural language instructions. Federated learning (FL) offers\na way to perform fine-tuning using the abundant data on end devices without\ncompromising data privacy. Most existing federated fine-tuning methods for LLMs\nrely on parameter-efficient fine-tuning techniques, which may not reach the\nperformance heights possible with full-parameter tuning. However, the\ncommunication overhead associated with full-parameter tuning is prohibitively\nhigh for both servers and clients. This work introduces FedKSeed, a novel\napproach that employs zeroth-order optimization (ZOO) with a set of random\nseeds. It enables federated full-parameter tuning of billion-sized LLMs\ndirectly on devices. Our method significantly reduces transmission requirements\nbetween the server and clients to just a few scalar gradients and random seeds,\namounting to only a few thousand bytes. Building on this, we develop a strategy\nto assess the significance of ZOO perturbations for FL, allowing for\nprobability-differentiated seed sampling. This prioritizes perturbations that\nhave a greater impact on model accuracy. Experiments across six scenarios with\ndifferent LLMs, datasets and data partitions demonstrate that our approach\noutperforms existing federated LLM fine-tuning methods in terms of both\ncommunication efficiency and new task generalization.",
        "date": "2023-12-11T13:03:21+00:00",
        "link": "http://arxiv.org/pdf/2312.06353v1"
    },
    {
        "title": "DiffAIL: Diffusion Adversarial Imitation Learning",
        "authors": [
            "Bingzheng Wang",
            "Yan Zhang",
            "Teng Pang",
            "Guoqiang Wu",
            "Yilong Yin"
        ],
        "abstract": "Imitation learning aims to solve the problem of defining reward functions in\nreal-world decision-making tasks. The current popular approach is the\nAdversarial Imitation Learning (AIL) framework, which matches expert\nstate-action occupancy measures to obtain a surrogate reward for forward\nreinforcement learning. However, the traditional discriminator is a simple\nbinary classifier and doesn't learn an accurate distribution, which may result\nin failing to identify expert-level state-action pairs induced by the policy\ninteracting with the environment. To address this issue, we propose a method\nnamed diffusion adversarial imitation learning (DiffAIL), which introduces the\ndiffusion model into the AIL framework. Specifically, DiffAIL models the\nstate-action pairs as unconditional diffusion models and uses diffusion loss as\npart of the discriminator's learning objective, which enables the discriminator\nto capture better expert demonstrations and improve generalization.\nExperimentally, the results show that our method achieves state-of-the-art\nperformance and significantly surpasses expert demonstration on two benchmark\ntasks, including the standard state-action setting and state-only settings. Our\ncode can be available at an anonymous link\nhttps://github.com/ML-Group-SDU/DiffAIL.",
        "date": "2023-12-11T12:53:30+00:00",
        "link": "http://arxiv.org/pdf/2312.06348v1"
    },
    {
        "title": "RankMatch: A Novel Approach to Semi-Supervised Label Distribution Learning Leveraging Inter-label Correlations",
        "authors": [
            "Kouzhiqiang Yucheng Xie",
            "Jing Wang",
            "Yuheng Jia",
            "Boyu Shi",
            "Xin Geng"
        ],
        "abstract": "This paper introduces RankMatch, an innovative approach for Semi-Supervised\nLabel Distribution Learning (SSLDL). Addressing the challenge of limited\nlabeled data, RankMatch effectively utilizes a small number of labeled examples\nin conjunction with a larger quantity of unlabeled data, reducing the need for\nextensive manual labeling in Deep Neural Network (DNN) applications.\nSpecifically, RankMatch introduces an ensemble learning-inspired averaging\nstrategy that creates a pseudo-label distribution from multiple weakly\naugmented images. This not only stabilizes predictions but also enhances the\nmodel's robustness. Beyond this, RankMatch integrates a pairwise relevance\nranking (PRR) loss, capturing the complex inter-label correlations and ensuring\nthat the predicted label distributions align with the ground truth.\n  We establish a theoretical generalization bound for RankMatch, and through\nextensive experiments, demonstrate its superiority in performance against\nexisting SSLDL methods.",
        "date": "2023-12-11T12:47:29+00:00",
        "link": "http://arxiv.org/pdf/2312.06343v1"
    },
    {
        "title": "Detecting Contextual Network Anomalies with Graph Neural Networks",
        "authors": [
            "Hamid Latif-Martínez",
            "José Suárez-Varela",
            "Albert Cabellos-Aparicio",
            "Pere Barlet-Ros"
        ],
        "abstract": "Detecting anomalies on network traffic is a complex task due to the massive\namount of traffic flows in today's networks, as well as the highly-dynamic\nnature of traffic over time. In this paper, we propose the use of Graph Neural\nNetworks (GNN) for network traffic anomaly detection. We formulate the problem\nas contextual anomaly detection on network traffic measurements, and propose a\ncustom GNN-based solution that detects traffic anomalies on origin-destination\nflows. In our evaluation, we use real-world data from Abilene (6 months), and\nmake a comparison with other widely used methods for the same task (PCA, EWMA,\nRNN). The results show that the anomalies detected by our solution are quite\ncomplementary to those captured by the baselines (with a max. of 36.33%\noverlapping anomalies for PCA). Moreover, we manually inspect the anomalies\ndetected by our method, and find that a large portion of them can be visually\nvalidated by a network expert (64% with high confidence, 18% with mid\nconfidence, 18% normal traffic). Lastly, we analyze the characteristics of the\nanomalies through two paradigmatic cases that are quite representative of the\nbulk of anomalies.",
        "date": "2023-12-11T12:45:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06342v1"
    },
    {
        "title": "Vehicle Lane Change Prediction based on Knowledge Graph Embeddings and Bayesian Inference",
        "authors": [
            "M. Manzour",
            "A. Ballardini",
            "R. Izquierdo",
            "M. A. Sotelo"
        ],
        "abstract": "Prediction of vehicle lane change maneuvers has gained a lot of momentum in\nthe last few years. Some recent works focus on predicting a vehicle's intention\nby predicting its trajectory first. This is not enough, as it ignores the\ncontext of the scene and the state of the surrounding vehicles (as they might\nbe risky to the target vehicle). Other works assessed the risk made by the\nsurrounding vehicles only by considering their existence around the target\nvehicle, or by considering the distance and relative velocities between them\nand the target vehicle as two separate numerical features. In this work, we\npropose a solution that leverages Knowledge Graphs (KGs) to anticipate lane\nchanges based on linguistic contextual information in a way that goes well\nbeyond the capabilities of current perception systems. Our solution takes the\nTime To Collision (TTC) with surrounding vehicles as input to assess the risk\non the target vehicle. Moreover, our KG is trained on the HighD dataset using\nthe TransE model to obtain the Knowledge Graph Embeddings (KGE). Then, we apply\nBayesian inference on top of the KG using the embeddings learned during\ntraining. Finally, the model can predict lane changes two seconds ahead with\n97.95% f1-score, which surpassed the state of the art, and three seconds before\nchanging lanes with 93.60% f1-score.",
        "date": "2023-12-11T12:33:44+00:00",
        "link": "http://arxiv.org/pdf/2312.06336v1"
    },
    {
        "title": "TPRNN: A Top-Down Pyramidal Recurrent Neural Network for Time Series Forecasting",
        "authors": [
            "Ling Chen",
            "Jiahua Cui"
        ],
        "abstract": "Time series refer to a series of data points indexed in time order, which can\nbe found in various fields, e.g., transportation, healthcare, and finance.\nAccurate time series forecasting can enhance optimization planning and\ndecision-making support. Time series have multi-scale characteristics, i.e.,\ndifferent temporal patterns at different scales, which presents a challenge for\ntime series forecasting. In this paper, we propose TPRNN, a Top-down Pyramidal\nRecurrent Neural Network for time series forecasting. We first construct\nsubsequences of different scales from the input, forming a pyramid structure.\nThen by executing a multi-scale information interaction module from top to\nbottom, we model both the temporal dependencies of each scale and the\ninfluences of subsequences of different scales, resulting in a complete\nmodeling of multi-scale temporal patterns in time series. Experiments on seven\nreal-world datasets demonstrate that TPRNN has achieved the state-of-the-art\nperformance with an average improvement of 8.13% in MSE compared to the best\nbaseline.",
        "date": "2023-12-11T12:21:45+00:00",
        "link": "http://arxiv.org/pdf/2312.06328v1"
    },
    {
        "title": "GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models",
        "authors": [
            "Jiaxu Zhao",
            "Meng Fang",
            "Shirui Pan",
            "Wenpeng Yin",
            "Mykola Pechenizkiy"
        ],
        "abstract": "Warning: This paper contains content that may be offensive or upsetting.\nThere has been a significant increase in the usage of large language models\n(LLMs) in various applications, both in their original form and through\nfine-tuned adaptations. As a result, LLMs have gained popularity and are being\nwidely adopted by a large user community. However, one of the concerns with\nLLMs is the potential generation of socially biased content. The existing\nevaluation methods have many constraints, and their results exhibit a limited\ndegree of interpretability. In this work, we propose a bias evaluation\nframework named GPTBIAS that leverages the high performance of LLMs (e.g.,\nGPT-4 \\cite{openai2023gpt4}) to assess bias in models. We also introduce\nprompts called Bias Attack Instructions, which are specifically designed for\nevaluating model bias. To enhance the credibility and interpretability of bias\nevaluation, our framework not only provides a bias score but also offers\ndetailed information, including bias types, affected demographics, keywords,\nreasons behind the biases, and suggestions for improvement. We conduct\nextensive experiments to demonstrate the effectiveness and usability of our\nbias evaluation framework.",
        "date": "2023-12-11T12:02:14+00:00",
        "link": "http://arxiv.org/pdf/2312.06315v1"
    },
    {
        "title": "An unsupervised learning approach to evaluate questionnaire data -- what one can learn from violations of measurement invariance",
        "authors": [
            "Max Hahn-Klimroth",
            "Paul W. Dierkes",
            "Matthias W. Kleespies"
        ],
        "abstract": "In several branches of the social sciences and humanities, surveys based on\nstandardized questionnaires are a prominent research tool. While there are a\nvariety of ways to analyze the data, some standard procedures have become\nestablished. When those surveys want to analyze differences in the answer\npatterns of different groups (e.g., countries, gender, age, ...), these\nprocedures can only be carried out in a meaningful way if there is measurement\ninvariance, i.e., the measured construct has psychometric equivalence across\ngroups. As recently raised as an open problem by Sauerwein et al. (2021), new\nevaluation methods that work in the absence of measurement invariance are\nneeded.\n  This paper promotes an unsupervised learning-based approach to such research\ndata by proposing a procedure that works in three phases: data preparation,\nclustering of questionnaires, and measuring similarity based on the obtained\nclustering and the properties of each group. We generate synthetic data in\nthree data sets, which allows us to compare our approach with the PCA approach\nunder measurement invariance and under violated measurement invariance. As a\nmain result, we obtain that the approach provides a natural comparison between\ngroups and a natural description of the response patterns of the groups.\nMoreover, it can be safely applied to a wide variety of data sets, even in the\nabsence of measurement invariance. Finally, this approach allows us to\ntranslate (violations of) measurement invariance into a meaningful measure of\nsimilarity.",
        "date": "2023-12-11T11:31:41+00:00",
        "link": "http://arxiv.org/pdf/2312.06309v1"
    },
    {
        "title": "A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML",
        "authors": [
            "Giorgos Borboudakis",
            "Paulos Charonyktakis",
            "Konstantinos Paraschakis",
            "Ioannis Tsamardinos"
        ],
        "abstract": "AutoML platforms have numerous options for the algorithms to try for each\nstep of the analysis, i.e., different possible algorithms for imputation,\ntransformations, feature selection, and modelling. Finding the optimal\ncombination of algorithms and hyper-parameter values is computationally\nexpensive, as the number of combinations to explore leads to an exponential\nexplosion of the space. In this paper, we present the Sequential\nHyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an\nAutoML tool with negligible drop in its predictive performance. SHSR is a\nmeta-level learning algorithm that analyzes past runs of an AutoML tool on\nseveral datasets and learns which hyper-parameter values to filter out from\nconsideration on a new dataset to analyze. SHSR is evaluated on 284\nclassification and 375 regression problems, showing an approximate 30%\nreduction in execution time with a performance drop of less than 0.1%.",
        "date": "2023-12-11T11:26:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06305v1"
    },
    {
        "title": "Exploiting Label Skews in Federated Learning with Model Concatenation",
        "authors": [
            "Yiqun Diao",
            "Qinbin Li",
            "Bingsheng He"
        ],
        "abstract": "Federated Learning (FL) has emerged as a promising solution to perform deep\nlearning on different data owners without exchanging raw data. However, non-IID\ndata has been a key challenge in FL, which could significantly degrade the\naccuracy of the final model. Among different non-IID types, label skews have\nbeen challenging and common in image classification and other tasks. Instead of\naveraging the local models in most previous studies, we propose FedConcat, a\nsimple and effective approach that concatenates these local models as the base\nof the global model to effectively aggregate the local knowledge. To reduce the\nsize of the global model, we adopt the clustering technique to group the\nclients by their label distributions and collaboratively train a model inside\neach cluster. We theoretically analyze the advantage of concatenation over\naveraging by analyzing the information bottleneck of deep neural networks.\nExperimental results demonstrate that FedConcat achieves significantly higher\naccuracy than previous state-of-the-art FL methods in various heterogeneous\nlabel skew distribution settings and meanwhile has lower communication costs.\nOur code is publicly available.",
        "date": "2023-12-11T10:44:52+00:00",
        "link": "http://arxiv.org/pdf/2312.06290v1"
    },
    {
        "title": "Extrapolating tipping points and simulating non-stationary dynamics of complex systems using efficient machine learning",
        "authors": [
            "Daniel Köglmayr",
            "Christoph Räth"
        ],
        "abstract": "Model-free and data-driven prediction of tipping point transitions in\nnonlinear dynamical systems is a challenging and outstanding task in complex\nsystems science. We propose a novel, fully data-driven machine learning\nalgorithm based on next-generation reservoir computing to extrapolate the\nbifurcation behavior of nonlinear dynamical systems using stationary training\ndata samples. We show that this method can extrapolate tipping point\ntransitions. Furthermore, it is demonstrated that the trained next-generation\nreservoir computing architecture can be used to predict non-stationary dynamics\nwith time-varying bifurcation parameters. In doing so, post-tipping point\ndynamics of unseen parameter regions can be simulated.",
        "date": "2023-12-11T10:37:28+00:00",
        "link": "http://arxiv.org/pdf/2312.06283v1"
    },
    {
        "title": "Adaptive Compression of the Latent Space in Variational Autoencoders",
        "authors": [
            "Gabriela Sejnova",
            "Michal Vavrecka",
            "Karla Stepanova"
        ],
        "abstract": "Variational Autoencoders (VAEs) are powerful generative models that have been\nwidely used in various fields, including image and text generation. However,\none of the known challenges in using VAEs is the model's sensitivity to its\nhyperparameters, such as the latent space size. This paper presents a simple\nextension of VAEs for automatically determining the optimal latent space size\nduring the training process by gradually decreasing the latent size through\nneuron removal and observing the model performance. The proposed method is\ncompared to traditional hyperparameter grid search and is shown to be\nsignificantly faster while still achieving the best optimal dimensionality on\nfour image datasets. Furthermore, we show that the final performance of our\nmethod is comparable to training on the optimal latent size from scratch, and\nmight thus serve as a convenient substitute.",
        "date": "2023-12-11T10:35:31+00:00",
        "link": "http://arxiv.org/pdf/2312.06280v1"
    },
    {
        "title": "Regional Correlation Aided Mobile Traffic Prediction with Spatiotemporal Deep Learning",
        "authors": [
            "JeongJun Park",
            "Lusungu J. Mwasinga",
            "Huigyu Yang",
            "Syed M. Raza",
            "Duc-Tai Le",
            "Moonseong Kim",
            "Min Young Chung",
            "Hyunseung Choo"
        ],
        "abstract": "Mobile traffic data in urban regions shows differentiated patterns during\ndifferent hours of the day. The exploitation of these patterns enables highly\naccurate mobile traffic prediction for proactive network management. However,\nrecent Deep Learning (DL) driven studies have only exploited spatiotemporal\nfeatures and have ignored the geographical correlations, causing high\ncomplexity and erroneous mobile traffic predictions. This paper addresses these\nlimitations by proposing an enhanced mobile traffic prediction scheme that\ncombines the clustering strategy of daily mobile traffic peak time and novel\nmulti Temporal Convolutional Network with a Long Short Term Memory (multi\nTCN-LSTM) model. The mobile network cells that exhibit peak traffic during the\nsame hour of the day are clustered together. Our experiments on large-scale\nreal-world mobile traffic data show up to 28% performance improvement compared\nto state-of-the-art studies, which confirms the efficacy and viability of the\nproposed approach.",
        "date": "2023-12-11T10:33:19+00:00",
        "link": "http://arxiv.org/pdf/2312.06279v1"
    },
    {
        "title": "DG-TTA: Out-of-domain medical image segmentation through Domain Generalization and Test-Time Adaptation",
        "authors": [
            "Christian Weihsbach",
            "Christian N. Kruse",
            "Alexander Bigalke",
            "Mattias P. Heinrich"
        ],
        "abstract": "Applying pre-trained medical segmentation models on out-of-domain images\noften yields predictions of insufficient quality. Several strategies have been\nproposed to maintain model performance, such as finetuning or unsupervised- and\nsource-free domain adaptation. These strategies set restrictive requirements\nfor data availability. In this study, we propose to combine domain\ngeneralization and test-time adaptation to create a highly effective approach\nfor reusing pre-trained models in unseen target domains. Domain-generalized\npre-training on source data is used to obtain the best initial performance in\nthe target domain. We introduce the MIND descriptor previously used in image\nregistration tasks as a further technique to achieve generalization and present\nsuperior performance for small-scale datasets compared to existing approaches.\nAt test-time, high-quality segmentation for every single unseen scan is ensured\nby optimizing the model weights for consistency given different image\naugmentations. That way, our method enables separate use of source and target\ndata and thus removes current data availability barriers. Moreover, the\npresented method is highly modular as it does not require specific model\narchitectures or prior knowledge of involved domains and labels. We demonstrate\nthis by integrating it into the nnUNet, which is currently the most popular and\naccurate framework for medical image segmentation. We employ multiple datasets\ncovering abdominal, cardiac, and lumbar spine scans and compose several\nout-of-domain scenarios in this study. We demonstrate that our method, combined\nwith pre-trained whole-body CT models, can effectively segment MR images with\nhigh accuracy in all of the aforementioned scenarios. Open-source code can be\nfound here: https://github.com/multimodallearning/DG-TTA",
        "date": "2023-12-11T10:26:21+00:00",
        "link": "http://arxiv.org/pdf/2312.06275v1"
    },
    {
        "title": "Regroup Median Loss for Combating Label Noise",
        "authors": [
            "Fengpeng Li",
            "Kemou Li",
            "Jinyu Tian",
            "Jiantao Zhou"
        ],
        "abstract": "The deep model training procedure requires large-scale datasets of annotated\ndata. Due to the difficulty of annotating a large number of samples, label\nnoise caused by incorrect annotations is inevitable, resulting in low model\nperformance and poor model generalization. To combat label noise, current\nmethods usually select clean samples based on the small-loss criterion and use\nthese samples for training. Due to some noisy samples similar to clean ones,\nthese small-loss criterion-based methods are still affected by label noise. To\naddress this issue, in this work, we propose Regroup Median Loss (RML) to\nreduce the probability of selecting noisy samples and correct losses of noisy\nsamples. RML randomly selects samples with the same label as the training\nsamples based on a new loss processing method. Then, we combine the stable mean\nloss and the robust median loss through a proposed regrouping strategy to\nobtain robust loss estimation for noisy samples. To further improve the model\nperformance against label noise, we propose a new sample selection strategy and\nbuild a semi-supervised method based on RML. Compared to state-of-the-art\nmethods, for both the traditionally trained and semi-supervised models, RML\nachieves a significant improvement on synthetic and complex real-world\ndatasets. The source code of the paper has been released.",
        "date": "2023-12-11T10:19:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06273v1"
    },
    {
        "title": "Creating Spoken Dialog Systems in Ultra-Low Resourced Settings",
        "authors": [
            "Moayad Elamin",
            "Muhammad Omer",
            "Yonas Chanie",
            "Henslaac Ndlovu"
        ],
        "abstract": "Automatic Speech Recognition (ASR) systems are a crucial technology that is\nused today to design a wide variety of applications, most notably, smart\nassistants, such as Alexa. ASR systems are essentially dialogue systems that\nemploy Spoken Language Understanding (SLU) to extract meaningful information\nfrom speech. The main challenge with designing such systems is that they\nrequire a huge amount of labeled clean data to perform competitively, such data\nis extremely hard to collect and annotate to respective SLU tasks, furthermore,\nwhen designing such systems for low resource languages, where data is extremely\nlimited, the severity of the problem intensifies. In this paper, we focus on a\nfairly popular SLU task, that is, Intent Classification while working with a\nlow resource language, namely, Flemish. Intent Classification is a task\nconcerned with understanding the intents of the user interacting with the\nsystem. We build on existing light models for intent classification in Flemish,\nand our main contribution is applying different augmentation techniques on two\nlevels -- the voice level, and the phonetic transcripts level -- to the\nexisting models to counter the problem of scarce labeled data in low-resource\nlanguages. We find that our data augmentation techniques, on both levels, have\nimproved the model performance on a number of tasks.",
        "date": "2023-12-11T10:04:05+00:00",
        "link": "http://arxiv.org/pdf/2312.06266v1"
    },
    {
        "title": "No Prior Mask: Eliminate Redundant Action for Deep Reinforcement Learning",
        "authors": [
            "Dianyu Zhong",
            "Yiqin Yang",
            "Qianchuan Zhao"
        ],
        "abstract": "The large action space is one fundamental obstacle to deploying Reinforcement\nLearning methods in the real world. The numerous redundant actions will cause\nthe agents to make repeated or invalid attempts, even leading to task failure.\nAlthough current algorithms conduct some initial explorations for this issue,\nthey either suffer from rule-based systems or depend on expert demonstrations,\nwhich significantly limits their applicability in many real-world settings. In\nthis work, we examine the theoretical analysis of what action can be eliminated\nin policy optimization and propose a novel redundant action filtering\nmechanism. Unlike other works, our method constructs the similarity factor by\nestimating the distance between the state distributions, which requires no\nprior knowledge. In addition, we combine the modified inverse model to avoid\nextensive computation in high-dimensional state space. We reveal the underlying\nstructure of action spaces and propose a simple yet efficient redundant action\nfiltering mechanism named No Prior Mask (NPM) based on the above techniques. We\nshow the superior performance of our method by conducting extensive experiments\non high-dimensional, pixel-input, and stochastic problems with various action\nredundancy. Our code is public online at https://github.com/zhongdy15/npm.",
        "date": "2023-12-11T09:56:02+00:00",
        "link": "http://arxiv.org/pdf/2312.06258v1"
    },
    {
        "title": "Neural Autoencoder-Based Structure-Preserving Model Order Reduction and Control Design for High-Dimensional Physical Systems",
        "authors": [
            "Marco Lepri",
            "Davide Bacciu",
            "Cosimo Della Santina"
        ],
        "abstract": "This work concerns control-oriented and structure-preserving learning of\nlow-dimensional approximations of high-dimensional physical systems, with a\nfocus on mechanical systems. We investigate the integration of neural\nautoencoders in model order reduction, while at the same time preserving\nHamiltonian or Lagrangian structures. We focus on extensively evaluating the\nconsidered methodology by performing simulation and control experiments on\nlarge mass-spring-damper networks, with hundreds of states. The empirical\nfindings reveal that compressed latent dynamics with less than 5 degrees of\nfreedom can accurately reconstruct the original systems' transient and\nsteady-state behavior with a relative total error of around 4\\%, while\nsimultaneously accurately reconstructing the total energy. Leveraging this\nsystem compression technique, we introduce a model-based controller that\nexploits the mathematical structure of the compressed model to regulate the\nconfiguration of heavily underactuated mechanical systems.",
        "date": "2023-12-11T09:52:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06256v1"
    },
    {
        "title": "Ensemble Interpretation: A Unified Method for Interpretable Machine Learning",
        "authors": [
            "Chao Min",
            "Guoyong Liao",
            "Guoquan Wen",
            "Yingjun Li",
            "Xing Guo"
        ],
        "abstract": "To address the issues of stability and fidelity in interpretable learning, a\nnovel interpretable methodology, ensemble interpretation, is presented in this\npaper which integrates multi-perspective explanation of various interpretation\nmethods. On one hand, we define a unified paradigm to describe the common\nmechanism of different interpretation methods, and then integrate the multiple\ninterpretation results to achieve more stable explanation. On the other hand, a\nsupervised evaluation method based on prior knowledge is proposed to evaluate\nthe explaining performance of an interpretation method. The experiment results\nshow that the ensemble interpretation is more stable and more consistent with\nhuman experience and cognition. As an application, we use the ensemble\ninterpretation for feature selection, and then the generalization performance\nof the corresponding learning model is significantly improved.",
        "date": "2023-12-11T09:51:24+00:00",
        "link": "http://arxiv.org/pdf/2312.06255v1"
    },
    {
        "title": "Modyn: A Platform for Model Training on Dynamic Datasets With Sample-Level Data Selection",
        "authors": [
            "Maximilian Böther",
            "Viktor Gsteiger",
            "Ties Robroek",
            "Ana Klimovic"
        ],
        "abstract": "Machine learning training data is often dynamic in real-world use cases,\ni.e., data is added or removed and may experience distribution shifts over\ntime. Models must incorporate this evolving training data to improve\ngeneralization, adapt to potential distribution shifts, and adhere to privacy\nregulations. However, the cost of model (re)training is proportional to how\noften the model trains and on how much data it trains on. While ML research\nexplores these topics in isolation, there is no end-to-end open-source platform\nto facilitate the exploration of model retraining and data selection policies\nand the deployment these algorithms efficiently at scale.\n  We present Modyn, a platform for model training on dynamic datasets that\nenables sample-level data selection and triggering policies. Modyn orchestrates\ncontinuous training pipelines while optimizing the underlying system\ninfrastructure to support fast access to arbitrary data samples for efficient\ndata selection. Modyn's extensible architecture allows users to run training\npipelines without modifying the platform code, and enables researchers to\neffortlessly extend the system. We evaluate Modyn's training throughput,\nshowing that even in memory-bound recommendation systems workloads, Modyn is\nable to reach 80 to 100 % of the throughput compared to loading big chunks of\ndata locally without sample-level data selection. Additionally, we showcase\nModyn's functionality with three different data selection policies.",
        "date": "2023-12-11T09:50:52+00:00",
        "link": "http://arxiv.org/pdf/2312.06254v1"
    },
    {
        "title": "Team-related Features in Code Review Prediction Models",
        "authors": [
            "Eduardo Witter",
            "Ingrid Nunes",
            "Dietmar Jannach"
        ],
        "abstract": "Modern Code Review (MCR) is an informal tool-assisted quality assurance\npractice. It relies on the asynchronous communication among the authors of code\nchanges and reviewers, who are developers that provide feedback. However, from\ncandidate developers, some are able to provide better feedback than others\ngiven a particular context. The selection of reviewers is thus an important\ntask, which can benefit from automated support. Many approaches have been\nproposed in this direction, using for example data from code review\nrepositories to recommend reviewers. In this paper, we propose the use of\nteam-related features to improve the performance of predictions that are\nhelpful to build code reviewer recommenders, with our target predictions being\nthe identification of reviewers that would participate in a review and the\nprovided amount of feedback. We evaluate the prediction power of these\nfeatures, which are related to code ownership, workload, and team relationship.\nThis evaluation was done by carefully addressing challenges imposed by the MCR\ndomain, such as temporal aspects of the dataset and unbalanced classes.\nMoreover, given that it is currently unknown how much past data is needed for\nbuilding MCR prediction models with acceptable performance, we explore the\namount of past data used to build prediction models. Our results show that,\nindividually, features related to code ownership have the best prediction\npower. However, based on feature selection, we conclude that all proposed\nfeatures together with lines of code can make the best predictions for both\nreviewer participation and amount of feedback. Regarding the amount of past\ndata, the timeframes of 3, 6, 9, and 12 months of data produce similar results.\nTherefore, models can be trained considering short timeframes, thus reducing\nthe computational costs with negligible impact in the prediction performance\n...",
        "date": "2023-12-11T09:30:09+00:00",
        "link": "http://arxiv.org/pdf/2312.06244v1"
    },
    {
        "title": "Improving Startup Success with Text Analysis",
        "authors": [
            "Emily Gavrilenko",
            "Foaad Khosmood",
            "Mahdi Rastad",
            "Sadra Amiri Moghaddam"
        ],
        "abstract": "Investors are interested in predicting future success of startup companies,\npreferably using publicly available data which can be gathered using free\nonline sources. Using public-only data has been shown to work, but there is\nstill much room for improvement. Two of the best performing prediction\nexperiments use 17 and 49 features respectively, mostly numeric and categorical\nin nature. In this paper, we significantly expand and diversify both the\nsources and the number of features (to 171) to achieve better prediction. Data\ncollected from Crunchbase, the Google Search API, and Twitter (now X) are used\nto predict whether a company will raise a round of funding within a fixed time\nhorizon. Much of the new features are textual and the Twitter subset include\nlinguistic metrics such as measures of passive voice and parts-of-speech. A\ntotal of ten machine learning models are also evaluated for best performance.\nThe adaptable model can be used to predict funding 1-5 years into the future,\nwith a variable cutoff threshold to favor either precision or recall.\nPrediction with comparable assumptions generally achieves F scores above 0.730\nwhich outperforms previous attempts in the literature (0.531), and does so with\nfewer examples. Furthermore, we find that the vast majority of the performance\nimpact comes from the top 18 of 171 features which are mostly generic company\nobservations, including the best performing individual feature which is the\nfree-form text description of the company.",
        "date": "2023-12-11T09:22:18+00:00",
        "link": "http://arxiv.org/pdf/2312.06236v1"
    },
    {
        "title": "CSOT: Curriculum and Structure-Aware Optimal Transport for Learning with Noisy Labels",
        "authors": [
            "Wanxing Chang",
            "Ye Shi",
            "Jingya Wang"
        ],
        "abstract": "Learning with noisy labels (LNL) poses a significant challenge in training a\nwell-generalized model while avoiding overfitting to corrupted labels. Recent\nadvances have achieved impressive performance by identifying clean labels and\ncorrecting corrupted labels for training. However, the current approaches rely\nheavily on the model's predictions and evaluate each sample independently\nwithout considering either the global and local structure of the sample\ndistribution. These limitations typically result in a suboptimal solution for\nthe identification and correction processes, which eventually leads to models\noverfitting to incorrect labels. In this paper, we propose a novel optimal\ntransport (OT) formulation, called Curriculum and Structure-aware Optimal\nTransport (CSOT). CSOT concurrently considers the inter- and intra-distribution\nstructure of the samples to construct a robust denoising and relabeling\nallocator. During the training process, the allocator incrementally assigns\nreliable labels to a fraction of the samples with the highest confidence. These\nlabels have both global discriminability and local coherence. Notably, CSOT is\na new OT formulation with a nonconvex objective function and curriculum\nconstraints, so it is not directly compatible with classical OT solvers. Here,\nwe develop a lightspeed computational method that involves a scaling iteration\nwithin a generalized conditional gradient framework to solve CSOT efficiently.\nExtensive experiments demonstrate the superiority of our method over the\ncurrent state-of-the-arts in LNL. Code is available at\nhttps://github.com/changwxx/CSOT-for-LNL.",
        "date": "2023-12-11T09:12:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06221v1"
    },
    {
        "title": "Dance of Channel and Sequence: An Efficient Attention-Based Approach for Multivariate Time Series Forecasting",
        "authors": [
            "Haoxin Wang",
            "Yipeng Mo",
            "Nan Yin",
            "Honghe Dai",
            "Bixiong Li",
            "Songhai Fan",
            "Site Mo"
        ],
        "abstract": "In recent developments, predictive models for multivariate time series\nanalysis have exhibited commendable performance through the adoption of the\nprevalent principle of channel independence. Nevertheless, it is imperative to\nacknowledge the intricate interplay among channels, which fundamentally\ninfluences the outcomes of multivariate predictions. Consequently, the notion\nof channel independence, while offering utility to a certain extent, becomes\nincreasingly impractical, leading to information degradation. In response to\nthis pressing concern, we present CSformer, an innovative framework\ncharacterized by a meticulously engineered two-stage self-attention mechanism.\nThis mechanism is purposefully designed to enable the segregated extraction of\nsequence-specific and channel-specific information, while sharing parameters to\npromote synergy and mutual reinforcement between sequences and channels.\nSimultaneously, we introduce sequence adapters and channel adapters, ensuring\nthe model's ability to discern salient features across various dimensions.\nRigorous experimentation, spanning multiple real-world datasets, underscores\nthe robustness of our approach, consistently establishing its position at the\nforefront of predictive performance across all datasets. This augmentation\nsubstantially enhances the capacity for feature extraction inherent to\nmultivariate time series data, facilitating a more comprehensive exploitation\nof the available information.",
        "date": "2023-12-11T09:10:38+00:00",
        "link": "http://arxiv.org/pdf/2312.06220v1"
    },
    {
        "title": "Structured state-space models are deep Wiener models",
        "authors": [
            "Fabio Bonassi",
            "Carl Andersson",
            "Per Mattsson",
            "Thomas B. Schön"
        ],
        "abstract": "The goal of this paper is to provide a system identification-friendly\nintroduction to the Structured State-space Models (SSMs). These models have\nbecome recently popular in the machine learning community since, owing to their\nparallelizability, they can be efficiently and scalably trained to tackle\nextremely-long sequence classification and regression problems. Interestingly,\nSSMs appear as an effective way to learn deep Wiener models, which allows to\nreframe SSMs as an extension of a model class commonly used in system\nidentification. In order to stimulate a fruitful exchange of ideas between the\nmachine learning and system identification communities, we deem it useful to\nsummarize the recent contributions on the topic in a structured and accessible\nform. At last, we highlight future research directions for which this community\ncould provide impactful contributions.",
        "date": "2023-12-11T08:53:38+00:00",
        "link": "http://arxiv.org/pdf/2312.06211v1"
    },
    {
        "title": "The Journey, Not the Destination: How Data Guides Diffusion Models",
        "authors": [
            "Kristian Georgiev",
            "Joshua Vendrow",
            "Hadi Salman",
            "Sung Min Park",
            "Aleksander Madry"
        ],
        "abstract": "Diffusion models trained on large datasets can synthesize photo-realistic\nimages of remarkable quality and diversity. However, attributing these images\nback to the training data-that is, identifying specific training examples which\ncaused an image to be generated-remains a challenge. In this paper, we propose\na framework that: (i) provides a formal notion of data attribution in the\ncontext of diffusion models, and (ii) allows us to counterfactually validate\nsuch attributions. Then, we provide a method for computing these attributions\nefficiently. Finally, we apply our method to find (and evaluate) such\nattributions for denoising diffusion probabilistic models trained on CIFAR-10\nand latent diffusion models trained on MS COCO. We provide code at\nhttps://github.com/MadryLab/journey-TRAK .",
        "date": "2023-12-11T08:39:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06205v1"
    },
    {
        "title": "Towards Transferable Adversarial Attacks with Centralized Perturbation",
        "authors": [
            "Shangbo Wu",
            "Yu-an Tan",
            "Yajie Wang",
            "Ruinan Ma",
            "Wencong Ma",
            "Yuanzhang Li"
        ],
        "abstract": "Adversarial transferability enables black-box attacks on unknown victim deep\nneural networks (DNNs), rendering attacks viable in real-world scenarios.\nCurrent transferable attacks create adversarial perturbation over the entire\nimage, resulting in excessive noise that overfit the source model.\nConcentrating perturbation to dominant image regions that are model-agnostic is\ncrucial to improving adversarial efficacy. However, limiting perturbation to\nlocal regions in the spatial domain proves inadequate in augmenting\ntransferability. To this end, we propose a transferable adversarial attack with\nfine-grained perturbation optimization in the frequency domain, creating\ncentralized perturbation. We devise a systematic pipeline to dynamically\nconstrain perturbation optimization to dominant frequency coefficients. The\nconstraint is optimized in parallel at each iteration, ensuring the directional\nalignment of perturbation optimization with model prediction. Our approach\nallows us to centralize perturbation towards sample-specific important\nfrequency features, which are shared by DNNs, effectively mitigating source\nmodel overfitting. Experiments demonstrate that by dynamically centralizing\nperturbation on dominating frequency coefficients, crafted adversarial examples\nexhibit stronger transferability, and allowing them to bypass various defenses.",
        "date": "2023-12-11T08:25:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06199v1"
    },
    {
        "title": "Why \"classic\" Transformers are shallow and how to make them go deep",
        "authors": [
            "Yueyao Yu",
            "Yin Zhang"
        ],
        "abstract": "Since its introduction in 2017, Transformer has emerged as the leading neural\nnetwork architecture, catalyzing revolutionary advancements in many AI\ndisciplines. The key innovation in Transformer is a Self-Attention (SA)\nmechanism designed to capture contextual information. However, extending the\noriginal Transformer design to models of greater depth has proven exceedingly\nchallenging, if not impossible. Even though various modifications have been\nproposed in order to stack more layers of SA mechanism into deeper models, a\nfull understanding of this depth problem remains elusive. In this paper, we\nconduct a comprehensive investigation, both theoretically and empirically, to\nsubstantiate the claim that the depth problem is caused by \\emph{token\nsimilarity escalation}; that is, tokens grow increasingly alike after repeated\napplications of the SA mechanism. Our analysis reveals that, driven by the\ninvariant leading eigenspace and large spectral gaps of attention matrices,\ntoken similarity provably escalates at a linear rate. Based on the gained\ninsight, we propose a simple strategy that, unlike most existing methods,\nsurgically removes excessive similarity without discounting the SA mechanism as\na whole. Preliminary experimental results confirm the effectiveness of the\nproposed approach on moderate-scale post-norm Transformer models.",
        "date": "2023-12-11T07:49:16+00:00",
        "link": "http://arxiv.org/pdf/2312.06182v1"
    },
    {
        "title": "Randomized Physics-Informed Machine Learning for Uncertainty Quantification in High-Dimensional Inverse Problems",
        "authors": [
            "Yifei Zong",
            "David Barajas-Solano",
            "Alexandre M. Tartakovsky"
        ],
        "abstract": "We propose a physics-informed machine learning method for uncertainty\nquantification (UQ) in high-dimensional inverse problems. In this method, the\nstates and parameters of partial differential equations (PDEs) are approximated\nwith truncated conditional Karhunen-Lo\\`eve expansions (CKLEs), which, by\nconstruction, match the measurements of the respective variables. The maximum a\nposteriori (MAP) solution of the inverse problem is formulated as a\nminimization problem over CKLE coefficients where the loss function is the sum\nof the norm of PDE residuals and $\\ell_2$ regularization term. This MAP\nformulation is known as the physics-informed CKLE (PICKLE) method. Uncertainty\nin the inverse solution is quantified in terms of the posterior distribution of\nCKLE coefficients, and we sample the posterior by solving a randomized PICKLE\nminimization problem, formulated by adding zero-mean Gaussian perturbations in\nthe PICKLE loss function. We call the proposed approach the randomized PICKLE\n(rPICKLE) method.\n  We test rPICKLE for the inverse problems of estimating parameters and states\nin groundwater models described by the diffusion (Darcy) equation with low and\nhigh-dimensional parameter space. We validate rPICKLE for the low-dimensional\ncase with 15 unknown CKLE parameters by showing that rPICKLE and Hamiltonian\nMonte Carlo (HMC) produce similar posterior distributions. The execution times\nof both methods increase with the dimensionality of the problem. However, the\nexecution time of HMC increases significantly faster with the problem\ndimensionality than that of rPICKLE. For the high-dimensional case (2000 CKLE\nparameters) with HMC does not reach the stopping criterion (the set number of\nsamples) after running the code for 30 days. On the other hand, rPICKLE\ngenerates the same number of samples in four to five days.",
        "date": "2023-12-11T07:33:16+00:00",
        "link": "http://arxiv.org/pdf/2312.06177v1"
    },
    {
        "title": "Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion",
        "authors": [
            "Anke Tang",
            "Li Shen",
            "Yong Luo",
            "Liang Ding",
            "Han Hu",
            "Bo Du",
            "Dacheng Tao"
        ],
        "abstract": "Merging models fine-tuned from a common, extensively pre-trained large model\nbut specialized for different tasks has been demonstrated as a cheap and\nscalable strategy to construct a multi-task model that performs well across\ndiverse tasks. Recent research, exemplified by task arithmetic, highlights that\nthis multi-task model can be derived through arithmetic operations on task\nvectors. Nevertheless, current merging techniques frequently resolve potential\nconflicts among parameters from task-specific models by evaluating individual\nattributes, such as the parameters' magnitude or sign, overlooking their\ncollective impact on the overall functionality of the model. In this work, we\npropose the CONtinuous relaxation of disCRETE (Concrete) subspace learning\nmethod to identify a common low-dimensional subspace and utilize its shared\ninformation to track the interference problem without sacrificing much\nperformance. Specifically, we model the problem as a bi-level optimization\nproblem and introduce a meta-learning framework to find the Concrete subspace\nmask through gradient-based techniques. At the upper level, we focus on\nlearning a shared Concrete mask to identify the subspace, while at the inner\nlevel, model merging is performed to maximize the performance of the merged\nmodel. We conduct extensive experiments on both vision domain and language\ndomain, and the results demonstrate the effectiveness of our method. The code\nis available at https://github.com/tanganke/subspace_fusion",
        "date": "2023-12-11T07:24:54+00:00",
        "link": "http://arxiv.org/pdf/2312.06173v1"
    },
    {
        "title": "Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments",
        "authors": [
            "Anthony Cintron Roman",
            "Jennifer Wortman Vaughan",
            "Valerie See",
            "Steph Ballard",
            "Nicolas Schifano",
            "Jehu Torres",
            "Caleb Robinson",
            "Juan M. Lavista Ferres"
        ],
        "abstract": "This paper introduces a no-code, machine-readable documentation framework for\nopen datasets, with a focus on Responsible AI (RAI) considerations. The\nframework aims to improve the accessibility, comprehensibility, and usability\nof open datasets, facilitating easier discovery and use, better understanding\nof content and context, and evaluation of dataset quality and accuracy. The\nproposed framework is designed to streamline the evaluation of datasets,\nhelping researchers, data scientists, and other open data users quickly\nidentify datasets that meet their needs and/or organizational policies or\nregulations. The paper also discusses the implementation of the framework and\nprovides recommendations to maximize its potential. The framework is expected\nto enhance the quality and reliability of data used in research and\ndecision-making, fostering the development of more responsible and trustworthy\nAI systems.",
        "date": "2023-12-11T06:41:14+00:00",
        "link": "http://arxiv.org/pdf/2312.06153v1"
    },
    {
        "title": "Improving the performance of weak supervision searches using transfer and meta-learning",
        "authors": [
            "Hugues Beauchesne",
            "Zong-En Chen",
            "Cheng-Wei Chiang"
        ],
        "abstract": "Weak supervision searches have in principle the advantages of both being able\nto train on experimental data and being able to learn distinctive signal\nproperties. However, the practical applicability of such searches is limited by\nthe fact that successfully training a neural network via weak supervision can\nrequire a large amount of signal. In this work, we seek to create neural\nnetworks that can learn from less experimental signal by using transfer and\nmeta-learning. The general idea is to first train a neural network on\nsimulations, thereby learning concepts that can be reused or becoming a more\nefficient learner. The neural network would then be trained on experimental\ndata and should require less signal because of its previous training. We find\nthat transfer and meta-learning can substantially improve the performance of\nweak supervision searches.",
        "date": "2023-12-11T06:40:46+00:00",
        "link": "http://arxiv.org/pdf/2312.06152v1"
    },
    {
        "title": "Compute-in-Memory based Neural Network Accelerators for Safety-Critical Systems: Worst-Case Scenarios and Protections",
        "authors": [
            "Zheyu Yan",
            "Xiaobo Sharon Hu",
            "Yiyu Shi"
        ],
        "abstract": "Emerging non-volatile memory (NVM)-based Computing-in-Memory (CiM)\narchitectures show substantial promise in accelerating deep neural networks\n(DNNs) due to their exceptional energy efficiency. However, NVM devices are\nprone to device variations. Consequently, the actual DNN weights mapped to NVM\ndevices can differ considerably from their targeted values, inducing\nsignificant performance degradation. Many existing solutions aim to optimize\naverage performance amidst device variations, which is a suitable strategy for\ngeneral-purpose conditions. However, the worst-case performance that is crucial\nfor safety-critical applications is largely overlooked in current research. In\nthis study, we define the problem of pinpointing the worst-case performance of\nCiM DNN accelerators affected by device variations. Additionally, we introduce\na strategy to identify a specific pattern of the device value deviations in the\ncomplex, high-dimensional value deviation space, responsible for this\nworst-case outcome. Our findings reveal that even subtle device variations can\nprecipitate a dramatic decline in DNN accuracy, posing risks for CiM-based\nplatforms in supporting safety-critical applications. Notably, we observe that\nprevailing techniques to bolster average DNN performance in CiM accelerators\nfall short in enhancing worst-case scenarios. In light of this issue, we\npropose a novel worst-case-aware training technique named A-TRICE that\nefficiently combines adversarial training and noise-injection training with\nright-censored Gaussian noise to improve the DNN accuracy in the worst-case\nscenarios. Our experimental results demonstrate that A-TRICE improves the\nworst-case accuracy under device variations by up to 33%.",
        "date": "2023-12-11T05:56:00+00:00",
        "link": "http://arxiv.org/pdf/2312.06137v1"
    },
    {
        "title": "Order Matters in the Presence of Dataset Imbalance for Multilingual Learning",
        "authors": [
            "Dami Choi",
            "Derrick Xin",
            "Hamid Dadkhahi",
            "Justin Gilmer",
            "Ankush Garg",
            "Orhan Firat",
            "Chih-Kuan Yeh",
            "Andrew M. Dai",
            "Behrooz Ghorbani"
        ],
        "abstract": "In this paper, we empirically study the optimization dynamics of multi-task\nlearning, particularly focusing on those that govern a collection of tasks with\nsignificant data imbalance. We present a simple yet effective method of\npre-training on high-resource tasks, followed by fine-tuning on a mixture of\nhigh/low-resource tasks. We provide a thorough empirical study and analysis of\nthis method's benefits showing that it achieves consistent improvements\nrelative to the performance trade-off profile of standard static weighting. We\nanalyze under what data regimes this method is applicable and show its\nimprovements empirically in neural machine translation (NMT) and multi-lingual\nlanguage modeling.",
        "date": "2023-12-11T05:46:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06134v1"
    },
    {
        "title": "Spreeze: High-Throughput Parallel Reinforcement Learning Framework",
        "authors": [
            "Jing Hou",
            "Guang Chen",
            "Ruiqi Zhang",
            "Zhijun Li",
            "Shangding Gu",
            "Changjun Jiang"
        ],
        "abstract": "The promotion of large-scale applications of reinforcement learning (RL)\nrequires efficient training computation. While existing parallel RL frameworks\nencompass a variety of RL algorithms and parallelization techniques, the\nexcessively burdensome communication frameworks hinder the attainment of the\nhardware's limit for final throughput and training effects on a single desktop.\nIn this paper, we propose Spreeze, a lightweight parallel framework for RL that\nefficiently utilizes a single desktop hardware resource to approach the\nthroughput limit. We asynchronously parallelize the experience sampling,\nnetwork update, performance evaluation, and visualization operations, and\nemploy multiple efficient data transmission techniques to transfer various\ntypes of data between processes. The framework can automatically adjust the\nparallelization hyperparameters based on the computing ability of the hardware\ndevice in order to perform efficient large-batch updates. Based on the\ncharacteristics of the \"Actor-Critic\" RL algorithm, our framework uses dual\nGPUs to independently update the network of actors and critics in order to\nfurther improve throughput. Simulation results show that our framework can\nachieve up to 15,000Hz experience sampling and 370,000Hz network update frame\nrate using only a personal desktop computer, which is an order of magnitude\nhigher than other mainstream parallel RL frameworks, resulting in a 73%\nreduction of training time. Our work on fully utilizing the hardware resources\nof a single desktop computer is fundamental to enabling efficient large-scale\ndistributed RL training.",
        "date": "2023-12-11T05:25:01+00:00",
        "link": "http://arxiv.org/pdf/2312.06126v1"
    },
    {
        "title": "GTA: Gated Toxicity Avoidance for LM Performance Preservation",
        "authors": [
            "Heegyu Kim",
            "Hyunsouk Cho"
        ],
        "abstract": "Caution: This paper includes offensive words that could potentially cause\nunpleasantness. The fast-paced evolution of generative language models such as\nGPT-4 has demonstrated outstanding results in various NLP generation tasks.\nHowever, due to the potential generation of offensive words related to race or\ngender, various Controllable Text Generation (CTG) methods have been proposed\nto mitigate the occurrence of harmful words. However, existing CTG methods not\nonly reduce toxicity but also negatively impact several aspects of the language\nmodel's generation performance, including topic consistency, grammar, and\nperplexity. This paper explores the limitations of previous methods and\nintroduces a novel solution in the form of a simple Gated Toxicity Avoidance\n(GTA) that can be applied to any CTG method. We also evaluate the effectiveness\nof the proposed GTA by comparing it with state-of-the-art CTG methods across\nvarious datasets. Our findings reveal that gated toxicity avoidance efficiently\nachieves comparable levels of toxicity reduction to the original CTG methods\nwhile preserving the generation performance of the language model.",
        "date": "2023-12-11T05:04:17+00:00",
        "link": "http://arxiv.org/pdf/2312.06122v1"
    },
    {
        "title": "Stellar: Systematic Evaluation of Human-Centric Personalized Text-to-Image Methods",
        "authors": [
            "Panos Achlioptas",
            "Alexandros Benetatos",
            "Iordanis Fostiropoulos",
            "Dimitris Skourtis"
        ],
        "abstract": "In this work, we systematically study the problem of personalized\ntext-to-image generation, where the output image is expected to portray\ninformation about specific human subjects. E.g., generating images of oneself\nappearing at imaginative places, interacting with various items, or engaging in\nfictional activities. To this end, we focus on text-to-image systems that input\na single image of an individual to ground the generation process along with\ntext describing the desired visual context. Our first contribution is to fill\nthe literature gap by curating high-quality, appropriate data for this task.\nNamely, we introduce a standardized dataset (Stellar) that contains\npersonalized prompts coupled with images of individuals that is an order of\nmagnitude larger than existing relevant datasets and where rich semantic\nground-truth annotations are readily available. Having established Stellar to\npromote cross-systems fine-grained comparisons further, we introduce a rigorous\nensemble of specialized metrics that highlight and disentangle fundamental\nproperties such systems should obey. Besides being intuitive, our new metrics\ncorrelate significantly more strongly with human judgment than currently used\nmetrics on this task. Last but not least, drawing inspiration from the recent\nworks of ELITE and SDXL, we derive a simple yet efficient, personalized\ntext-to-image baseline that does not require test-time fine-tuning for each\nsubject and which sets quantitatively and in human trials a new SoTA. For more\ninformation, please visit our project's website:\nhttps://stellar-gen-ai.github.io/.",
        "date": "2023-12-11T04:47:39+00:00",
        "link": "http://arxiv.org/pdf/2312.06116v1"
    },
    {
        "title": "AUGCAL: Improving Sim2Rreal Adaptation by Uncertainty Calibration on Augmented Synthetic Images",
        "authors": [
            "Prithvijit Chattopadhyay",
            "Bharat Goyal",
            "Boglarka Ecsedi",
            "Viraj Prabhu",
            "Judy Hoffman"
        ],
        "abstract": "Synthetic data (SIM) drawn from simulators have emerged as a popular\nalternative for training models where acquiring annotated real-world images is\ndifficult. However, transferring models trained on synthetic images to\nreal-world applications can be challenging due to appearance disparities. A\ncommonly employed solution to counter this SIM2REAL gap is unsupervised domain\nadaptation, where models are trained using labeled SIM data and unlabeled REAL\ndata. Mispredictions made by such SIM2REAL adapted models are often associated\nwith miscalibration - stemming from overconfident predictions on real data. In\nthis paper, we introduce AUGCAL, a simple training-time patch for unsupervised\nadaptation that improves SIM2REAL adapted models by - (1) reducing overall\nmiscalibration, (2) reducing overconfidence in incorrect predictions and (3)\nimproving confidence score reliability by better guiding misclassification\ndetection - all while retaining or improving SIM2REAL performance. Given a base\nSIM2REAL adaptation algorithm, at training time, AUGCAL involves replacing\nvanilla SIM images with strongly augmented views (AUG intervention) and\nadditionally optimizing for a training time calibration loss on augmented SIM\npredictions (CAL intervention). We motivate AUGCAL using a brief analytical\njustification of how to reduce miscalibration on unlabeled REAL data. Through\nour experiments, we empirically show the efficacy of AUGCAL across multiple\nadaptation methods, backbones, tasks and shifts.",
        "date": "2023-12-11T04:24:11+00:00",
        "link": "http://arxiv.org/pdf/2312.06106v1"
    },
    {
        "title": "Learning Unknown Intervention Targets in Structural Causal Models from Heterogeneous Data",
        "authors": [
            "Yuqin Yang",
            "Saber Salehkaleybar",
            "Negar Kiyavash"
        ],
        "abstract": "We study the problem of identifying the unknown intervention targets in\nstructural causal models where we have access to heterogeneous data collected\nfrom multiple environments. The unknown intervention targets are the set of\nendogenous variables whose corresponding exogenous noises change across the\nenvironments. We propose a two-phase approach which in the first phase recovers\nthe exogenous noises corresponding to unknown intervention targets whose\ndistributions have changed across environments. In the second phase, the\nrecovered noises are matched with the corresponding endogenous variables. For\nthe recovery phase, we provide sufficient conditions for learning these\nexogenous noises up to some component-wise invertible transformation. For the\nmatching phase, under the causal sufficiency assumption, we show that the\nproposed method uniquely identifies the intervention targets. In the presence\nof latent confounders, the intervention targets among the observed variables\ncannot be determined uniquely. We provide a candidate intervention target set\nwhich is a superset of the true intervention targets. Our approach improves\nupon the state of the art as the returned candidate set is always a subset of\nthe target set returned by previous work. Moreover, we do not require\nrestrictive assumptions such as linearity of the causal model or performing\ninvariance tests to learn whether a distribution is changing across\nenvironments which could be highly sample inefficient. Our experimental results\nshow the effectiveness of our proposed algorithm in practice.",
        "date": "2023-12-11T03:31:54+00:00",
        "link": "http://arxiv.org/pdf/2312.06091v1"
    },
    {
        "title": "TabMT: Generating tabular data with masked transformers",
        "authors": [
            "Manbir S Gulati",
            "Paul F Roysdon"
        ],
        "abstract": "Autoregressive and Masked Transformers are incredibly effective as generative\nmodels and classifiers. While these models are most prevalent in NLP, they also\nexhibit strong performance in other domains, such as vision. This work\ncontributes to the exploration of transformer-based models in synthetic data\ngeneration for diverse application domains. In this paper, we present TabMT, a\nnovel Masked Transformer design for generating synthetic tabular data. TabMT\neffectively addresses the unique challenges posed by heterogeneous data fields\nand is natively able to handle missing data. Our design leverages improved\nmasking techniques to allow for generation and demonstrates state-of-the-art\nperformance from extremely small to extremely large tabular datasets. We\nevaluate TabMT for privacy-focused applications and find that it is able to\ngenerate high quality data with superior privacy tradeoffs.",
        "date": "2023-12-11T03:28:11+00:00",
        "link": "http://arxiv.org/pdf/2312.06089v1"
    },
    {
        "title": "Complex-valued Neural Networks -- Theory and Analysis",
        "authors": [
            "Rayyan Abdalla"
        ],
        "abstract": "Complex-valued neural networks (CVNNs) have recently been successful in\nvarious pioneering areas which involve wave-typed information and\nfrequency-domain processing. This work addresses different structures and\nclassification of CVNNs. The theory behind complex activation functions,\nimplications related to complex differentiability and special activations for\nCVNN output layers are presented. The work also discusses CVNN learning and\noptimization using gradient and non-gradient based algorithms. Complex\nBackpropagation utilizing complex chain rule is also explained in terms of\nWirtinger calculus. Moreover, special modules for building CVNN models, such as\ncomplex batch normalization and complex random initialization are also\ndiscussed. The work also highlights libraries and software blocks proposed for\nCVNN implementations and discusses future directions. The objective of this\nwork is to understand the dynamics and most recent developments of CVNNs.",
        "date": "2023-12-11T03:24:26+00:00",
        "link": "http://arxiv.org/pdf/2312.06087v1"
    },
    {
        "title": "An Ambiguity Measure for Recognizing the Unknowns in Deep Learning",
        "authors": [
            "Roozbeh Yousefzadeh"
        ],
        "abstract": "We study the understanding of deep neural networks from the scope in which\nthey are trained on. While the accuracy of these models is usually impressive\non the aggregate level, they still make mistakes, sometimes on cases that\nappear to be trivial. Moreover, these models are not reliable in realizing what\nthey do not know leading to failures such as adversarial vulnerability and\nout-of-distribution failures. Here, we propose a measure for quantifying the\nambiguity of inputs for any given model with regard to the scope of its\ntraining. We define the ambiguity based on the geometric arrangements of the\ndecision boundaries and the convex hull of training set in the feature space\nlearned by the trained model, and demonstrate that a single ambiguity measure\nmay detect a considerable portion of mistakes of a model on in-distribution\nsamples, adversarial inputs, as well as out-of-distribution inputs. Using our\nambiguity measure, a model may abstain from classification when it encounters\nambiguous inputs leading to a better model accuracy not just on a given testing\nset, but on the inputs it may encounter at the world at large. In pursuit of\nthis measure, we develop a theoretical framework that can identify the unknowns\nof the model in relation to its scope. We put this in perspective with the\nconfidence of the model and develop formulations to identify the regions of the\ndomain which are unknown to the model, yet the model is guaranteed to have high\nconfidence.",
        "date": "2023-12-11T02:57:12+00:00",
        "link": "http://arxiv.org/pdf/2312.06077v1"
    },
    {
        "title": "Probabilistic Precipitation Downscaling with Optical Flow-Guided Diffusion",
        "authors": [
            "Prakhar Srivastava",
            "Ruihan Yang",
            "Gavin Kerrigan",
            "Gideon Dresdner",
            "Jeremy McGibbon",
            "Christopher Bretherton",
            "Stephan Mandt"
        ],
        "abstract": "In climate science and meteorology, local precipitation predictions are\nlimited by the immense computational costs induced by the high spatial\nresolution that simulation methods require. A common workaround is statistical\ndownscaling (aka superresolution), where a low-resolution prediction is\nsuper-resolved using statistical approaches. While traditional computer vision\ntasks mainly focus on human perception or mean squared error, applications in\nweather and climate require capturing the conditional distribution of\nhigh-resolution patterns given low-resolution patterns so that reliable\nensemble averages can be taken. Our approach relies on extending recent video\ndiffusion models to precipitation superresolution: an optical flow on the\nhigh-resolution output induces temporally coherent predictions, whereas a\ntemporally-conditioned diffusion model generates residuals that capture the\ncorrect noise characteristics and high-frequency patterns. We test our approach\non X-SHiELD, an established large-scale climate simulation dataset, and compare\nagainst two state-of-the-art baselines, focusing on CRPS, MSE, precipitation\ndistributions, as well as an illustrative case -- the complex terrain of\nCalifornia. Our approach sets a new standard for data-driven precipitation\ndownscaling.",
        "date": "2023-12-11T02:38:07+00:00",
        "link": "http://arxiv.org/pdf/2312.06071v1"
    },
    {
        "title": "CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models",
        "authors": [
            "Tuna Han Salih Meral",
            "Enis Simsar",
            "Federico Tombari",
            "Pinar Yanardag"
        ],
        "abstract": "Images produced by text-to-image diffusion models might not always faithfully\nrepresent the semantic intent of the provided text prompt, where the model\nmight overlook or entirely fail to produce certain objects. Existing solutions\noften require customly tailored functions for each of these problems, leading\nto sub-optimal results, especially for complex prompts. Our work introduces a\nnovel perspective by tackling this challenge in a contrastive context. Our\napproach intuitively promotes the segregation of objects in attention maps\nwhile also maintaining that pairs of related attributes are kept close to each\nother. We conduct extensive experiments across a wide variety of scenarios,\neach involving unique combinations of objects, attributes, and scenes. These\nexperiments effectively showcase the versatility, efficiency, and flexibility\nof our method in working with both latent and pixel-based diffusion models,\nincluding Stable Diffusion and Imagen. Moreover, we publicly share our source\ncode to facilitate further research.",
        "date": "2023-12-11T01:42:15+00:00",
        "link": "http://arxiv.org/pdf/2312.06059v1"
    },
    {
        "title": "IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions",
        "authors": [
            "Ziheng Zeng",
            "Kellen Tan Cheng",
            "Srihari Venkat Nanniyur",
            "Jianing Zhou",
            "Suma Bhat"
        ],
        "abstract": "Idiomatic expression (IE) processing and comprehension have challenged\npre-trained language models (PTLMs) because their meanings are\nnon-compositional. Unlike prior works that enable IE comprehension through\nfine-tuning PTLMs with sentences containing IEs, in this work, we construct\nIEKG, a commonsense knowledge graph for figurative interpretations of IEs. This\nextends the established ATOMIC2020 graph, converting PTLMs into knowledge\nmodels (KMs) that encode and infer commonsense knowledge related to IE use.\nExperiments show that various PTLMs can be converted into KMs with IEKG. We\nverify the quality of IEKG and the ability of the trained KMs with automatic\nand human evaluation. Through applications in natural language understanding,\nwe show that a PTLM injected with knowledge from IEKG exhibits improved IE\ncomprehension ability and can generalize to IEs unseen during training.",
        "date": "2023-12-11T00:57:11+00:00",
        "link": "http://arxiv.org/pdf/2312.06053v1"
    },
    {
        "title": "Federated Multilinear Principal Component Analysis with Applications in Prognostics",
        "authors": [
            "Chengyu Zhou",
            "Yuqi Su",
            "Tangbin Xia",
            "Xiaolei Fang"
        ],
        "abstract": "Multilinear Principal Component Analysis (MPCA) is a widely utilized method\nfor the dimension reduction of tensor data. However, the integration of MPCA\ninto federated learning remains unexplored in existing research. To tackle this\ngap, this article proposes a Federated Multilinear Principal Component Analysis\n(FMPCA) method, which enables multiple users to collaboratively reduce the\ndimension of their tensor data while keeping each user's data local and\nconfidential. The proposed FMPCA method is guaranteed to have the same\nperformance as traditional MPCA. An application of the proposed FMPCA in\nindustrial prognostics is also demonstrated. Simulated data and a real-world\ndata set are used to validate the performance of the proposed method.",
        "date": "2023-12-11T00:46:34+00:00",
        "link": "http://arxiv.org/pdf/2312.06050v1"
    },
    {
        "title": "Music-PAW: Learning Music Representations via Hierarchical Part-whole Interaction and Contrast",
        "authors": [
            "Dong Yao",
            "Shengyu Zhang",
            "Zhou Zhao",
            "Jieming Zhu",
            "Liqun Deng",
            "Wenqiao Zhang",
            "Zhenhua Dong",
            "Ruiming Tang",
            "Xin Jiang"
        ],
        "abstract": "The excellent performance of recent self-supervised learning methods on\nvarious downstream tasks has attracted great attention from academia and\nindustry. Some recent research efforts have been devoted to self-supervised\nmusic representation learning. Nevertheless, most of them learn to represent\nequally-sized music clips in the waveform or a spectrogram. Despite being\neffective in some tasks, learning music representations in such a manner\nlargely neglect the inherent part-whole hierarchies of music. Due to the\nhierarchical nature of the auditory cortex [24], understanding the bottom-up\nstructure of music, i.e., how different parts constitute the whole at different\nlevels, is essential for music understanding and representation learning. This\nwork pursues hierarchical music representation learning and introduces the\nMusic-PAW framework, which enables feature interactions of cropped music clips\nwith part-whole hierarchies. From a technical perspective, we propose a\ntransformer-based part-whole interaction module to progressively reason the\nstructural relationships between part-whole music clips at adjacent levels.\nBesides, to create a multi-hierarchy representation space, we devise a\nhierarchical contrastive learning objective to align part-whole music\nrepresentations in adjacent hierarchies. The merits of audio representation\nlearning from part-whole hierarchies have been validated on various downstream\ntasks, including music classification (single-label and multi-label), cover\nsong identification and acoustic scene classification.",
        "date": "2023-12-11T08:17:58+00:00",
        "link": "http://arxiv.org/pdf/2312.06197v1"
    },
    {
        "title": "Jointly Explicit and Implicit Cross-Modal Interaction Network for Anterior Chamber Inflammation Diagnosis",
        "authors": [
            "Qian Shao",
            "Ye Dai",
            "Haochao Ying",
            "Kan Xu",
            "Jinhong Wang",
            "Wei Chi",
            "Jian Wu"
        ],
        "abstract": "Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)\nfor optimal treatment. However, current diagnostic methods only rely on a\nlimited single-modal disease perspective, which leads to poor performance. In\nthis paper, we investigate a promising yet challenging way to fuse multimodal\ndata for ACI diagnosis. Notably, existing fusion paradigms focus on empowering\nimplicit modality interactions (i.e., self-attention and its variants), but\nneglect to inject explicit modality interactions, especially from clinical\nknowledge and imaging property. To this end, we propose a jointly Explicit and\nimplicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber\nInflammation Diagnosis that uses anterior segment optical coherence tomography\n(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we\nfirst develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract\nefficient feature representations in different modalities. Then, we devise an\nExplicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a\nkind of explicit clinical knowledge based on the tabular feature maps, then\nintegrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder\nto focus on more effective informativeness of the slit-lamp images. After that,\nthe Implicit Cross-Modal Interaction Module (ICIM), a transformer-based\nnetwork, further implicitly enhances modality interactions. Finally, we\nconstruct a considerable real-world dataset from our collaborative hospital and\nconduct sufficient experiments to demonstrate the superior performance of our\nproposed EiCI-Net compared with the state-of-the-art classification methods in\nvarious metrics.",
        "date": "2023-12-11T07:20:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06171v1"
    },
    {
        "title": "MATK: The Meme Analytical Tool Kit",
        "authors": [
            "Ming Shan Hee",
            "Aditi Kumaresan",
            "Nguyen Khoi Hoang",
            "Nirmalendu Prakash",
            "Rui Cao",
            "Roy Ka-Wei Lee"
        ],
        "abstract": "The rise of social media platforms has brought about a new digital culture\ncalled memes. Memes, which combine visuals and text, can strongly influence\npublic opinions on social and cultural issues. As a result, people have become\ninterested in categorizing memes, leading to the development of various\ndatasets and multimodal models that show promising results in this field.\nHowever, there is currently a lack of a single library that allows for the\nreproduction, evaluation, and comparison of these models using fair benchmarks\nand settings. To fill this gap, we introduce the Meme Analytical Tool Kit\n(MATK), an open-source toolkit specifically designed to support existing memes\ndatasets and cutting-edge multimodal models. MATK aims to assist researchers\nand engineers in training and reproducing these multimodal models for meme\nclassification tasks, while also providing analysis techniques to gain insights\ninto their strengths and weaknesses. To access MATK, please visit\n\\url{https://github.com/Social-AI-Studio/MATK}.",
        "date": "2023-12-11T03:36:59+00:00",
        "link": "http://arxiv.org/pdf/2312.06094v1"
    },
    {
        "title": "PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using Large Language Models",
        "authors": [
            "Nirmalendu Prakash",
            "Han Wang",
            "Nguyen Khoi Hoang",
            "Ming Shan Hee",
            "Roy Ka-Wei Lee"
        ],
        "abstract": "The proliferation of social media has given rise to a new form of\ncommunication: memes. Memes are multimodal and often contain a combination of\ntext and visual elements that convey meaning, humor, and cultural significance.\nWhile meme analysis has been an active area of research, little work has been\ndone on unsupervised multimodal topic modeling of memes, which is important for\ncontent moderation, social media analysis, and cultural studies. We propose\n\\textsf{PromptMTopic}, a novel multimodal prompt-based model designed to learn\ntopics from both text and visual modalities by leveraging the language modeling\ncapabilities of large language models. Our model effectively extracts and\nclusters topics learned from memes, considering the semantic interaction\nbetween the text and visual modalities. We evaluate our proposed model through\nextensive experiments on three real-world meme datasets, which demonstrate its\nsuperiority over state-of-the-art topic modeling baselines in learning\ndescriptive topics in memes. Additionally, our qualitative analysis shows that\n\\textsf{PromptMTopic} can identify meaningful and culturally relevant topics\nfrom memes. Our work contributes to the understanding of the topics and themes\nof memes, a crucial form of communication in today's society.\\\\\n\\red{\\textbf{Disclaimer: This paper contains sensitive content that may be\ndisturbing to some readers.}}",
        "date": "2023-12-11T03:36:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06093v1"
    },
    {
        "title": "Photorealistic Video Generation with Diffusion Models",
        "authors": [
            "Agrim Gupta",
            "Lijun Yu",
            "Kihyuk Sohn",
            "Xiuye Gu",
            "Meera Hahn",
            "Li Fei-Fei",
            "Irfan Essa",
            "Lu Jiang",
            "José Lezama"
        ],
        "abstract": "We present W.A.L.T, a transformer-based approach for photorealistic video\ngeneration via diffusion modeling. Our approach has two key design decisions.\nFirst, we use a causal encoder to jointly compress images and videos within a\nunified latent space, enabling training and generation across modalities.\nSecond, for memory and training efficiency, we use a window attention\narchitecture tailored for joint spatial and spatiotemporal generative modeling.\nTaken together these design decisions enable us to achieve state-of-the-art\nperformance on established video (UCF-101 and Kinetics-600) and image\n(ImageNet) generation benchmarks without using classifier free guidance.\nFinally, we also train a cascade of three models for the task of text-to-video\ngeneration consisting of a base latent video diffusion model, and two video\nsuper-resolution diffusion models to generate videos of $512 \\times 896$\nresolution at $8$ frames per second.",
        "date": "2023-12-11T18:59:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06662v1"
    },
    {
        "title": "Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?",
        "authors": [
            "Shabaz Patel",
            "Hassan Kane",
            "Rayhan Patel"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nnumerous natural language understanding use cases. However, this impressive\nperformance comes with inherent limitations, such as the tendency to perpetuate\nstereotypical biases or fabricate non-existent facts. In the context of Islam\nand its representation, accurate and factual representation of its beliefs and\nteachings rooted in the Quran and Sunnah is key. This work focuses on the\nchallenge of building domain-specific LLMs faithful to the Islamic worldview\nand proposes ways to build and evaluate such systems. Firstly, we define this\nopen-ended goal as a technical problem and propose various solutions.\nSubsequently, we critically examine known challenges inherent to each approach\nand highlight evaluation methodologies that can be used to assess such systems.\nThis work highlights the need for high-quality datasets, evaluations, and\ninterdisciplinary work blending machine learning with Islamic scholarship.",
        "date": "2023-12-11T18:59:09+00:00",
        "link": "http://arxiv.org/pdf/2312.06652v1"
    },
    {
        "title": "4M: Massively Multimodal Masked Modeling",
        "authors": [
            "David Mizrahi",
            "Roman Bachmann",
            "Oğuzhan Fatih Kar",
            "Teresa Yeo",
            "Mingfei Gao",
            "Afshin Dehghan",
            "Amir Zamir"
        ],
        "abstract": "Current machine learning models for vision are often highly specialized and\nlimited to a single modality and task. In contrast, recent large language\nmodels exhibit a wide range of capabilities, hinting at a possibility for\nsimilarly versatile models in computer vision. In this paper, we take a step in\nthis direction and propose a multimodal training scheme called 4M. It consists\nof training a single unified Transformer encoder-decoder using a masked\nmodeling objective across a wide range of input/output modalities - including\ntext, images, geometric, and semantic modalities, as well as neural network\nfeature maps. 4M achieves scalability by unifying the representation space of\nall modalities through mapping them into discrete tokens and performing\nmultimodal masked modeling on a small randomized subset of tokens.\n  4M leads to models that exhibit several key capabilities: (1) they can\nperform a diverse set of vision tasks out of the box, (2) they excel when\nfine-tuned for unseen downstream tasks or new input modalities, and (3) they\ncan function as a generative model that can be conditioned on arbitrary\nmodalities, enabling a wide variety of expressive multimodal editing\ncapabilities with remarkable flexibility.\n  Through experimental analyses, we demonstrate the potential of 4M for\ntraining versatile and scalable foundation models for vision tasks, setting the\nstage for further exploration in multimodal learning for vision and other\ndomains.",
        "date": "2023-12-11T18:57:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06647v1"
    },
    {
        "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
        "authors": [
            "Tong Chen",
            "Hongwei Wang",
            "Sihao Chen",
            "Wenhao Yu",
            "Kaixin Ma",
            "Xinran Zhao",
            "Dong Yu",
            "Hongming Zhang"
        ],
        "abstract": "Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our results reveal that\nproposition-based retrieval significantly outperforms traditional passage or\nsentence-based methods in dense retrieval. Moreover, retrieval by proposition\nalso enhances the performance of downstream QA tasks, since the retrieved texts\nare more condensed with question-relevant information, reducing the need for\nlengthy input tokens and minimizing the inclusion of extraneous, irrelevant\ninformation.",
        "date": "2023-12-11T18:57:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06648v1"
    },
    {
        "title": "Computational Copyright: Towards A Royalty Model for AI Music Generation Platforms",
        "authors": [
            "Junwei Deng",
            "Jiaqi Ma"
        ],
        "abstract": "The advancement of generative AI has given rise to pressing copyright\nchallenges, particularly in music industry. This paper focuses on the economic\naspects of these challenges, emphasizing that the economic impact constitutes a\ncentral issue in the copyright arena. The complexity of the black-box\ngenerative AI technologies not only suggests but necessitates algorithmic\nsolutions. However, such solutions have been largely missing, leading to\nregulatory challenges in this landscape. We aim to bridge the gap in current\napproaches by proposing potential royalty models for revenue sharing on AI\nmusic generation platforms. Our methodology involves a detailed analysis of\nexisting royalty models in platforms like Spotify and YouTube, and adapting\nthese to the unique context of AI-generated music. A significant challenge we\naddress is the attribution of AI-generated music to influential copyrighted\ncontent in the training data. To this end, we present algorithmic solutions\nemploying data attribution techniques. Our experimental results verify the\neffectiveness of these solutions. This research represents a pioneering effort\nin integrating technical advancements with economic and legal considerations in\nthe field of generative AI, offering a computational copyright solution for the\nchallenges posed by the opaque nature of AI technologies.",
        "date": "2023-12-11T18:57:20+00:00",
        "link": "http://arxiv.org/pdf/2312.06646v1"
    },
    {
        "title": "AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes",
        "authors": [
            "Zehao Wen",
            "Zichen Liu",
            "Srinath Sridhar",
            "Rao Fu"
        ],
        "abstract": "We introduce AnyHome, a framework that translates open-vocabulary\ndescriptions, ranging from simple labels to elaborate paragraphs, into\nwell-structured and textured 3D indoor scenes at a house-scale. Inspired by\ncognition theories, AnyHome employs an amodal structured representation to\ncapture 3D spatial cues from textual narratives and then uses egocentric\ninpainting to enrich these scenes. To this end, we begin by using specially\ndesigned template prompts for Large Language Models (LLMs), which enable\nprecise control over the textual input. We then utilize intermediate\nrepresentations to maintain the spatial structure's consistency, ensuring that\nthe 3D scenes align closely with the textual description. Then, we apply a\nScore Distillation Sampling process to refine the placement of objects. Lastly,\nan egocentric inpainting process is incorporated to enhance the realism and\nappearance of the scenes. AnyHome stands out due to its hierarchical structured\nrepresentation combined with the versatility of open-vocabulary text\ninterpretation. This allows for extensive customization of indoor scenes at\nvarious levels of granularity. We demonstrate that AnyHome can reliably\ngenerate a range of diverse indoor scenes, characterized by their detailed\nspatial structures and textures, all corresponding to the free-form textual\ninputs.",
        "date": "2023-12-11T18:56:37+00:00",
        "link": "http://arxiv.org/pdf/2312.06644v1"
    },
    {
        "title": "Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration",
        "authors": [
            "Pooja Prajod",
            "Matteo Lavit Nicora",
            "Marta Mondellini",
            "Giovanni Tauro",
            "Rocco Vertechy",
            "Matteo Malosio",
            "Elisabeth André"
        ],
        "abstract": "Collaborative robots (cobots) are widely used in industrial applications, yet\nextensive research is still needed to enhance human-robot collaborations and\noperator experience. A potential approach to improve the collaboration\nexperience involves adapting cobot behavior based on natural cues from the\noperator. Inspired by the literature on human-human interactions, we conducted\na wizard-of-oz study to examine whether a gaze towards the cobot can serve as a\ntrigger for initiating joint activities in collaborative sessions. In this\nstudy, 37 participants engaged in an assembly task while their gaze behavior\nwas analyzed. We employ a gaze-based attention recognition model to identify\nwhen the participants look at the cobot. Our results indicate that in most\ncases (84.88\\%), the joint activity is preceded by a gaze towards the cobot.\nFurthermore, during the entire assembly cycle, the participants tend to look at\nthe cobot around the time of the joint activity. To the best of our knowledge,\nthis is the first study to analyze the natural gaze behavior of participants\nworking on a joint activity with a robot during a collaborative assembly task.",
        "date": "2023-12-11T18:56:03+00:00",
        "link": "http://arxiv.org/pdf/2312.06643v1"
    },
    {
        "title": "Harmonic Mobile Manipulation",
        "authors": [
            "Ruihan Yang",
            "Yejin Kim",
            "Aniruddha Kembhavi",
            "Xiaolong Wang",
            "Kiana Ehsani"
        ],
        "abstract": "Recent advancements in robotics have enabled robots to navigate complex\nscenes or manipulate diverse objects independently. However, robots are still\nimpotent in many household tasks requiring coordinated behaviors such as\nopening doors. The factorization of navigation and manipulation, while\neffective for some tasks, fails in scenarios requiring coordinated actions. To\naddress this challenge, we introduce, HarmonicMM, an end-to-end learning method\nthat optimizes both navigation and manipulation, showing notable improvement\nover existing techniques in everyday tasks. This approach is validated in\nsimulated and real-world environments and adapts to novel unseen settings\nwithout additional tuning. Our contributions include a new benchmark for mobile\nmanipulation and the successful deployment in a real unseen apartment,\ndemonstrating the potential for practical indoor robot deployment in daily\nlife. More results are on our project site:\nhttps://rchalyang.github.io/HarmonicMM/",
        "date": "2023-12-11T18:54:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06639v1"
    },
    {
        "title": "SurvBeNIM: The Beran-Based Neural Importance Model for Explaining the Survival Models",
        "authors": [
            "Lev V. Utkin",
            "Danila Y. Eremenko",
            "Andrei V. Konstantinov"
        ],
        "abstract": "A new method called the Survival Beran-based Neural Importance Model\n(SurvBeNIM) is proposed. It aims to explain predictions of machine learning\nsurvival models, which are in the form of survival or cumulative hazard\nfunctions. The main idea behind SurvBeNIM is to extend the Beran estimator by\nincorporating the importance functions into its kernels and by implementing\nthese importance functions as a set of neural networks which are jointly\ntrained in an end-to-end manner. Two strategies of using and training the whole\nneural network implementing SurvBeNIM are proposed. The first one explains a\nsingle instance, and the neural network is trained for each explained instance.\nAccording to the second strategy, the neural network only learns once on all\ninstances from the dataset and on all generated instances. Then the neural\nnetwork is used to explain any instance in a dataset domain. Various numerical\nexperiments compare the method with different existing explanation methods. A\ncode implementing the proposed method is publicly available.",
        "date": "2023-12-11T18:54:26+00:00",
        "link": "http://arxiv.org/pdf/2312.06638v1"
    },
    {
        "title": "Examining the Effect of Implementation Factors on Deep Learning Reproducibility",
        "authors": [
            "Kevin Coakley",
            "Christine R. Kirkpatrick",
            "Odd Erik Gundersen"
        ],
        "abstract": "Reproducing published deep learning papers to validate their conclusions can\nbe difficult due to sources of irreproducibility. We investigate the impact\nthat implementation factors have on the results and how they affect\nreproducibility of deep learning studies. Three deep learning experiments were\nran five times each on 13 different hardware environments and four different\nsoftware environments. The analysis of the 780 combined results showed that\nthere was a greater than 6% accuracy range on the same deterministic examples\nintroduced from hardware or software environment variations alone. To account\nfor these implementation factors, researchers should run their experiments\nmultiple times in different hardware and software environments to verify their\nconclusions are not affected.",
        "date": "2023-12-11T18:51:13+00:00",
        "link": "http://arxiv.org/pdf/2312.06633v1"
    },
    {
        "title": "Control Risk for Potential Misuse of Artificial Intelligence in Science",
        "authors": [
            "Jiyan He",
            "Weitao Feng",
            "Yaosen Min",
            "Jingwei Yi",
            "Kunsheng Tang",
            "Shuai Li",
            "Jie Zhang",
            "Kejiang Chen",
            "Wenbo Zhou",
            "Xing Xie",
            "Weiming Zhang",
            "Nenghai Yu",
            "Shuxin Zheng"
        ],
        "abstract": "The expanding application of Artificial Intelligence (AI) in scientific\nfields presents unprecedented opportunities for discovery and innovation.\nHowever, this growth is not without risks. AI models in science, if misused,\ncan amplify risks like creation of harmful substances, or circumvention of\nestablished regulations. In this study, we aim to raise awareness of the\ndangers of AI misuse in science, and call for responsible AI development and\nuse in this domain. We first itemize the risks posed by AI in scientific\ncontexts, then demonstrate the risks by highlighting real-world examples of\nmisuse in chemical science. These instances underscore the need for effective\nrisk management strategies. In response, we propose a system called SciGuard to\ncontrol misuse risks for AI models in science. We also propose a red-teaming\nbenchmark SciMT-Safety to assess the safety of different systems. Our proposed\nSciGuard shows the least harmful impact in the assessment without compromising\nperformance in benign tests. Finally, we highlight the need for a\nmultidisciplinary and collaborative effort to ensure the safe and ethical use\nof AI models in science. We hope that our study can spark productive\ndiscussions on using AI ethically in science among researchers, practitioners,\npolicymakers, and the public, to maximize benefits and minimize the risks of\nmisuse.",
        "date": "2023-12-11T18:50:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06632v1"
    },
    {
        "title": "Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops",
        "authors": [
            "Aditya Prakash",
            "Arjun Gupta",
            "Saurabh Gupta"
        ],
        "abstract": "Objects undergo varying amounts of perspective distortion as they move across\na camera's field of view. Models for predicting 3D from a single image often\nwork with crops around the object of interest and ignore the location of the\nobject in the camera's field of view. We note that ignoring this location\ninformation further exaggerates the inherent ambiguity in making 3D inferences\nfrom 2D images and can prevent models from even fitting to the training data.\nTo mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding\n(KPE), which incorporates information about the location of crops in the image\nand camera intrinsics. Experiments on three popular 3D-from-a-single-image\nbenchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes,\nand predicting 3D shapes of articulated objects on ARCTIC, show the benefits of\nKPE.",
        "date": "2023-12-11T18:28:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06594v1"
    },
    {
        "title": "3D Hand Pose Estimation in Egocentric Images in the Wild",
        "authors": [
            "Aditya Prakash",
            "Ruisen Tu",
            "Matthew Chang",
            "Saurabh Gupta"
        ],
        "abstract": "We present WildHands, a method for 3D hand pose estimation in egocentric\nimages in the wild. This is challenging due to (a) lack of 3D hand pose\nannotations for images in the wild, and (b) a form of perspective\ndistortion-induced shape ambiguity that arises in the analysis of crops around\nhands. For the former, we use auxiliary supervision on in-the-wild data in the\nform of segmentation masks & grasp labels in addition to 3D supervision\navailable in lab datasets. For the latter, we provide spatial cues about the\nlocation of the hand crop in the camera's field of view. Our approach achieves\nthe best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, a\npopular and robust approach for estimating hand pose in the wild, by 45.3% when\nevaluated on 2D hand pose on our EPIC-HandKps dataset.",
        "date": "2023-12-11T18:15:47+00:00",
        "link": "http://arxiv.org/pdf/2312.06583v1"
    },
    {
        "title": "Grokking Group Multiplication with Cosets",
        "authors": [
            "Dashiell Stander",
            "Qinan Yu",
            "Honglu Fan",
            "Stella Biderman"
        ],
        "abstract": "We use the group Fourier transform over the symmetric group $S_n$ to reverse\nengineer a 1-layer feedforward network that has \"grokked\" the multiplication of\n$S_5$ and $S_6$. Each model discovers the true subgroup structure of the full\ngroup and converges on circuits that decompose the group multiplication into\nthe multiplication of the group's conjugate subgroups. We demonstrate the value\nof using the symmetries of the data and models to understand their mechanisms\nand hold up the ``coset circuit'' that the model uses as a fascinating example\nof the way neural networks implement computations. We also draw attention to\ncurrent challenges in conducting mechanistic interpretability research by\ncomparing our work to Chughtai et al. [6] which alleges to find a different\nalgorithm for this same problem.",
        "date": "2023-12-11T18:12:18+00:00",
        "link": "http://arxiv.org/pdf/2312.06581v1"
    },
    {
        "title": "Amazon Locker Capacity Management",
        "authors": [
            "Samyukta Sethuraman",
            "Ankur Bansal",
            "Setareh Mardan",
            "Mauricio G. C. Resende",
            "Timothy L. Jacobs"
        ],
        "abstract": "Amazon Locker is a self-service delivery or pickup location where customers\ncan pick up packages and drop off returns. A basic first-come-first-served\npolicy for accepting package delivery requests to lockers results in lockers\nbecoming full with standard shipping speed (3-5 day shipping) packages, and\nleaving no space left for expedited packages which are mostly Next-Day or\nTwo-Day shipping. This paper proposes a solution to the problem of determining\nhow much locker capacity to reserve for different ship-option packages. Yield\nmanagement is a much researched field with popular applications in the airline,\ncar rental, and hotel industries. However, Amazon Locker poses a unique\nchallenge in this field since the number of days a package will wait in a\nlocker (package dwell time) is, in general, unknown. The proposed solution\ncombines machine learning techniques to predict locker demand and package dwell\ntime, and linear programming to maximize throughput in lockers. The decision\nvariables from this optimization provide optimal capacity reservation values\nfor different ship options. This resulted in a year-over-year increase of 9% in\nLocker throughput worldwide during holiday season of 2018, impacting millions\nof customers.",
        "date": "2023-12-11T18:10:08+00:00",
        "link": "http://arxiv.org/pdf/2312.06579v1"
    },
    {
        "title": "Sparse but Strong: Crafting Adversarially Robust Graph Lottery Tickets",
        "authors": [
            "Subhajit Dutta Chowdhury",
            "Zhiyu Ni",
            "Qingyuan Peng",
            "Souvik Kundu",
            "Pierluigi Nuzzo"
        ],
        "abstract": "Graph Lottery Tickets (GLTs), comprising a sparse adjacency matrix and a\nsparse graph neural network (GNN), can significantly reduce the inference\nlatency and compute footprint compared to their dense counterparts. Despite\nthese benefits, their performance against adversarial structure perturbations\nremains to be fully explored. In this work, we first investigate the resilience\nof GLTs against different structure perturbation attacks and observe that they\nare highly vulnerable and show a large drop in classification accuracy. Based\non this observation, we then present an adversarially robust graph\nsparsification (ARGS) framework that prunes the adjacency matrix and the GNN\nweights by optimizing a novel loss function capturing the graph homophily\nproperty and information associated with both the true labels of the train\nnodes and the pseudo labels of the test nodes. By iteratively applying ARGS to\nprune both the perturbed graph adjacency matrix and the GNN model weights, we\ncan find adversarially robust graph lottery tickets that are highly sparse yet\nachieve competitive performance under different untargeted training-time\nstructure attacks. Evaluations conducted on various benchmarks, considering\ndifferent poisoning structure attacks, namely, PGD, MetaAttack, Meta-PGD, and\nPR-BCD demonstrate that the GLTs generated by ARGS can significantly improve\nthe robustness, even when subjected to high levels of sparsity.",
        "date": "2023-12-11T17:52:46+00:00",
        "link": "http://arxiv.org/pdf/2312.06568v1"
    },
    {
        "title": "Promoting Counterfactual Robustness through Diversity",
        "authors": [
            "Francesco Leofante",
            "Nico Potyka"
        ],
        "abstract": "Counterfactual explanations shed light on the decisions of black-box models\nby explaining how an input can be altered to obtain a favourable decision from\nthe model (e.g., when a loan application has been rejected). However, as noted\nrecently, counterfactual explainers may lack robustness in the sense that a\nminor change in the input can cause a major change in the explanation. This can\ncause confusion on the user side and open the door for adversarial attacks. In\nthis paper, we study some sources of non-robustness. While there are\nfundamental reasons for why an explainer that returns a single counterfactual\ncannot be robust in all instances, we show that some interesting robustness\nguarantees can be given by reporting multiple rather than a single\ncounterfactual. Unfortunately, the number of counterfactuals that need to be\nreported for the theoretical guarantees to hold can be prohibitively large. We\ntherefore propose an approximation algorithm that uses a diversity criterion to\nselect a feasible number of most relevant explanations and study its robustness\nempirically. Our experiments indicate that our method improves the\nstate-of-the-art in generating robust explanations, while maintaining other\ndesirable properties and providing competitive computational performance.",
        "date": "2023-12-11T17:49:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06564v1"
    },
    {
        "title": "On Meta-Prompting",
        "authors": [
            "Adrian de Wynter",
            "Xun Wang",
            "Qilong Gu",
            "Si-Qing Chen"
        ],
        "abstract": "Certain statistical models are capable of interpreting input strings as\ninstructions, or prompts, and carry out tasks based on them. Many approaches to\nprompting and pre-training these models involve the automated generation of\nthese prompts. We call these approaches meta-prompting, or prompting to obtain\nprompts. We propose a theoretical framework based on category theory to\ngeneralize and describe them. This framework is flexible enough to account for\nLLM stochasticity; and allows us to obtain formal results around task\nagnosticity and equivalence of various meta-prompting approaches. We experiment\nwith meta-prompting in two active areas of model research: creativity and\nideation. We find that user preference favors (p < 0.01) the prompts generated\nunder meta-prompting, as well as their corresponding outputs, over a series of\nhardcoded baseline prompts that include the original task prompt. Using our\nframework, we argue that meta-prompting is more effective than basic prompting\nat generating desirable outputs.",
        "date": "2023-12-11T17:46:44+00:00",
        "link": "http://arxiv.org/pdf/2312.06562v1"
    },
    {
        "title": "LLM360: Towards Fully Transparent Open-Source LLMs",
        "authors": [
            "Zhengzhong Liu",
            "Aurick Qiao",
            "Willie Neiswanger",
            "Hongyi Wang",
            "Bowen Tan",
            "Tianhua Tao",
            "Junbo Li",
            "Yuqi Wang",
            "Suqi Sun",
            "Omkar Pangarkar",
            "Richard Fan",
            "Yi Gu",
            "Victor Miller",
            "Yonghao Zhuang",
            "Guowei He",
            "Haonan Li",
            "Fajri Koto",
            "Liping Tang",
            "Nikhil Ranjan",
            "Zhiqiang Shen",
            "Xuguang Ren",
            "Roberto Iriondo",
            "Cun Mu",
            "Zhiting Hu",
            "Mark Schulze",
            "Preslav Nakov",
            "Tim Baldwin",
            "Eric P. Xing"
        ],
        "abstract": "The recent surge in open-source Large Language Models (LLMs), such as LLaMA,\nFalcon, and Mistral, provides diverse options for AI practitioners and\nresearchers. However, most LLMs have only released partial artifacts, such as\nthe final model weights or inference code, and technical reports increasingly\nlimit their scope to high-level design choices and surface statistics. These\nchoices hinder progress in the field by degrading transparency into the\ntraining of LLMs and forcing teams to rediscover many details in the training\nprocess. We present LLM360, an initiative to fully open-source LLMs, which\nadvocates for all training code and data, model checkpoints, and intermediate\nresults to be made available to the community. The goal of LLM360 is to support\nopen and collaborative AI research by making the end-to-end LLM training\nprocess transparent and reproducible by everyone. As a first step of LLM360, we\nrelease two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder,\nincluding their training code, data, intermediate checkpoints, and analyses (at\nhttps://www.llm360.ai). We are committed to continually pushing the boundaries\nof LLMs through this open-source effort. More large-scale and stronger models\nare underway and will be released in the future.",
        "date": "2023-12-11T17:39:00+00:00",
        "link": "http://arxiv.org/pdf/2312.06550v1"
    },
    {
        "title": "Unsupervised KPIs-Based Clustering of Jobs in HPC Data Centers",
        "authors": [
            "Mohamed S. Halawa",
            "Rebeca P. Díaz-Redondo",
            "Ana Fernández-Vilas"
        ],
        "abstract": "Performance analysis is an essential task in High-Performance Computing (HPC)\nsystems and it is applied for different purposes such as anomaly detection,\noptimal resource allocation, and budget planning. HPC monitoring tasks generate\na huge number of Key Performance Indicators (KPIs) to supervise the status of\nthe jobs running in these systems. KPIs give data about CPU usage, memory\nusage, network (interface) traffic, or other sensors that monitor the hardware.\nAnalyzing this data, it is possible to obtain insightful information about\nrunning jobs, such as their characteristics, performance, and failures. The\nmain contribution in this paper is to identify which metric/s (KPIs) is/are the\nmost appropriate to identify/classify different types of jobs according to\ntheir behavior in the HPC system. With this aim, we have applied different\nclustering techniques (partition and hierarchical clustering algorithms) using\na real dataset from the Galician Computation Center (CESGA). We have concluded\nthat (i) those metrics (KPIs) related to the Network (interface) traffic\nmonitoring provide the best cohesion and separation to cluster HPC jobs, and\n(ii) hierarchical clustering algorithms are the most suitable for this task.\nOur approach was validated using a different real dataset from the same HPC\ncenter.",
        "date": "2023-12-11T17:31:46+00:00",
        "link": "http://arxiv.org/pdf/2312.06546v1"
    },
    {
        "title": "KPIs-Based Clustering and Visualization of HPC jobs: a Feature Reduction Approach",
        "authors": [
            "Mohamed Soliman Halawa",
            "Rebeca P. Díaz-Redondo",
            "Ana Fernández-Vilas"
        ],
        "abstract": "High-Performance Computing (HPC) systems need to be constantly monitored to\nensure their stability. The monitoring systems collect a tremendous amount of\ndata about different parameters or Key Performance Indicators (KPIs), such as\nresource usage, IO waiting time, etc. A proper analysis of this data, usually\nstored as time series, can provide insight in choosing the right management\nstrategies as well as the early detection of issues. In this paper, we\nintroduce a methodology to cluster HPC jobs according to their KPI indicators.\nOur approach reduces the inherent high dimensionality of the collected data by\napplying two techniques to the time series: literature-based and variance-based\nfeature extraction. We also define a procedure to visualize the obtained\nclusters by combining the two previous approaches and the Principal Component\nAnalysis (PCA). Finally, we have validated our contributions on a real data set\nto conclude that those KPIs related to CPU usage provide the best cohesion and\nseparation for clustering analysis and the good results of our visualization\nmethodology.",
        "date": "2023-12-11T17:13:54+00:00",
        "link": "http://arxiv.org/pdf/2312.06534v1"
    },
    {
        "title": "Can Reinforcement Learning support policy makers? A preliminary study with Integrated Assessment Models",
        "authors": [
            "Theodore Wolf",
            "Nantas Nardelli",
            "John Shawe-Taylor",
            "Maria Perez-Ortiz"
        ],
        "abstract": "Governments around the world aspire to ground decision-making on evidence.\nMany of the foundations of policy making - e.g. sensing patterns that relate to\nsocietal needs, developing evidence-based programs, forecasting potential\noutcomes of policy changes, and monitoring effectiveness of policy programs -\nhave the potential to benefit from the use of large-scale datasets or\nsimulations together with intelligent algorithms. These could, if designed and\ndeployed in a way that is well grounded on scientific evidence, enable a more\ncomprehensive, faster, and rigorous approach to policy making. Integrated\nAssessment Models (IAM) is a broad umbrella covering scientific models that\nattempt to link main features of society and economy with the biosphere into\none modelling framework. At present, these systems are probed by policy makers\nand advisory groups in a hypothesis-driven manner. In this paper, we\nempirically demonstrate that modern Reinforcement Learning can be used to probe\nIAMs and explore the space of solutions in a more principled manner. While the\nimplication of our results are modest since the environment is simplistic, we\nbelieve that this is a stepping stone towards more ambitious use cases, which\ncould allow for effective exploration of policies and understanding of their\nconsequences and limitations.",
        "date": "2023-12-11T17:04:30+00:00",
        "link": "http://arxiv.org/pdf/2312.06527v1"
    },
    {
        "title": "Label Smoothing for Enhanced Text Sentiment Classification",
        "authors": [
            "Yijie Gao",
            "Shijing Si"
        ],
        "abstract": "Label smoothing is a widely used technique in various domains, such as image\nclassification and speech recognition, known for effectively combating model\noverfitting. However, there is few research on its application to text\nsentiment classification. To fill in the gap, this study investigates the\nimplementation of label smoothing for sentiment classification by utilizing\ndifferent levels of smoothing. The primary objective is to enhance sentiment\nclassification accuracy by transforming discrete labels into smoothed label\ndistributions. Through extensive experiments, we demonstrate the superior\nperformance of label smoothing in text sentiment classification tasks across\neight diverse datasets and deep learning architectures: TextCNN, BERT, and\nRoBERTa, under two learning schemes: training from scratch and fine-tuning.",
        "date": "2023-12-11T17:00:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06522v1"
    },
    {
        "title": "A GAN Approach for Node Embedding in Heterogeneous Graphs Using Subgraph Sampling",
        "authors": [
            "Hung Chun Hsu",
            "Bo-Jun Wu",
            "Ming-Yi Hong",
            "Che Lin",
            "Chih-Yu Wang"
        ],
        "abstract": "Our research addresses class imbalance issues in heterogeneous graphs using\ngraph neural networks (GNNs). We propose a novel method combining the strengths\nof Generative Adversarial Networks (GANs) with GNNs, creating synthetic nodes\nand edges that effectively balance the dataset. This approach directly targets\nand rectifies imbalances at the data level. The proposed framework resolves\nissues such as neglecting graph structures during data generation and creating\nsynthetic structures usable with GNN-based classifiers in downstream tasks. It\nprocesses node and edge information concurrently, improving edge balance\nthrough node augmentation and subgraph sampling. Additionally, our framework\nintegrates a threshold strategy, aiding in determining optimal edge thresholds\nduring training without time-consuming parameter adjustments. Experiments on\nthe Amazon and Yelp Review datasets highlight the effectiveness of the\nframework we proposed, especially in minority node identification, where it\nconsistently outperforms baseline models across key performance metrics,\ndemonstrating its potential in the field.",
        "date": "2023-12-11T16:52:20+00:00",
        "link": "http://arxiv.org/pdf/2312.06519v1"
    },
    {
        "title": "Where exactly does contextualization in a PLM happen?",
        "authors": [
            "Soniya Vijayakumar",
            "Tanja Bäumel",
            "Simon Ostermann",
            "Josef van Genabith"
        ],
        "abstract": "Pre-trained Language Models (PLMs) have shown to be consistently successful\nin a plethora of NLP tasks due to their ability to learn contextualized\nrepresentations of words (Ethayarajh, 2019). BERT (Devlin et al., 2018), ELMo\n(Peters et al., 2018) and other PLMs encode word meaning via textual context,\nas opposed to static word embeddings, which encode all meanings of a word in a\nsingle vector representation. In this work, we present a study that aims to\nlocalize where exactly in a PLM word contextualization happens. In order to\nfind the location of this word meaning transformation, we investigate\nrepresentations of polysemous words in the basic BERT uncased 12 layer\narchitecture (Devlin et al., 2018), a masked language model trained on an\nadditional sentence adjacency objective, using qualitative and quantitative\nmeasures.",
        "date": "2023-12-11T16:39:52+00:00",
        "link": "http://arxiv.org/pdf/2312.06514v1"
    },
    {
        "title": "Automated Planning Techniques for Elementary Proofs in Abstract Algebra",
        "authors": [
            "Alice Petrov",
            "Christian Muise"
        ],
        "abstract": "This paper explores the application of automated planning to automated\ntheorem proving, which is a branch of automated reasoning concerned with the\ndevelopment of algorithms and computer programs to construct mathematical\nproofs. In particular, we investigate the use of planning to construct\nelementary proofs in abstract algebra, which provides a rigorous and axiomatic\nframework for studying algebraic structures such as groups, rings, fields, and\nmodules. We implement basic implications, equalities, and rules in both\ndeterministic and non-deterministic domains to model commutative rings and\ndeduce elementary results about them. The success of this initial\nimplementation suggests that the well-established techniques seen in automated\nplanning are applicable to the relatively newer field of automated theorem\nproving. Likewise, automated theorem proving provides a new, challenging domain\nfor automated planning.",
        "date": "2023-12-11T16:17:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06490v1"
    },
    {
        "title": "Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation",
        "authors": [
            "Qi Yang",
            "Xing Nie",
            "Tong Li",
            "Pengfei Gao",
            "Ying Guo",
            "Cheng Zhen",
            "Pengfei Yan",
            "Shiming Xiang"
        ],
        "abstract": "Recently, an audio-visual segmentation (AVS) task has been introduced, aiming\nto group pixels with sounding objects within a given video. This task\nnecessitates a first-ever audio-driven pixel-level understanding of the scene,\nposing significant challenges. In this paper, we propose an innovative\naudio-visual transformer framework, termed COMBO, an acronym for COoperation of\nMulti-order Bilateral relatiOns. For the first time, our framework explores\nthree types of bilateral entanglements within AVS: pixel entanglement, modality\nentanglement, and temporal entanglement. Regarding pixel entanglement, we\nemploy a Siam-Encoder Module (SEM) that leverages prior knowledge to generate\nmore precise visual features from the foundational model. For modality\nentanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to\nalign corresponding visual and auditory signals bi-directionally. As for\ntemporal entanglement, we introduce an innovative adaptive inter-frame\nconsistency loss according to the inherent rules of temporal. Comprehensive\nexperiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou\non MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that\nCOMBO surpasses previous state-of-the-art methods. Code and more results will\nbe publicly available at https://combo-avs.github.io/.",
        "date": "2023-12-11T15:51:38+00:00",
        "link": "http://arxiv.org/pdf/2312.06462v1"
    },
    {
        "title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping",
        "authors": [
            "Will E. Thompson",
            "David M. Vidmar",
            "Jessica K. De Freitas",
            "John M. Pfeifer",
            "Brandon K. Fornwalt",
            "Ruijun Chen",
            "Gabriel Altay",
            "Kabir Manghnani",
            "Andrew C. Nelsen",
            "Kellie Morland",
            "Martin C. Stumpe",
            "Riccardo Miotto"
        ],
        "abstract": "Identifying disease phenotypes from electronic health records (EHRs) is\ncritical for numerous secondary uses. Manually encoding physician knowledge\ninto rules is particularly challenging for rare diseases due to inadequate EHR\ncoding, necessitating review of clinical notes. Large language models (LLMs)\noffer promise in text understanding but may not efficiently handle real-world\nclinical documentation. We propose a zero-shot LLM-based method enriched by\nretrieval-augmented generation and MapReduce, which pre-identifies\ndisease-related text snippets to be used in parallel as queries for the LLM to\nestablish diagnosis. We show that this method as applied to pulmonary\nhypertension (PH), a rare disease characterized by elevated arterial pressures\nin the lungs, significantly outperforms physician logic rules ($F_1$ score of\n0.62 vs. 0.75). This method has the potential to enhance rare disease cohort\nidentification, expanding the scope of robust clinical research and care gap\nidentification.",
        "date": "2023-12-11T15:45:27+00:00",
        "link": "http://arxiv.org/pdf/2312.06457v1"
    },
    {
        "title": "Revisiting Graph-based Fraud Detection in Sight of Heterophily and Spectrum",
        "authors": [
            "Fan Xu",
            "Nan Wang",
            "Hao Wu",
            "Xuezhi Wen",
            "Xibin Zhao"
        ],
        "abstract": "Graph-based fraud detection (GFD) can be regarded as a challenging\nsemi-supervised node binary classification task. In recent years, Graph Neural\nNetworks(GNN) have been widely applied to GFD, characterizing the anomalous\npossibility of a node by aggregating neighbor information. However, fraud\ngraphs are inherently heterophilic, thus most of GNNs perform poorly due to\ntheir assumption of homophily. In addition, due to the existence of heterophily\nand class imbalance problem, the existing models do not fully utilize the\nprecious node label information. To address the above issues, this paper\nproposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector\nincludes a hybrid filtering module and a local environmental constraint module,\nthe two modules are utilized to solve heterophily and label utilization problem\nrespectively. The first module starts from the perspective of the spectral\ndomain, and solves the heterophily problem to a certain extent. Specifically,\nit divides the spectrum into multiple mixed frequency bands according to the\ncorrelation between spectrum energy distribution and heterophily. Then in order\nto make full use of the node label information, a local environmental\nconstraint module is adaptively designed. The comprehensive experimental\nresults on four real-world fraud detection datasets show that SEC-GFD\noutperforms other competitive graph-based fraud detectors.",
        "date": "2023-12-11T15:18:51+00:00",
        "link": "http://arxiv.org/pdf/2312.06441v1"
    },
    {
        "title": "Reward Certification for Policy Smoothed Reinforcement Learning",
        "authors": [
            "Ronghui Mu",
            "Leandro Soriano Marcolino",
            "Tianle Zhang",
            "Yanghao Zhang",
            "Xiaowei Huang",
            "Wenjie Ruan"
        ],
        "abstract": "Reinforcement Learning (RL) has achieved remarkable success in\nsafety-critical areas, but it can be weakened by adversarial attacks. Recent\nstudies have introduced \"smoothed policies\" in order to enhance its robustness.\nYet, it is still challenging to establish a provable guarantee to certify the\nbound of its total reward. Prior methods relied primarily on computing bounds\nusing Lipschitz continuity or calculating the probability of cumulative reward\nabove specific thresholds. However, these techniques are only suited for\ncontinuous perturbations on the RL agent's observations and are restricted to\nperturbations bounded by the l_2-norm. To address these limitations, this paper\nproposes a general black-box certification method capable of directly\ncertifying the cumulative reward of the smoothed policy under various\n$l_p$-norm bounded perturbations. Furthermore, we extend our methodology to\ncertify perturbations on action spaces. Our approach leverages f-divergence to\nmeasure the distinction between the original distribution and the perturbed\ndistribution, subsequently determining the certification bound by solving a\nconvex optimisation problem. We provide a comprehensive theoretical analysis\nand run sufficient experiments in multiple environments. Our results show that\nour method not only improves the certified lower bound of mean cumulative\nreward but also demonstrates better efficiency than state-of-the-art\ntechniques.",
        "date": "2023-12-11T15:07:58+00:00",
        "link": "http://arxiv.org/pdf/2312.06436v1"
    },
    {
        "title": "Internet of Federated Digital Twins (IoFDT): Connecting Twins Beyond Borders for Society 5.0",
        "authors": [
            "Tao Yu",
            "Zongdian Li",
            "Kei Sakaguchi",
            "Omar Hashash",
            "Walid Saad",
            "Merouane Debbah"
        ],
        "abstract": "The concept of digital twin (DT), which enables the creation of a\nprogrammable, digital representation of physical systems, is expected to\nrevolutionize future industries and will lie at the heart of the vision of a\nfuture smart society, namely, Society 5.0, in which high integration between\ncyber (digital) and physical spaces is exploited to bring economic and societal\nadvancements. However, the success of such a DT-driven Society 5.0 requires a\nsynergistic convergence of artificial intelligence and networking technologies\ninto an integrated, programmable system that can coordinate networks of DTs to\neffectively deliver diverse Society 5.0 services. Prior works remain restricted\nto either qualitative study, simple analysis or software implementations of a\nsingle DT, and thus, they cannot provide the highly synergistic integration of\ndigital and physical spaces as required by Society 5.0. In contrast, this paper\nenvisions a novel concept of an Internet of Federated Digital Twins (IoFDT)\nthat holistically integrates heterogeneous and physically separated DTs\nrepresenting different Society 5.0 services within a single framework and\nsystem. For this concept of IoFDT, we first introduce a hierarchical\narchitecture that integrates federated DTs through horizontal and vertical\ninteractions, bridging the cyber and physical spaces to unlock new\npossibilities. Then, we discuss the challenges of realizing IoFDT, highlighting\nthe intricacies across communication, computing, and AI-native networks while\nalso underscoring potential innovative solutions. Subsequently, we elaborate on\nthe importance of the implementation of a unified IoFDT platform that\nintegrates all technical components and orchestrates their interactions,\nemphasizing the necessity of practical experimental platforms with a focus on\nreal-world applications in areas like smart mobility.",
        "date": "2023-12-11T14:56:27+00:00",
        "link": "http://arxiv.org/pdf/2312.06432v1"
    },
    {
        "title": "VisionTraj: A Noise-Robust Trajectory Recovery Framework based on Large-scale Camera Network",
        "authors": [
            "Zhishuai Li",
            "Ziyue Li",
            "Xiaoru Hu",
            "Guoqing Du",
            "Yunhao Nie",
            "Feng Zhu",
            "Lei Bai",
            "Rui Zhao"
        ],
        "abstract": "Trajectory recovery based on the snapshots from the city-wide multi-camera\nnetwork facilitates urban mobility sensing and driveway optimization. The\nstate-of-the-art solutions devoted to such a vision-based scheme typically\nincorporate predefined rules or unsupervised iterative feedback, struggling\nwith multi-fold challenges such as lack of open-source datasets for training\nthe whole pipeline, and the vulnerability to the noises from visual inputs. In\nresponse to the dilemma, this paper proposes VisionTraj, the first\nlearning-based model that reconstructs vehicle trajectories from snapshots\nrecorded by road network cameras. Coupled with it, we elaborate on two rational\nvision-trajectory datasets, which produce extensive trajectory data along with\ncorresponding visual snapshots, enabling supervised vision-trajectory interplay\nextraction. Following the data creation, based on the results from the\noff-the-shelf multi-modal vehicle clustering, we first re-formulate the\ntrajectory recovery problem as a generative task and introduce the canonical\nTransformer as the autoregressive backbone. Then, to identify clustering noises\n(e.g., false positives) with the bound on the snapshots' spatiotemporal\ndependencies, a GCN-based soft-denoising module is conducted based on the fine-\nand coarse-grained Re-ID clusters. Additionally, we harness strong semantic\ninformation extracted from the tracklet to provide detailed insights into the\nvehicle's entry and exit actions during trajectory recovery. The denoising and\ntracklet components can also act as plug-and-play modules to boost baselines.\nExperimental results on the two hand-crafted datasets show that the proposed\nVisionTraj achieves a maximum +11.5% improvement against the sub-best model.",
        "date": "2023-12-11T14:52:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06428v1"
    },
    {
        "title": "MalPurifier: Enhancing Android Malware Detection with Adversarial Purification against Evasion Attacks",
        "authors": [
            "Yuyang Zhou",
            "Guang Cheng",
            "Zongyao Chen",
            "Shui Yu"
        ],
        "abstract": "Machine learning (ML) has gained significant adoption in Android malware\ndetection to address the escalating threats posed by the rapid proliferation of\nmalware attacks. However, recent studies have revealed the inherent\nvulnerabilities of ML-based detection systems to evasion attacks. While efforts\nhave been made to address this critical issue, many of the existing defensive\nmethods encounter challenges such as lower effectiveness or reduced\ngeneralization capabilities. In this paper, we introduce a novel Android\nmalware detection method, MalPurifier, which exploits adversarial purification\nto eliminate perturbations independently, resulting in attack mitigation in a\nlight and flexible way. Specifically, MalPurifier employs a Denoising\nAutoEncoder (DAE)-based purification model to preprocess input samples,\nremoving potential perturbations from them and then leading to correct\nclassification. To enhance defense effectiveness, we propose a diversified\nadversarial perturbation mechanism that strengthens the purification model\nagainst different manipulations from various evasion attacks. We also\nincorporate randomized \"protective noises\" onto benign samples to prevent\nexcessive purification. Furthermore, we customize a loss function for improving\nthe DAE model, combining reconstruction loss and prediction loss, to enhance\nfeature representation learning, resulting in accurate reconstruction and\nclassification. Experimental results on two Android malware datasets\ndemonstrate that MalPurifier outperforms the state-of-the-art defenses, and it\nsignificantly strengthens the vulnerable malware detector against 37 evasion\nattacks, achieving accuracies over 90.91%. Notably, MalPurifier demonstrates\neasy scalability to other detectors, offering flexibility and robustness in its\nimplementation.",
        "date": "2023-12-11T14:48:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06423v1"
    },
    {
        "title": "DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics",
        "authors": [
            "Zhiao Huang",
            "Feng Chen",
            "Yewen Pu",
            "Chunru Lin",
            "Hao Su",
            "Chuang Gan"
        ],
        "abstract": "Combining gradient-based trajectory optimization with differentiable physics\nsimulation is an efficient technique for solving soft-body manipulation\nproblems. Using a well-crafted optimization objective, the solver can quickly\nconverge onto a valid trajectory. However, writing the appropriate objective\nfunctions requires expert knowledge, making it difficult to collect a large set\nof naturalistic problems from non-expert users. We introduce DiffVL, a method\nthat enables non-expert users to communicate soft-body manipulation tasks -- a\ncombination of vision and natural language, given in multiple stages -- that\ncan be readily leveraged by a differential physics solver. We have developed\nGUI tools that enable non-expert users to specify 100 tasks inspired by\nreal-life soft-body manipulations from online videos, which we'll make public.\nWe leverage large language models to translate task descriptions into\nmachine-interpretable optimization objectives. The optimization objectives can\nhelp differentiable physics solvers to solve these long-horizon multistage\ntasks that are challenging for previous baselines.",
        "date": "2023-12-11T14:29:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06408v1"
    },
    {
        "title": "Partial End-to-end Reinforcement Learning for Robustness Against Modelling Error in Autonomous Racing",
        "authors": [
            "Andrew Murdoch",
            "Johannes Cornelius Schoeman",
            "Hendrik Willem Jordaan"
        ],
        "abstract": "In this paper, we address the issue of increasing the performance of\nreinforcement learning (RL) solutions for autonomous racing cars when\nnavigating under conditions where practical vehicle modelling errors (commonly\nknown as \\emph{model mismatches}) are present. To address this challenge, we\npropose a partial end-to-end algorithm that decouples the planning and control\ntasks. Within this framework, an RL agent generates a trajectory comprising a\npath and velocity, which is subsequently tracked using a pure pursuit steering\ncontroller and a proportional velocity controller, respectively. In contrast,\nmany current learning-based (i.e., reinforcement and imitation learning)\nalgorithms utilise an end-to-end approach whereby a deep neural network\ndirectly maps from sensor data to control commands. By leveraging the\nrobustness of a classical controller, our partial end-to-end driving algorithm\nexhibits better robustness towards model mismatches than standard end-to-end\nalgorithms.",
        "date": "2023-12-11T14:27:10+00:00",
        "link": "http://arxiv.org/pdf/2312.06406v1"
    },
    {
        "title": "DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers",
        "authors": [
            "Aaron Mir",
            "Eduardo Alonso",
            "Esther Mondragón"
        ],
        "abstract": "We propose a novel talking head synthesis pipeline called \"DiT-Head\", which\nis based on diffusion transformers and uses audio as a condition to drive the\ndenoising process of a diffusion model. Our method is scalable and can\ngeneralise to multiple identities while producing high-quality results. We\ntrain and evaluate our proposed approach and compare it against existing\nmethods of talking head synthesis. We show that our model can compete with\nthese methods in terms of visual quality and lip-sync accuracy. Our results\nhighlight the potential of our proposed approach to be used for a wide range of\napplications, including virtual assistants, entertainment, and education. For a\nvideo demonstration of the results and our user study, please refer to our\nsupplementary material.",
        "date": "2023-12-11T14:09:56+00:00",
        "link": "http://arxiv.org/pdf/2312.06400v1"
    },
    {
        "title": "NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos",
        "authors": [
            "Jinxi Li",
            "Ziyang Song",
            "Bo Yang"
        ],
        "abstract": "In this paper, we aim to model 3D scene dynamics from multi-view videos.\nUnlike the majority of existing works which usually focus on the common task of\nnovel view synthesis within the training time period, we propose to\nsimultaneously learn the geometry, appearance, and physical velocity of 3D\nscenes only from video frames, such that multiple desirable applications can be\nsupported, including future frame extrapolation, unsupervised 3D semantic scene\ndecomposition, and dynamic motion transfer. Our method consists of three major\ncomponents, 1) the keyframe dynamic radiance field, 2) the interframe velocity\nfield, and 3) a joint keyframe and interframe optimization module which is the\ncore of our framework to effectively train both networks. To validate our\nmethod, we further introduce two dynamic 3D datasets: 1) Dynamic Object\ndataset, and 2) Dynamic Indoor Scene dataset. We conduct extensive experiments\non multiple datasets, demonstrating the superior performance of our method over\nall baselines, particularly in the critical tasks of future frame extrapolation\nand unsupervised 3D semantic scene decomposition.",
        "date": "2023-12-11T14:07:31+00:00",
        "link": "http://arxiv.org/pdf/2312.06398v1"
    },
    {
        "title": "ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation",
        "authors": [
            "Cédric Rommel",
            "Victor Letzelter",
            "Nermin Samet",
            "Renaud Marlet",
            "Matthieu Cord",
            "Patrick Pérez",
            "Eduardo Valle"
        ],
        "abstract": "Monocular 3D human pose estimation (3D-HPE) is an inherently ambiguous task,\nas a 2D pose in an image might originate from different possible 3D poses. Yet,\nmost 3D-HPE methods rely on regression models, which assume a one-to-one\nmapping between inputs and outputs. In this work, we provide theoretical and\nempirical evidence that, because of this ambiguity, common regression models\nare bound to predict topologically inconsistent poses, and that traditional\nevaluation metrics, such as the MPJPE, P-MPJPE and PCK, are insufficient to\nassess this aspect. As a solution, we propose ManiPose, a novel\nmanifold-constrained multi-hypothesis model capable of proposing multiple\ncandidate 3D poses for each 2D input, together with their corresponding\nplausibility. Unlike previous multi-hypothesis approaches, our solution is\ncompletely supervised and does not rely on complex generative models, thus\ngreatly facilitating its training and usage. Furthermore, by constraining our\nmodel to lie within the human pose manifold, we can guarantee the consistency\nof all hypothetical poses predicted with our approach, which was not possible\nin previous works. We illustrate the usefulness of ManiPose in a synthetic\n1D-to-2D lifting setting and demonstrate on real-world datasets that it\noutperforms state-of-the-art models in pose consistency by a large margin,\nwhile still reaching competitive MPJPE performance.",
        "date": "2023-12-11T13:50:10+00:00",
        "link": "http://arxiv.org/pdf/2312.06386v1"
    },
    {
        "title": "BAT: Behavior-Aware Human-Like Trajectory Prediction for Autonomous Driving",
        "authors": [
            "Haicheng Liao",
            "Zhenning Li",
            "Huanming Shen",
            "Wenxuan Zeng",
            "Guofa Li",
            "Shengbo Eben Li",
            "Chengzhong Xu"
        ],
        "abstract": "The ability to accurately predict the trajectory of surrounding vehicles is a\ncritical hurdle to overcome on the journey to fully autonomous vehicles. To\naddress this challenge, we pioneer a novel behavior-aware trajectory prediction\nmodel (BAT) that incorporates insights and findings from traffic psychology,\nhuman behavior, and decision-making. Our model consists of behavior-aware,\ninteraction-aware, priority-aware, and position-aware modules that perceive and\nunderstand the underlying interactions and account for uncertainty and\nvariability in prediction, enabling higher-level learning and flexibility\nwithout rigid categorization of driving behavior. Importantly, this approach\neliminates the need for manual labeling in the training process and addresses\nthe challenges of non-continuous behavior labeling and the selection of\nappropriate time windows. We evaluate BAT's performance across the Next\nGeneration Simulation (NGSIM), Highway Drone (HighD), Roundabout Drone (RounD),\nand Macao Connected Autonomous Driving (MoCAD) datasets, showcasing its\nsuperiority over prevailing state-of-the-art (SOTA) benchmarks in terms of\nprediction accuracy and efficiency. Remarkably, even when trained on reduced\nportions of the training data (25%), our model outperforms most of the\nbaselines, demonstrating its robustness and efficiency in predicting vehicle\ntrajectories, and the potential to reduce the amount of data required to train\nautonomous vehicles, especially in corner cases. In conclusion, the\nbehavior-aware model represents a significant advancement in the development of\nautonomous vehicles capable of predicting trajectories with the same level of\nproficiency as human drivers. The project page is available at\nhttps://github.com/Petrichor625/BATraj-Behavior-aware-Model.",
        "date": "2023-12-11T13:27:51+00:00",
        "link": "http://arxiv.org/pdf/2312.06371v1"
    },
    {
        "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
        "authors": [
            "Tao Chen",
            "Enwei Zhang",
            "Yuting Gao",
            "Ke Li",
            "Xing Sun",
            "Yan Zhang",
            "Hui Li"
        ],
        "abstract": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input.",
        "date": "2023-12-11T13:11:04+00:00",
        "link": "http://arxiv.org/pdf/2312.06363v1"
    },
    {
        "title": "FOSS: A Self-Learned Doctor for Query Optimizer",
        "authors": [
            "Kai Zhong",
            "Luming Sun",
            "Tao Ji",
            "Cuiping Li",
            "Hong Chen"
        ],
        "abstract": "Various works have utilized deep reinforcement learning (DRL) to address the\nquery optimization problem in database system. They either learn to construct\nplans from scratch in a bottom-up manner or guide the plan generation behavior\nof traditional optimizer using hints. While these methods have achieved some\nsuccess, they face challenges in either low training efficiency or limited plan\nsearch space. To address these challenges, we introduce FOSS, a novel DRL-based\nframework for query optimization. FOSS initiates optimization from the original\nplan generated by a traditional optimizer and incrementally refines suboptimal\nnodes of the plan through a sequence of actions. Additionally, we devise an\nasymmetric advantage model to evaluate the advantage between two plans. We\nintegrate it with a traditional optimizer to form a simulated environment.\nLeveraging this simulated environment, FOSS can bootstrap itself to rapidly\ngenerate a large amount of high-quality simulated experiences. FOSS then learns\nand improves its optimization capability from these simulated experiences. We\nevaluate the performance of FOSS on Join Order Benchmark, TPC-DS, and Stack\nOverflow. The experimental results demonstrate that FOSS outperforms the\nstate-of-the-art methods in terms of latency performance and optimization time.\nCompared to PostgreSQL, FOSS achieves savings ranging from 15% to 83% in total\nlatency across different benchmarks.",
        "date": "2023-12-11T13:05:51+00:00",
        "link": "http://arxiv.org/pdf/2312.06357v1"
    },
    {
        "title": "Detecting Contextual Network Anomalies with Graph Neural Networks",
        "authors": [
            "Hamid Latif-Martínez",
            "José Suárez-Varela",
            "Albert Cabellos-Aparicio",
            "Pere Barlet-Ros"
        ],
        "abstract": "Detecting anomalies on network traffic is a complex task due to the massive\namount of traffic flows in today's networks, as well as the highly-dynamic\nnature of traffic over time. In this paper, we propose the use of Graph Neural\nNetworks (GNN) for network traffic anomaly detection. We formulate the problem\nas contextual anomaly detection on network traffic measurements, and propose a\ncustom GNN-based solution that detects traffic anomalies on origin-destination\nflows. In our evaluation, we use real-world data from Abilene (6 months), and\nmake a comparison with other widely used methods for the same task (PCA, EWMA,\nRNN). The results show that the anomalies detected by our solution are quite\ncomplementary to those captured by the baselines (with a max. of 36.33%\noverlapping anomalies for PCA). Moreover, we manually inspect the anomalies\ndetected by our method, and find that a large portion of them can be visually\nvalidated by a network expert (64% with high confidence, 18% with mid\nconfidence, 18% normal traffic). Lastly, we analyze the characteristics of the\nanomalies through two paradigmatic cases that are quite representative of the\nbulk of anomalies.",
        "date": "2023-12-11T12:45:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06342v1"
    },
    {
        "title": "BoschAI @ Causal News Corpus 2023: Robust Cause-Effect Span Extraction using Multi-Layer Sequence Tagging and Data Augmentation",
        "authors": [
            "Timo Pierre Schrader",
            "Simon Razniewski",
            "Lukas Lange",
            "Annemarie Friedrich"
        ],
        "abstract": "Understanding causality is a core aspect of intelligence. The Event Causality\nIdentification with Causal News Corpus Shared Task addresses two aspects of\nthis challenge: Subtask 1 aims at detecting causal relationships in texts, and\nSubtask 2 requires identifying signal words and the spans that refer to the\ncause or effect, respectively. Our system, which is based on pre-trained\ntransformers, stacked sequence tagging, and synthetic data augmentation, ranks\nthird in Subtask 1 and wins Subtask 2 with an F1 score of 72.8, corresponding\nto a margin of 13 pp. to the second-best system.",
        "date": "2023-12-11T12:35:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06338v1"
    },
    {
        "title": "Vehicle Lane Change Prediction based on Knowledge Graph Embeddings and Bayesian Inference",
        "authors": [
            "M. Manzour",
            "A. Ballardini",
            "R. Izquierdo",
            "M. A. Sotelo"
        ],
        "abstract": "Prediction of vehicle lane change maneuvers has gained a lot of momentum in\nthe last few years. Some recent works focus on predicting a vehicle's intention\nby predicting its trajectory first. This is not enough, as it ignores the\ncontext of the scene and the state of the surrounding vehicles (as they might\nbe risky to the target vehicle). Other works assessed the risk made by the\nsurrounding vehicles only by considering their existence around the target\nvehicle, or by considering the distance and relative velocities between them\nand the target vehicle as two separate numerical features. In this work, we\npropose a solution that leverages Knowledge Graphs (KGs) to anticipate lane\nchanges based on linguistic contextual information in a way that goes well\nbeyond the capabilities of current perception systems. Our solution takes the\nTime To Collision (TTC) with surrounding vehicles as input to assess the risk\non the target vehicle. Moreover, our KG is trained on the HighD dataset using\nthe TransE model to obtain the Knowledge Graph Embeddings (KGE). Then, we apply\nBayesian inference on top of the KG using the embeddings learned during\ntraining. Finally, the model can predict lane changes two seconds ahead with\n97.95% f1-score, which surpassed the state of the art, and three seconds before\nchanging lanes with 93.60% f1-score.",
        "date": "2023-12-11T12:33:44+00:00",
        "link": "http://arxiv.org/pdf/2312.06336v1"
    },
    {
        "title": "Navigating Open Set Scenarios for Skeleton-based Action Recognition",
        "authors": [
            "Kunyu Peng",
            "Cheng Yin",
            "Junwei Zheng",
            "Ruiping Liu",
            "David Schneider",
            "Jiaming Zhang",
            "Kailun Yang",
            "M. Saquib Sarfraz",
            "Rainer Stiefelhagen",
            "Alina Roitberg"
        ],
        "abstract": "In real-world scenarios, human actions often fall outside the distribution of\ntraining data, making it crucial for models to recognize known actions and\nreject unknown ones. However, using pure skeleton data in such open-set\nconditions poses challenges due to the lack of visual background cues and the\ndistinct sparse structure of body pose sequences. In this paper, we tackle the\nunexplored Open-Set Skeleton-based Action Recognition (OS-SAR) task and\nformalize the benchmark on three skeleton-based datasets. We assess the\nperformance of seven established open-set approaches on our task and identify\ntheir limits and critical generalization issues when dealing with skeleton\ninformation. To address these challenges, we propose a distance-based\ncross-modality ensemble method that leverages the cross-modal alignment of\nskeleton joints, bones, and velocities to achieve superior open-set recognition\nperformance. We refer to the key idea as CrossMax - an approach that utilizes a\nnovel cross-modality mean max discrepancy suppression mechanism to align latent\nspaces during training and a cross-modality distance-based logits refinement\nmethod during testing. CrossMax outperforms existing approaches and\nconsistently yields state-of-the-art results across all datasets and backbones.\nThe benchmark, code, and models will be released at\nhttps://github.com/KPeng9510/OS-SAR.",
        "date": "2023-12-11T12:29:32+00:00",
        "link": "http://arxiv.org/pdf/2312.06330v1"
    },
    {
        "title": "DMS*: Minimizing Makespan for Multi-Agent Combinatorial Path Finding",
        "authors": [
            "Zhongqiang Ren",
            "Anushtup Nandy",
            "Sivakumar Rathinam",
            "Howie Choset"
        ],
        "abstract": "Multi-Agent Combinatorial Path Finding (MCPF) seeks collision-free paths for\nmultiple agents from their initial to goal locations, while visiting a set of\nintermediate target locations in the middle of the paths. MCPF is challenging\nas it involves both planning collision-free paths for multiple agents and\ntarget sequencing, i.e., solving traveling salesman problems to assign targets\nto and find the visiting order for the agents. Recent work develops methods to\naddress MCPF while minimizing the sum of individual arrival times at goals.\nSuch a problem formulation may result in paths with different arrival times and\nlead to a long makespan, the maximum arrival time, among the agents. This paper\nproposes a min-max variant of MCPF, denoted as MCPF-max, that minimizes the\nmakespan of the agents. While the existing methods (such as MS*) for MCPF can\nbe adapted to solve MCPF-max, we further develop two new techniques based on\nMS* to defer the expensive target sequencing during planning to expedite the\noverall computation. We analyze the properties of the resulting algorithm\nDeferred MS* (DMS*), and test DMS* with up to 20 agents and 80 targets. We\ndemonstrate the use of DMS* on differential-drive robots.",
        "date": "2023-12-11T11:53:31+00:00",
        "link": "http://arxiv.org/pdf/2312.06314v1"
    },
    {
        "title": "Attribute Annotation and Bias Evaluation in Visual Datasets for Autonomous Driving",
        "authors": [
            "David Fernández Llorca",
            "Pedro Frau",
            "Ignacio Parra",
            "Rubén Izquierdo",
            "Emilia Gómez"
        ],
        "abstract": "This paper addresses the often overlooked issue of fairness in the autonomous\ndriving domain, particularly in vision-based perception and prediction systems,\nwhich play a pivotal role in the overall functioning of Autonomous Vehicles\n(AVs). We focus our analysis on biases present in some of the most commonly\nused visual datasets for training person and vehicle detection systems. We\nintroduce an annotation methodology and a specialised annotation tool, both\ndesigned to annotate protected attributes of agents in visual datasets. We\nvalidate our methodology through an inter-rater agreement analysis and provide\nthe distribution of attributes across all datasets. These include annotations\nfor the attributes age, sex, skin tone, group, and means of transport for more\nthan 90K people, as well as vehicle type, colour, and car type for over 50K\nvehicles. Generally, diversity is very low for most attributes, with some\ngroups, such as children, wheelchair users, or personal mobility vehicle users,\nbeing extremely underrepresented in the analysed datasets. The study\ncontributes significantly to efforts to consider fairness in the evaluation of\nperception and prediction systems for AVs. This paper follows reproducibility\nprinciples. The annotation tool, scripts and the annotated attributes can be\naccessed publicly at https://github.com/ec-jrc/humaint_annotator.",
        "date": "2023-12-11T11:27:01+00:00",
        "link": "http://arxiv.org/pdf/2312.06306v1"
    },
    {
        "title": "A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML",
        "authors": [
            "Giorgos Borboudakis",
            "Paulos Charonyktakis",
            "Konstantinos Paraschakis",
            "Ioannis Tsamardinos"
        ],
        "abstract": "AutoML platforms have numerous options for the algorithms to try for each\nstep of the analysis, i.e., different possible algorithms for imputation,\ntransformations, feature selection, and modelling. Finding the optimal\ncombination of algorithms and hyper-parameter values is computationally\nexpensive, as the number of combinations to explore leads to an exponential\nexplosion of the space. In this paper, we present the Sequential\nHyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an\nAutoML tool with negligible drop in its predictive performance. SHSR is a\nmeta-level learning algorithm that analyzes past runs of an AutoML tool on\nseveral datasets and learns which hyper-parameter values to filter out from\nconsideration on a new dataset to analyze. SHSR is evaluated on 284\nclassification and 375 regression problems, showing an approximate 30%\nreduction in execution time with a performance drop of less than 0.1%.",
        "date": "2023-12-11T11:26:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06305v1"
    },
    {
        "title": "MMDesign: Multi-Modality Transfer Learning for Generative Protein Design",
        "authors": [
            "Jiangbin Zheng",
            "Siyuan Li",
            "Yufei Huang",
            "Zhangyang Gao",
            "Cheng Tan",
            "Bozhen Hu",
            "Jun Xia",
            "Ge Wang",
            "Stan Z. Li"
        ],
        "abstract": "Protein design involves generating protein sequences based on their\ncorresponding protein backbones. While deep generative models show promise for\nlearning protein design directly from data, the lack of publicly available\nstructure-sequence pairings limits their generalization capabilities. Previous\nefforts of generative protein design have focused on architectural improvements\nand pseudo-data augmentation to overcome this bottleneck. To further address\nthis challenge, we propose a novel protein design paradigm called MMDesign,\nwhich leverages multi-modality transfer learning. To our knowledge, MMDesign is\nthe first framework that combines a pretrained structural module with a\npretrained contextual module, using an auto-encoder (AE) based language model\nto incorporate prior semantic knowledge of protein sequences. We also introduce\na cross-layer cross-modal alignment algorithm to enable the structural module\nto learn long-term temporal information and ensure consistency between\nstructural and contextual modalities. Experimental results, only training with\nthe small CATH dataset, demonstrate that our MMDesign framework consistently\noutperforms other baselines on various public test sets. To further assess the\nbiological plausibility of the generated protein sequences and data\ndistribution, we present systematic quantitative analysis techniques that\nprovide interpretability and reveal more about the laws of protein design.",
        "date": "2023-12-11T10:59:23+00:00",
        "link": "http://arxiv.org/pdf/2312.06297v1"
    },
    {
        "title": "Mobile Edge Computing and AI Enabled Web3 Metaverse over 6G Wireless Communications: A Deep Reinforcement Learning Approach",
        "authors": [
            "Wenhan Yu",
            "Terence Jie Chua",
            "Jun Zhao"
        ],
        "abstract": "The Metaverse is gaining attention among academics as maturing technologies\nempower the promises and envisagements of a multi-purpose, integrated virtual\nenvironment. An interactive and immersive socialization experience between\npeople is one of the promises of the Metaverse. In spite of the rapid\nadvancements in current technologies, the computation required for a smooth,\nseamless and immersive socialization experience in the Metaverse is\noverbearing, and the accumulated user experience is essential to be considered.\nThe computation burden calls for computation offloading, where the integration\nof virtual and physical world scenes is offloaded to an edge server. This paper\nintroduces a novel Quality-of-Service (QoS) model for the accumulated\nexperience in multi-user socialization on a multichannel wireless network. This\nQoS model utilizes deep reinforcement learning approaches to find the\nnear-optimal channel resource allocation. Comprehensive experiments demonstrate\nthat the adoption of the QoS model enhances the overall socialization\nexperience.",
        "date": "2023-12-11T10:49:47+00:00",
        "link": "http://arxiv.org/pdf/2312.06293v1"
    },
    {
        "title": "Compensation Sampling for Improved Convergence in Diffusion Models",
        "authors": [
            "Hui Lu",
            "Albert ali Salah",
            "Ronald Poppe"
        ],
        "abstract": "Diffusion models achieve remarkable quality in image generation, but at a\ncost. Iterative denoising requires many time steps to produce high fidelity\nimages. We argue that the denoising process is crucially limited by an\naccumulation of the reconstruction error due to an initial inaccurate\nreconstruction of the target data. This leads to lower quality outputs, and\nslower convergence. To address this issue, we propose compensation sampling to\nguide the generation towards the target domain. We introduce a compensation\nterm, implemented as a U-Net, which adds negligible computation overhead during\ntraining and, optionally, inference. Our approach is flexible and we\ndemonstrate its application in unconditional generation, face inpainting, and\nface de-occlusion using benchmark datasets CIFAR-10, CelebA, CelebA-HQ,\nFFHQ-256, and FSG. Our approach consistently yields state-of-the-art results in\nterms of image quality, while accelerating the denoising process to converge\nduring training by up to an order of magnitude.",
        "date": "2023-12-11T10:39:01+00:00",
        "link": "http://arxiv.org/pdf/2312.06285v1"
    },
    {
        "title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models",
        "authors": [
            "Samuel J. Paech"
        ],
        "abstract": "We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of\nemotional intelligence in Large Language Models (LLMs). We assess the ability\nof LLMs to understand complex emotions and social interactions by asking them\nto predict the intensity of emotional states of characters in a dialogue. The\nbenchmark is able to discriminate effectively between a wide range of models.\nWe find that EQ-Bench correlates strongly with comprehensive multi-domain\nbenchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may\nbe capturing similar aspects of broad intelligence. Our benchmark produces\nhighly repeatable results using a set of 60 English-language questions. We also\nprovide open-source code for an automated benchmarking pipeline at\nhttps://github.com/EQ-bench/EQ-Bench and a leaderboard at\nhttps://www.eqbench.com",
        "date": "2023-12-11T10:35:32+00:00",
        "link": "http://arxiv.org/pdf/2312.06281v1"
    },
    {
        "title": "Adaptive Compression of the Latent Space in Variational Autoencoders",
        "authors": [
            "Gabriela Sejnova",
            "Michal Vavrecka",
            "Karla Stepanova"
        ],
        "abstract": "Variational Autoencoders (VAEs) are powerful generative models that have been\nwidely used in various fields, including image and text generation. However,\none of the known challenges in using VAEs is the model's sensitivity to its\nhyperparameters, such as the latent space size. This paper presents a simple\nextension of VAEs for automatically determining the optimal latent space size\nduring the training process by gradually decreasing the latent size through\nneuron removal and observing the model performance. The proposed method is\ncompared to traditional hyperparameter grid search and is shown to be\nsignificantly faster while still achieving the best optimal dimensionality on\nfour image datasets. Furthermore, we show that the final performance of our\nmethod is comparable to training on the optimal latent size from scratch, and\nmight thus serve as a convenient substitute.",
        "date": "2023-12-11T10:35:31+00:00",
        "link": "http://arxiv.org/pdf/2312.06280v1"
    },
    {
        "title": "Regional Correlation Aided Mobile Traffic Prediction with Spatiotemporal Deep Learning",
        "authors": [
            "JeongJun Park",
            "Lusungu J. Mwasinga",
            "Huigyu Yang",
            "Syed M. Raza",
            "Duc-Tai Le",
            "Moonseong Kim",
            "Min Young Chung",
            "Hyunseung Choo"
        ],
        "abstract": "Mobile traffic data in urban regions shows differentiated patterns during\ndifferent hours of the day. The exploitation of these patterns enables highly\naccurate mobile traffic prediction for proactive network management. However,\nrecent Deep Learning (DL) driven studies have only exploited spatiotemporal\nfeatures and have ignored the geographical correlations, causing high\ncomplexity and erroneous mobile traffic predictions. This paper addresses these\nlimitations by proposing an enhanced mobile traffic prediction scheme that\ncombines the clustering strategy of daily mobile traffic peak time and novel\nmulti Temporal Convolutional Network with a Long Short Term Memory (multi\nTCN-LSTM) model. The mobile network cells that exhibit peak traffic during the\nsame hour of the day are clustered together. Our experiments on large-scale\nreal-world mobile traffic data show up to 28% performance improvement compared\nto state-of-the-art studies, which confirms the efficacy and viability of the\nproposed approach.",
        "date": "2023-12-11T10:33:19+00:00",
        "link": "http://arxiv.org/pdf/2312.06279v1"
    },
    {
        "title": "Survey on Foundation Models for Prognostics and Health Management in Industrial Cyber-Physical Systems",
        "authors": [
            "Ruonan Liu",
            "Quanhu Zhang",
            "Te Han",
            "Weidong Zhang",
            "Di Lin",
            "C. L. Philip Chen"
        ],
        "abstract": "Industrial Cyber-Physical Systems (ICPS) integrate the disciplines of\ncomputer science, communication technology, and engineering, and have emerged\nas integral components of contemporary manufacturing and industries. However,\nICPS encounters various challenges in long-term operation, including equipment\nfailures, performance degradation, and security threats. To achieve efficient\nmaintenance and management, prognostics and health management (PHM) finds\nwidespread application in ICPS for critical tasks, including failure\nprediction, health monitoring, and maintenance decision-making. The emergence\nof large-scale foundation models (LFMs) like BERT and GPT signifies a\nsignificant advancement in AI technology, and ChatGPT stands as a remarkable\naccomplishment within this research paradigm, harboring potential for General\nArtificial Intelligence. Considering the ongoing enhancement in data\nacquisition technology and data processing capability, LFMs are anticipated to\nassume a crucial role in the PHM domain of ICPS. However, at present, a\nconsensus is lacking regarding the application of LFMs to PHM in ICPS,\nnecessitating systematic reviews and roadmaps to elucidate future directions.\nTo bridge this gap, this paper elucidates the key components and recent\nadvances in the underlying model.A comprehensive examination and comprehension\nof the latest advances in grand modeling for PHM in ICPS can offer valuable\nreferences for decision makers and researchers in the industrial field while\nfacilitating further enhancements in the reliability, availability, and safety\nof ICPS.",
        "date": "2023-12-11T09:58:46+00:00",
        "link": "http://arxiv.org/pdf/2312.06261v1"
    },
    {
        "title": "No Prior Mask: Eliminate Redundant Action for Deep Reinforcement Learning",
        "authors": [
            "Dianyu Zhong",
            "Yiqin Yang",
            "Qianchuan Zhao"
        ],
        "abstract": "The large action space is one fundamental obstacle to deploying Reinforcement\nLearning methods in the real world. The numerous redundant actions will cause\nthe agents to make repeated or invalid attempts, even leading to task failure.\nAlthough current algorithms conduct some initial explorations for this issue,\nthey either suffer from rule-based systems or depend on expert demonstrations,\nwhich significantly limits their applicability in many real-world settings. In\nthis work, we examine the theoretical analysis of what action can be eliminated\nin policy optimization and propose a novel redundant action filtering\nmechanism. Unlike other works, our method constructs the similarity factor by\nestimating the distance between the state distributions, which requires no\nprior knowledge. In addition, we combine the modified inverse model to avoid\nextensive computation in high-dimensional state space. We reveal the underlying\nstructure of action spaces and propose a simple yet efficient redundant action\nfiltering mechanism named No Prior Mask (NPM) based on the above techniques. We\nshow the superior performance of our method by conducting extensive experiments\non high-dimensional, pixel-input, and stochastic problems with various action\nredundancy. Our code is public online at https://github.com/zhongdy15/npm.",
        "date": "2023-12-11T09:56:02+00:00",
        "link": "http://arxiv.org/pdf/2312.06258v1"
    },
    {
        "title": "Modyn: A Platform for Model Training on Dynamic Datasets With Sample-Level Data Selection",
        "authors": [
            "Maximilian Böther",
            "Viktor Gsteiger",
            "Ties Robroek",
            "Ana Klimovic"
        ],
        "abstract": "Machine learning training data is often dynamic in real-world use cases,\ni.e., data is added or removed and may experience distribution shifts over\ntime. Models must incorporate this evolving training data to improve\ngeneralization, adapt to potential distribution shifts, and adhere to privacy\nregulations. However, the cost of model (re)training is proportional to how\noften the model trains and on how much data it trains on. While ML research\nexplores these topics in isolation, there is no end-to-end open-source platform\nto facilitate the exploration of model retraining and data selection policies\nand the deployment these algorithms efficiently at scale.\n  We present Modyn, a platform for model training on dynamic datasets that\nenables sample-level data selection and triggering policies. Modyn orchestrates\ncontinuous training pipelines while optimizing the underlying system\ninfrastructure to support fast access to arbitrary data samples for efficient\ndata selection. Modyn's extensible architecture allows users to run training\npipelines without modifying the platform code, and enables researchers to\neffortlessly extend the system. We evaluate Modyn's training throughput,\nshowing that even in memory-bound recommendation systems workloads, Modyn is\nable to reach 80 to 100 % of the throughput compared to loading big chunks of\ndata locally without sample-level data selection. Additionally, we showcase\nModyn's functionality with three different data selection policies.",
        "date": "2023-12-11T09:50:52+00:00",
        "link": "http://arxiv.org/pdf/2312.06254v1"
    },
    {
        "title": "Uncovering communities of pipelines in the task-fMRI analytical space",
        "authors": [
            "Elodie Germani",
            "Elisa Fromont",
            "Camille Maumet"
        ],
        "abstract": "Functional magnetic resonance imaging analytical workflows are highly\nflexible with no definite consensus on how to choose a pipeline. While methods\nhave been developed to explore this analytical space, there is still a lack of\nunderstanding of the relationships between the different pipelines. We use\ncommunity detection algorithms to explore the pipeline space and assess its\nstability across different contexts. We show that there are subsets of\npipelines that give similar results, especially those sharing specific\nparameters (e.g. number of motion regressors, software packages, etc.), with\nrelative stability across groups of participants. By visualizing the\ndifferences between these subsets, we describe the effect of pipeline\nparameters and derive general relationships in the analytical space.",
        "date": "2023-12-11T09:18:14+00:00",
        "link": "http://arxiv.org/pdf/2312.06231v1"
    },
    {
        "title": "Tackling Cyberattacks through AI-based Reactive Systems: A Holistic Review and Future Vision",
        "authors": [
            "Sergio Bernardez Molina",
            "Pantaleone Nespoli",
            "Félix Gómez Mármol"
        ],
        "abstract": "There is no denying that the use of Information Technology (IT) is undergoing\nexponential growth in today's world. This digital transformation has also given\nrise to a multitude of security challenges, notably in the realm of cybercrime.\nIn response to these growing threats, public and private sectors have\nprioritized the strengthening of IT security measures. In light of the growing\nsecurity concern, Artificial Intelligence (AI) has gained prominence within the\ncybersecurity landscape. This paper presents a comprehensive survey of recent\nadvancements in AI-driven threat response systems. To the best of our\nknowledge, the most recent survey covering the AI reaction domain was conducted\nin 2017. Since then, considerable literature has been published and therefore\nit is worth reviewing it. By means of several shared features, each of the\nstudies is compared on a common ground. Through an analysis of the research\npapers conducted on a standardized basis, this survey aims to unravel the\ncomplexities and opportunities of integrating AI into cyber defense. The\nconclusions drawn from this collective analysis provide a comprehensive\nsnapshot of the evolving landscape at the intersection of AI and cybersecurity.\nThis landscape underscores the growing significance of not only anticipating\nand detecting threats but also responding to them effectively. Additionally,\nfrom these reviews, various research challenges for the future are presented.\nThese challenges serve as a roadmap for researchers and practitioners in the\nfield of AI-integrated reactive strategies.",
        "date": "2023-12-11T09:17:01+00:00",
        "link": "http://arxiv.org/pdf/2312.06229v1"
    },
    {
        "title": "Invariant Representation Learning via Decoupling Style and Spurious Features",
        "authors": [
            "Ruimeng Li",
            "Yuanhao Pu",
            "Zhaoyi Li",
            "Hong Xie",
            "Defu Lian"
        ],
        "abstract": "This paper considers the out-of-distribution (OOD) generalization problem\nunder the setting that both style distribution shift and spurious features\nexist and domain labels are missing. This setting frequently arises in\nreal-world applications and is underlooked because previous approaches mainly\nhandle either of these two factors. The critical challenge is decoupling style\nand spurious features in the absence of domain labels. To address this\nchallenge, we first propose a structural causal model (SCM) for the image\ngeneration process, which captures both style distribution shift and spurious\nfeatures. The proposed SCM enables us to design a new framework called IRSS,\nwhich can gradually separate style distribution and spurious features from\nimages by introducing adversarial neural networks and multi-environment\noptimization, thus achieving OOD generalization. Moreover, it does not require\nadditional supervision (e.g., domain labels) other than the images and their\ncorresponding labels. Experiments on benchmark datasets demonstrate that IRSS\noutperforms traditional OOD methods and solves the problem of Invariant risk\nminimization (IRM) degradation, enabling the extraction of invariant features\nunder distribution shift.",
        "date": "2023-12-11T09:14:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06226v1"
    },
    {
        "title": "Dance of Channel and Sequence: An Efficient Attention-Based Approach for Multivariate Time Series Forecasting",
        "authors": [
            "Haoxin Wang",
            "Yipeng Mo",
            "Nan Yin",
            "Honghe Dai",
            "Bixiong Li",
            "Songhai Fan",
            "Site Mo"
        ],
        "abstract": "In recent developments, predictive models for multivariate time series\nanalysis have exhibited commendable performance through the adoption of the\nprevalent principle of channel independence. Nevertheless, it is imperative to\nacknowledge the intricate interplay among channels, which fundamentally\ninfluences the outcomes of multivariate predictions. Consequently, the notion\nof channel independence, while offering utility to a certain extent, becomes\nincreasingly impractical, leading to information degradation. In response to\nthis pressing concern, we present CSformer, an innovative framework\ncharacterized by a meticulously engineered two-stage self-attention mechanism.\nThis mechanism is purposefully designed to enable the segregated extraction of\nsequence-specific and channel-specific information, while sharing parameters to\npromote synergy and mutual reinforcement between sequences and channels.\nSimultaneously, we introduce sequence adapters and channel adapters, ensuring\nthe model's ability to discern salient features across various dimensions.\nRigorous experimentation, spanning multiple real-world datasets, underscores\nthe robustness of our approach, consistently establishing its position at the\nforefront of predictive performance across all datasets. This augmentation\nsubstantially enhances the capacity for feature extraction inherent to\nmultivariate time series data, facilitating a more comprehensive exploitation\nof the available information.",
        "date": "2023-12-11T09:10:38+00:00",
        "link": "http://arxiv.org/pdf/2312.06220v1"
    },
    {
        "title": "Interpretable Long Term Waypoint-Based Trajectory Prediction Model",
        "authors": [
            "Amina Ghoul",
            "Itheri Yahiaoui",
            "Fawzi Nashashibi"
        ],
        "abstract": "Predicting the future trajectories of dynamic agents in complex environments\nis crucial for a variety of applications, including autonomous driving,\nrobotics, and human-computer interaction. It is a challenging task as the\nbehavior of the agent is unknown and intrinsically multimodal. Our key insight\nis that the agents behaviors are influenced not only by their past trajectories\nand their interaction with their immediate environment but also largely with\ntheir long term waypoint (LTW). In this paper, we study the impact of adding a\nlong-term goal on the performance of a trajectory prediction framework. We\npresent an interpretable long term waypoint-driven prediction framework\n(WayDCM). WayDCM first predict an agent's intermediate goal (IG) by encoding\nhis interactions with the environment as well as his LTW using a combination of\na Discrete choice Model (DCM) and a Neural Network model (NN). Then, our model\npredicts the corresponding trajectories. This is in contrast to previous work\nwhich does not consider the ultimate intent of the agent to predict his\ntrajectory. We evaluate and show the effectiveness of our approach on the Waymo\nOpen dataset.",
        "date": "2023-12-11T09:10:22+00:00",
        "link": "http://arxiv.org/pdf/2312.06219v1"
    },
    {
        "title": "Offloading and Quality Control for AI Generated Content Services in Edge Computing Networks",
        "authors": [
            "Yitong Wang",
            "Chang Liu",
            "Jun Zhao"
        ],
        "abstract": "AI-Generated Content (AIGC), as a novel manner of providing Metaverse\nservices in the forthcoming Internet paradigm, can resolve the obstacles of\nimmersion requirements. Concurrently, edge computing, as an evolutionary\nparadigm of computing in communication systems, effectively augments real-time\ninteractive services. In pursuit of enhancing the accessibility of AIGC\nservices, the deployment of AIGC models (e.g., diffusion models) to edge\nservers and local devices has become a prevailing trend. Nevertheless, this\napproach faces constraints imposed by battery life and computational resources\nwhen tasks are offloaded to local devices, limiting the capacity to deliver\nhigh-quality content to users while adhering to stringent latency requirements.\nSo there will be a tradeoff between the utility of AIGC models and offloading\ndecisions in the edge computing paradigm. This paper proposes a joint\noptimization algorithm for offloading decisions, computation time, and\ndiffusion steps of the diffusion models in the reverse diffusion stage.\nMoreover, we take the average error into consideration as the metric for\nevaluating the quality of the generated results. Experimental results\nconclusively demonstrate that the proposed algorithm achieves superior joint\noptimization performance compared to the baselines.",
        "date": "2023-12-11T08:36:27+00:00",
        "link": "http://arxiv.org/pdf/2312.06203v1"
    },
    {
        "title": "KnowGPT: Black-Box Knowledge Injection for Large Language Models",
        "authors": [
            "Qinggang Zhang",
            "Junnan Dong",
            "Hao Chen",
            "Xiao Huang",
            "Daochen Zha",
            "Zailiang Yu"
        ],
        "abstract": "Generative Large Language Models (LLMs), such as ChatGPT, offer interactive\nAPIs that can answer common questions at a human-expert level. However, these\nmodels often give inaccurate or incorrect responses when faced with questions\nrequiring domain-specific or professional-specific knowledge not covered in\ntheir training corpus. Furthermore, many state-of-the-art LLMs are not\nopen-source, making it challenging to inject knowledge with model APIs only. In\nthis work, we introduce KnowGPT, a black-box knowledge injection framework for\nLLMs in question answering. KnowGPT leverages deep reinforcement learning (RL)\nto extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed\nBandit (MAB) to construct the most suitable prompt for each question. Our\nextensive experiments on three benchmark datasets showcase that KnowGPT\nsignificantly enhances the existing methods. Notably, KnowGPT achieves an\naverage improvement of 23.7% over ChatGPT and an average improvement of 2.9%\nover GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA\nofficial leaderboard, which is comparable to human-level performance.",
        "date": "2023-12-11T07:56:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06185v1"
    },
    {
        "title": "Why \"classic\" Transformers are shallow and how to make them go deep",
        "authors": [
            "Yueyao Yu",
            "Yin Zhang"
        ],
        "abstract": "Since its introduction in 2017, Transformer has emerged as the leading neural\nnetwork architecture, catalyzing revolutionary advancements in many AI\ndisciplines. The key innovation in Transformer is a Self-Attention (SA)\nmechanism designed to capture contextual information. However, extending the\noriginal Transformer design to models of greater depth has proven exceedingly\nchallenging, if not impossible. Even though various modifications have been\nproposed in order to stack more layers of SA mechanism into deeper models, a\nfull understanding of this depth problem remains elusive. In this paper, we\nconduct a comprehensive investigation, both theoretically and empirically, to\nsubstantiate the claim that the depth problem is caused by \\emph{token\nsimilarity escalation}; that is, tokens grow increasingly alike after repeated\napplications of the SA mechanism. Our analysis reveals that, driven by the\ninvariant leading eigenspace and large spectral gaps of attention matrices,\ntoken similarity provably escalates at a linear rate. Based on the gained\ninsight, we propose a simple strategy that, unlike most existing methods,\nsurgically removes excessive similarity without discounting the SA mechanism as\na whole. Preliminary experimental results confirm the effectiveness of the\nproposed approach on moderate-scale post-norm Transformer models.",
        "date": "2023-12-11T07:49:16+00:00",
        "link": "http://arxiv.org/pdf/2312.06182v1"
    },
    {
        "title": "Improvement in Variational Quantum Algorithms by Measurement Simplification",
        "authors": [
            "Jaehoon Hahm",
            "Hayeon Kim",
            "Young June Park"
        ],
        "abstract": "Variational Quantum Algorithms (VQAs) are expected to be promising algorithms\nwith quantum advantages that can be run at quantum computers in the close\nfuture. In this work, we review simple rules in basic quantum circuits, and\npropose a simplification method, Measurement Simplification, that simplifies\nthe expression for the measurement of quantum circuit. By the Measurement\nSimplification, we simplified the specific result expression of VQAs and\nobtained large improvements in calculation time and required memory size. Here\nwe applied Measurement Simplification to Variational Quantum Linear Solver\n(VQLS), Variational Quantum Eigensolver (VQE) and other Quantum Machine\nLearning Algorithms to show an example of speedup in the calculation time and\nrequired memory size.",
        "date": "2023-12-11T07:32:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06176v1"
    },
    {
        "title": "Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments",
        "authors": [
            "Anthony Cintron Roman",
            "Jennifer Wortman Vaughan",
            "Valerie See",
            "Steph Ballard",
            "Nicolas Schifano",
            "Jehu Torres",
            "Caleb Robinson",
            "Juan M. Lavista Ferres"
        ],
        "abstract": "This paper introduces a no-code, machine-readable documentation framework for\nopen datasets, with a focus on Responsible AI (RAI) considerations. The\nframework aims to improve the accessibility, comprehensibility, and usability\nof open datasets, facilitating easier discovery and use, better understanding\nof content and context, and evaluation of dataset quality and accuracy. The\nproposed framework is designed to streamline the evaluation of datasets,\nhelping researchers, data scientists, and other open data users quickly\nidentify datasets that meet their needs and/or organizational policies or\nregulations. The paper also discusses the implementation of the framework and\nprovides recommendations to maximize its potential. The framework is expected\nto enhance the quality and reliability of data used in research and\ndecision-making, fostering the development of more responsible and trustworthy\nAI systems.",
        "date": "2023-12-11T06:41:14+00:00",
        "link": "http://arxiv.org/pdf/2312.06153v1"
    },
    {
        "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models",
        "authors": [
            "Lifu Tu",
            "Semih Yavuz",
            "Jin Qu",
            "Jiacheng Xu",
            "Rui Meng",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text\ngeneration. However, achieving optimal results with a given prompt or\ninstruction can be challenging, especially for billion-sized models.\nAdditionally, undesired behaviors such as toxicity or hallucinations can\nmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength in\nmitigating these issues, there is still no guarantee of complete prevention. In\nthis work, we propose formalizing text generation as a future-constrained\ngeneration problem to minimize undesirable behaviors and enforce faithfulness\nto instructions. The estimation of future constraint satisfaction, accomplished\nusing LLMs, guides the text generation process. Our extensive experiments\ndemonstrate the effectiveness of the proposed approach across three distinct\ntext generation tasks: keyword-constrained generation (Lin et al., 2020),\ntoxicity reduction (Gehman et al., 2020), and factual correctness in\nquestion-answering (Gao et al., 2023).",
        "date": "2023-12-11T06:35:33+00:00",
        "link": "http://arxiv.org/pdf/2312.06149v1"
    },
    {
        "title": "Proxy-based Item Representation for Attribute and Context-aware Recommendation",
        "authors": [
            "Jinseok Seol",
            "Minseok Gang",
            "Sang-goo Lee",
            "Jaehui Park"
        ],
        "abstract": "Neural network approaches in recommender systems have shown remarkable\nsuccess by representing a large set of items as a learnable vector embedding\ntable. However, infrequent items may suffer from inadequate training\nopportunities, making it difficult to learn meaningful representations. We\nexamine that in attribute and context-aware settings, the poorly learned\nembeddings of infrequent items impair the recommendation accuracy. To address\nsuch an issue, we propose a proxy-based item representation that allows each\nitem to be expressed as a weighted sum of learnable proxy embeddings. Here, the\nproxy weight is determined by the attributes and context of each item and may\nincorporate bias terms in case of frequent items to further reflect\ncollaborative signals. The proxy-based method calculates the item\nrepresentations compositionally, ensuring each representation resides inside a\nwell-trained simplex and, thus, acquires guaranteed quality. Additionally, that\nthe proxy embeddings are shared across all items allows the infrequent items to\nborrow training signals of frequent items in a unified model structure and\nend-to-end manner. Our proposed method is a plug-and-play model that can\nreplace the item encoding layer of any neural network-based recommendation\nmodel, while consistently improving the recommendation performance with much\nsmaller parameter usage. Experiments conducted on real-world recommendation\nbenchmark datasets demonstrate that our proposed model outperforms\nstate-of-the-art models in terms of recommendation accuracy by up to 17% while\nusing only 10% of the parameters.",
        "date": "2023-12-11T06:22:34+00:00",
        "link": "http://arxiv.org/pdf/2312.06145v1"
    },
    {
        "title": "Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications",
        "authors": [
            "Savya Khosla",
            "Zhen Zhu",
            "Yifie He"
        ],
        "abstract": "This paper explores Memory-Augmented Neural Networks (MANNs), delving into\nhow they blend human-like memory processes into AI. It covers different memory\ntypes, like sensory, short-term, and long-term memory, linking psychological\ntheories with AI applications. The study investigates advanced architectures\nsuch as Hopfield Networks, Neural Turing Machines, Correlation Matrix Memories,\nMemformer, and Neural Attention Memory, explaining how they work and where they\nexcel. It dives into real-world uses of MANNs across Natural Language\nProcessing, Computer Vision, Multimodal Learning, and Retrieval Models, showing\nhow memory boosters enhance accuracy, efficiency, and reliability in AI tasks.\nOverall, this survey provides a comprehensive view of MANNs, offering insights\nfor future research in memory-based AI systems.",
        "date": "2023-12-11T06:05:09+00:00",
        "link": "http://arxiv.org/pdf/2312.06141v1"
    },
    {
        "title": "Spreeze: High-Throughput Parallel Reinforcement Learning Framework",
        "authors": [
            "Jing Hou",
            "Guang Chen",
            "Ruiqi Zhang",
            "Zhijun Li",
            "Shangding Gu",
            "Changjun Jiang"
        ],
        "abstract": "The promotion of large-scale applications of reinforcement learning (RL)\nrequires efficient training computation. While existing parallel RL frameworks\nencompass a variety of RL algorithms and parallelization techniques, the\nexcessively burdensome communication frameworks hinder the attainment of the\nhardware's limit for final throughput and training effects on a single desktop.\nIn this paper, we propose Spreeze, a lightweight parallel framework for RL that\nefficiently utilizes a single desktop hardware resource to approach the\nthroughput limit. We asynchronously parallelize the experience sampling,\nnetwork update, performance evaluation, and visualization operations, and\nemploy multiple efficient data transmission techniques to transfer various\ntypes of data between processes. The framework can automatically adjust the\nparallelization hyperparameters based on the computing ability of the hardware\ndevice in order to perform efficient large-batch updates. Based on the\ncharacteristics of the \"Actor-Critic\" RL algorithm, our framework uses dual\nGPUs to independently update the network of actors and critics in order to\nfurther improve throughput. Simulation results show that our framework can\nachieve up to 15,000Hz experience sampling and 370,000Hz network update frame\nrate using only a personal desktop computer, which is an order of magnitude\nhigher than other mainstream parallel RL frameworks, resulting in a 73%\nreduction of training time. Our work on fully utilizing the hardware resources\nof a single desktop computer is fundamental to enabling efficient large-scale\ndistributed RL training.",
        "date": "2023-12-11T05:25:01+00:00",
        "link": "http://arxiv.org/pdf/2312.06126v1"
    },
    {
        "title": "Can LLMs Configure Software Tools",
        "authors": [
            "Jai Kannan"
        ],
        "abstract": "In software engineering, the meticulous configuration of software tools is\ncrucial in ensuring optimal performance within intricate systems. However, the\ncomplexity inherent in selecting optimal configurations is exacerbated by the\nhigh-dimensional search spaces presented in modern applications. Conventional\ntrial-and-error or intuition-driven methods are both inefficient and\nerror-prone, impeding scalability and reproducibility. In this study, we embark\non an exploration of leveraging Large-Language Models (LLMs) to streamline the\nsoftware configuration process. We identify that the task of hyperparameter\nconfiguration for machine learning components within intelligent applications\nis particularly challenging due to the extensive search space and\nperformance-critical nature. Existing methods, including Bayesian optimization,\nhave limitations regarding initial setup, computational cost, and convergence\nefficiency. Our work presents a novel approach that employs LLMs, such as\nChat-GPT, to identify starting conditions and narrow down the search space,\nimproving configuration efficiency. We conducted a series of experiments to\ninvestigate the variability of LLM-generated responses, uncovering intriguing\nfindings such as potential response caching and consistent behavior based on\ndomain-specific keywords. Furthermore, our results from hyperparameter\noptimization experiments reveal the potential of LLMs in expediting\ninitialization processes and optimizing configurations. While our initial\ninsights are promising, they also indicate the need for further in-depth\ninvestigations and experiments in this domain.",
        "date": "2023-12-11T05:03:02+00:00",
        "link": "http://arxiv.org/pdf/2312.06121v1"
    },
    {
        "title": "ROSE: A Recognition-Oriented Speech Enhancement Framework in Air Traffic Control Using Multi-Objective Learning",
        "authors": [
            "Xincheng Yu",
            "Dongyue Guo",
            "Jianwei Zhang",
            "Yi Lin"
        ],
        "abstract": "Radio speech echo is a specific phenomenon in the air traffic control (ATC)\ndomain, which degrades speech quality and further impacts automatic speech\nrecognition (ASR) accuracy. In this work, a recognition-oriented speech\nenhancement (ROSE) framework is proposed to improve speech intelligibility and\nalso advance ASR accuracy, which serves as a plug-and-play tool in ATC\nscenarios and does not require additional retraining of the ASR model.\nSpecifically, an encoder-decoder-based U-Net framework is proposed to eliminate\nthe radio speech echo based on the real-world collected corpus. By\nincorporating the SE-oriented and ASR-oriented loss, ROSE is implemented in a\nmulti-objective manner by learning shared representations across the two\noptimization objectives. An attention-based skip-fusion (ABSF) mechanism is\napplied to skip connections to refine the features. A channel and sequence\nattention (CSAtt) block is innovatively designed to guide the model to focus on\ninformative representations and suppress disturbing features. The experimental\nresults show that the ROSE significantly outperforms other state-of-the-art\nmethods for both the SE and ASR tasks. In addition, the proposed approach can\ncontribute to the desired performance improvements on public datasets.",
        "date": "2023-12-11T04:51:41+00:00",
        "link": "http://arxiv.org/pdf/2312.06118v1"
    },
    {
        "title": "Stellar: Systematic Evaluation of Human-Centric Personalized Text-to-Image Methods",
        "authors": [
            "Panos Achlioptas",
            "Alexandros Benetatos",
            "Iordanis Fostiropoulos",
            "Dimitris Skourtis"
        ],
        "abstract": "In this work, we systematically study the problem of personalized\ntext-to-image generation, where the output image is expected to portray\ninformation about specific human subjects. E.g., generating images of oneself\nappearing at imaginative places, interacting with various items, or engaging in\nfictional activities. To this end, we focus on text-to-image systems that input\na single image of an individual to ground the generation process along with\ntext describing the desired visual context. Our first contribution is to fill\nthe literature gap by curating high-quality, appropriate data for this task.\nNamely, we introduce a standardized dataset (Stellar) that contains\npersonalized prompts coupled with images of individuals that is an order of\nmagnitude larger than existing relevant datasets and where rich semantic\nground-truth annotations are readily available. Having established Stellar to\npromote cross-systems fine-grained comparisons further, we introduce a rigorous\nensemble of specialized metrics that highlight and disentangle fundamental\nproperties such systems should obey. Besides being intuitive, our new metrics\ncorrelate significantly more strongly with human judgment than currently used\nmetrics on this task. Last but not least, drawing inspiration from the recent\nworks of ELITE and SDXL, we derive a simple yet efficient, personalized\ntext-to-image baseline that does not require test-time fine-tuning for each\nsubject and which sets quantitatively and in human trials a new SoTA. For more\ninformation, please visit our project's website:\nhttps://stellar-gen-ai.github.io/.",
        "date": "2023-12-11T04:47:39+00:00",
        "link": "http://arxiv.org/pdf/2312.06116v1"
    },
    {
        "title": "Converting and Smoothing False Negatives for Vision-Language Pre-training",
        "authors": [
            "Jaeseok Byun",
            "Dohoon Kim",
            "Taesup Moon"
        ],
        "abstract": "We consider the critical issue of false negatives in Vision-Language\nPre-training (VLP), a challenge that arises from the inherent many-to-many\ncorrespondence of image-text pairs in large-scale web-crawled datasets. The\npresence of false negatives can impede achieving optimal performance and even\nlead to learning failures. To address this challenge, we propose a method\ncalled COSMO (COnverting and SMOoothing false negatives) that manages the false\nnegative issues, especially powerful in hard negative sampling. Building upon\nthe recently developed GRouped mIni-baTch sampling (GRIT) strategy, our\napproach consists of two pivotal components: 1) an efficient connection mining\nprocess that identifies and converts false negatives into positives, and 2)\nlabel smoothing for the image-text contrastive loss (ITC). Our comprehensive\nexperiments verify the effectiveness of COSMO across multiple downstream tasks,\nemphasizing the crucial role of addressing false negatives in VLP, potentially\neven surpassing the importance of addressing false positives. In addition, the\ncompatibility of COSMO with the recent BLIP-family model is also demonstrated.",
        "date": "2023-12-11T04:33:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06112v1"
    },
    {
        "title": "Learning Unknown Intervention Targets in Structural Causal Models from Heterogeneous Data",
        "authors": [
            "Yuqin Yang",
            "Saber Salehkaleybar",
            "Negar Kiyavash"
        ],
        "abstract": "We study the problem of identifying the unknown intervention targets in\nstructural causal models where we have access to heterogeneous data collected\nfrom multiple environments. The unknown intervention targets are the set of\nendogenous variables whose corresponding exogenous noises change across the\nenvironments. We propose a two-phase approach which in the first phase recovers\nthe exogenous noises corresponding to unknown intervention targets whose\ndistributions have changed across environments. In the second phase, the\nrecovered noises are matched with the corresponding endogenous variables. For\nthe recovery phase, we provide sufficient conditions for learning these\nexogenous noises up to some component-wise invertible transformation. For the\nmatching phase, under the causal sufficiency assumption, we show that the\nproposed method uniquely identifies the intervention targets. In the presence\nof latent confounders, the intervention targets among the observed variables\ncannot be determined uniquely. We provide a candidate intervention target set\nwhich is a superset of the true intervention targets. Our approach improves\nupon the state of the art as the returned candidate set is always a subset of\nthe target set returned by previous work. Moreover, we do not require\nrestrictive assumptions such as linearity of the causal model or performing\ninvariance tests to learn whether a distribution is changing across\nenvironments which could be highly sample inefficient. Our experimental results\nshow the effectiveness of our proposed algorithm in practice.",
        "date": "2023-12-11T03:31:54+00:00",
        "link": "http://arxiv.org/pdf/2312.06091v1"
    },
    {
        "title": "TabMT: Generating tabular data with masked transformers",
        "authors": [
            "Manbir S Gulati",
            "Paul F Roysdon"
        ],
        "abstract": "Autoregressive and Masked Transformers are incredibly effective as generative\nmodels and classifiers. While these models are most prevalent in NLP, they also\nexhibit strong performance in other domains, such as vision. This work\ncontributes to the exploration of transformer-based models in synthetic data\ngeneration for diverse application domains. In this paper, we present TabMT, a\nnovel Masked Transformer design for generating synthetic tabular data. TabMT\neffectively addresses the unique challenges posed by heterogeneous data fields\nand is natively able to handle missing data. Our design leverages improved\nmasking techniques to allow for generation and demonstrates state-of-the-art\nperformance from extremely small to extremely large tabular datasets. We\nevaluate TabMT for privacy-focused applications and find that it is able to\ngenerate high quality data with superior privacy tradeoffs.",
        "date": "2023-12-11T03:28:11+00:00",
        "link": "http://arxiv.org/pdf/2312.06089v1"
    },
    {
        "title": "XAI meets Biology: A Comprehensive Review of Explainable AI in Bioinformatics Applications",
        "authors": [
            "Zhongliang Zhou",
            "Mengxuan Hu",
            "Mariah Salcedo",
            "Nathan Gravel",
            "Wayland Yeung",
            "Aarya Venkat",
            "Dongliang Guo",
            "Jielu Zhang",
            "Natarajan Kannan",
            "Sheng Li"
        ],
        "abstract": "Artificial intelligence (AI), particularly machine learning and deep learning\nmodels, has significantly impacted bioinformatics research by offering powerful\ntools for analyzing complex biological data. However, the lack of\ninterpretability and transparency of these models presents challenges in\nleveraging these models for deeper biological insights and for generating\ntestable hypotheses. Explainable AI (XAI) has emerged as a promising solution\nto enhance the transparency and interpretability of AI models in\nbioinformatics. This review provides a comprehensive analysis of various XAI\ntechniques and their applications across various bioinformatics domains\nincluding DNA, RNA, and protein sequence analysis, structural analysis, gene\nexpression and genome analysis, and bioimaging analysis. We introduce the most\npertinent machine learning and XAI methods, then discuss their diverse\napplications and address the current limitations of available XAI tools. By\noffering insights into XAI's potential and challenges, this review aims to\nfacilitate its practical implementation in bioinformatics research and help\nresearchers navigate the landscape of XAI tools.",
        "date": "2023-12-11T03:08:18+00:00",
        "link": "http://arxiv.org/pdf/2312.06082v1"
    },
    {
        "title": "An Ambiguity Measure for Recognizing the Unknowns in Deep Learning",
        "authors": [
            "Roozbeh Yousefzadeh"
        ],
        "abstract": "We study the understanding of deep neural networks from the scope in which\nthey are trained on. While the accuracy of these models is usually impressive\non the aggregate level, they still make mistakes, sometimes on cases that\nappear to be trivial. Moreover, these models are not reliable in realizing what\nthey do not know leading to failures such as adversarial vulnerability and\nout-of-distribution failures. Here, we propose a measure for quantifying the\nambiguity of inputs for any given model with regard to the scope of its\ntraining. We define the ambiguity based on the geometric arrangements of the\ndecision boundaries and the convex hull of training set in the feature space\nlearned by the trained model, and demonstrate that a single ambiguity measure\nmay detect a considerable portion of mistakes of a model on in-distribution\nsamples, adversarial inputs, as well as out-of-distribution inputs. Using our\nambiguity measure, a model may abstain from classification when it encounters\nambiguous inputs leading to a better model accuracy not just on a given testing\nset, but on the inputs it may encounter at the world at large. In pursuit of\nthis measure, we develop a theoretical framework that can identify the unknowns\nof the model in relation to its scope. We put this in perspective with the\nconfidence of the model and develop formulations to identify the regions of the\ndomain which are unknown to the model, yet the model is guaranteed to have high\nconfidence.",
        "date": "2023-12-11T02:57:12+00:00",
        "link": "http://arxiv.org/pdf/2312.06077v1"
    },
    {
        "title": "A Vision for Operationalising Diversity and Inclusion in AI",
        "authors": [
            "Muneera Bano",
            "Didar Zowghi",
            "Vincenzo Gervasi"
        ],
        "abstract": "The growing presence of Artificial Intelligence (AI) in various sectors\nnecessitates systems that accurately reflect societal diversity. This study\nseeks to envision the operationalization of the ethical imperatives of\ndiversity and inclusion (D&I) within AI ecosystems, addressing the current\ndisconnect between ethical guidelines and their practical implementation. A\nsignificant challenge in AI development is the effective operationalization of\nD&I principles, which is critical to prevent the reinforcement of existing\nbiases and ensure equity across AI applications. This paper proposes a vision\nof a framework for developing a tool utilizing persona-based simulation by\nGenerative AI (GenAI). The approach aims to facilitate the representation of\nthe needs of diverse users in the requirements analysis process for AI\nsoftware. The proposed framework is expected to lead to a comprehensive persona\nrepository with diverse attributes that inform the development process with\ndetailed user narratives. This research contributes to the development of an\ninclusive AI paradigm that ensures future technological advances are designed\nwith a commitment to the diverse fabric of humanity.",
        "date": "2023-12-11T02:44:39+00:00",
        "link": "http://arxiv.org/pdf/2312.06074v1"
    },
    {
        "title": "Contrastive Multi-view Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks",
        "authors": [
            "Renxiang Guan",
            "Zihao Li",
            "Xianju Li",
            "Chang Tang",
            "Ruyi Feng"
        ],
        "abstract": "High-dimensional and complex spectral structures make the clustering of\nhyperspectral images (HSI) a challenging task. Subspace clustering is an\neffective approach for addressing this problem. However, current subspace\nclustering algorithms are primarily designed for a single view and do not fully\nexploit the spatial or textural feature information in HSI. In this study,\ncontrastive multi-view subspace clustering of HSI was proposed based on graph\nconvolutional networks. Pixel neighbor textural and spatial-spectral\ninformation were sent to construct two graph convolutional subspaces to learn\ntheir affinity matrices. To maximize the interaction between different views, a\ncontrastive learning algorithm was introduced to promote the consistency of\npositive samples and assist the model in extracting robust features. An\nattention-based fusion module was used to adaptively integrate these affinity\nmatrices, constructing a more discriminative affinity matrix. The model was\nevaluated using four popular HSI datasets: Indian Pines, Pavia University,\nHouston, and Xu Zhou. It achieved overall accuracies of 97.61%, 96.69%, 87.21%,\nand 97.65%, respectively, and significantly outperformed state-of-the-art\nclustering methods. In conclusion, the proposed model effectively improves the\nclustering accuracy of HSI.",
        "date": "2023-12-11T02:22:10+00:00",
        "link": "http://arxiv.org/pdf/2312.06068v1"
    },
    {
        "title": "PCRDiffusion: Diffusion Probabilistic Models for Point Cloud Registration",
        "authors": [
            "Yue Wu",
            "Yongzhe Yuan",
            "Xiaolong Fan",
            "Xiaoshui Huang",
            "Maoguo Gong",
            "Qiguang Miao"
        ],
        "abstract": "We propose a new framework that formulates point cloud registration as a\ndenoising diffusion process from noisy transformation to object transformation.\nDuring training stage, object transformation diffuses from ground-truth\ntransformation to random distribution, and the model learns to reverse this\nnoising process. In sampling stage, the model refines randomly generated\ntransformation to the output result in a progressive way. We derive the\nvariational bound in closed form for training and provide implementations of\nthe model. Our work provides the following crucial findings: (i) In contrast to\nmost existing methods, our framework, Diffusion Probabilistic Models for Point\nCloud Registration (PCRDiffusion) does not require repeatedly update source\npoint cloud to refine the predicted transformation. (ii) Point cloud\nregistration, one of the representative discriminative tasks, can be solved by\na generative way and the unified probabilistic formulation. Finally, we discuss\nand provide an outlook on the application of diffusion model in different\nscenarios for point cloud registration. Experimental results demonstrate that\nour model achieves competitive performance in point cloud registration. In\ncorrespondence-free and correspondence-based scenarios, PCRDifussion can both\nachieve exceeding 50\\% performance improvements.",
        "date": "2023-12-11T01:56:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06063v1"
    },
    {
        "title": "CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models",
        "authors": [
            "Tuna Han Salih Meral",
            "Enis Simsar",
            "Federico Tombari",
            "Pinar Yanardag"
        ],
        "abstract": "Images produced by text-to-image diffusion models might not always faithfully\nrepresent the semantic intent of the provided text prompt, where the model\nmight overlook or entirely fail to produce certain objects. Existing solutions\noften require customly tailored functions for each of these problems, leading\nto sub-optimal results, especially for complex prompts. Our work introduces a\nnovel perspective by tackling this challenge in a contrastive context. Our\napproach intuitively promotes the segregation of objects in attention maps\nwhile also maintaining that pairs of related attributes are kept close to each\nother. We conduct extensive experiments across a wide variety of scenarios,\neach involving unique combinations of objects, attributes, and scenes. These\nexperiments effectively showcase the versatility, efficiency, and flexibility\nof our method in working with both latent and pixel-based diffusion models,\nincluding Stable Diffusion and Imagen. Moreover, we publicly share our source\ncode to facilitate further research.",
        "date": "2023-12-11T01:42:15+00:00",
        "link": "http://arxiv.org/pdf/2312.06059v1"
    },
    {
        "title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities",
        "authors": [
            "Sangwon Hyun",
            "Mingyu Guo",
            "M. Ali Babar"
        ],
        "abstract": "Large-Language Models (LLMs) have shifted the paradigm of natural language\ndata processing. However, their black-boxed and probabilistic characteristics\ncan lead to potential risks in the quality of outputs in diverse LLM\napplications. Recent studies have tested Quality Attributes (QAs), such as\nrobustness or fairness, of LLMs by generating adversarial input texts. However,\nexisting studies have limited their coverage of QAs and tasks in LLMs and are\ndifficult to extend. Additionally, these studies have only used one evaluation\nmetric, Attack Success Rate (ASR), to assess the effectiveness of their\napproaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL)\nframework to address these issues by applying Metamorphic Testing (MT)\ntechniques. This approach facilitates the systematic testing of LLM qualities\nby defining Metamorphic Relations (MRs), which serve as modularized evaluation\nmetrics. The METAL framework can automatically generate hundreds of MRs from\ntemplates that cover various QAs and tasks. In addition, we introduced novel\nmetrics that integrate the ASR method into the semantic qualities of text to\nassess the effectiveness of MRs accurately. Through the experiments conducted\nwith three prominent LLMs, we have confirmed that the METAL framework\neffectively evaluates essential QAs on primary LLM tasks and reveals the\nquality risks in LLMs. Moreover, the newly proposed metrics can guide the\noptimal MRs for testing each task and suggest the most effective method for\ngenerating MRs.",
        "date": "2023-12-11T01:29:19+00:00",
        "link": "http://arxiv.org/pdf/2312.06056v1"
    },
    {
        "title": "MaskConver: Revisiting Pure Convolution Model for Panoptic Segmentation",
        "authors": [
            "Abdullah Rashwan",
            "Jiageng Zhang",
            "Ali Taalimi",
            "Fan Yang",
            "Xingyi Zhou",
            "Chaochao Yan",
            "Liang-Chieh Chen",
            "Yeqing Li"
        ],
        "abstract": "In recent years, transformer-based models have dominated panoptic\nsegmentation, thanks to their strong modeling capabilities and their unified\nrepresentation for both semantic and instance classes as global binary masks.\nIn this paper, we revisit pure convolution model and propose a novel panoptic\narchitecture named MaskConver. MaskConver proposes to fully unify things and\nstuff representation by predicting their centers. To that extent, it creates a\nlightweight class embedding module that can break the ties when multiple\ncenters co-exist in the same location. Furthermore, our study shows that the\ndecoder design is critical in ensuring that the model has sufficient context\nfor accurate detection and segmentation. We introduce a powerful ConvNeXt-UNet\ndecoder that closes the performance gap between convolution- and\ntransformerbased models. With ResNet50 backbone, our MaskConver achieves 53.6%\nPQ on the COCO panoptic val set, outperforming the modern convolution-based\nmodel, Panoptic FCN, by 9.3% as well as transformer-based models such as\nMask2Former (+1.7% PQ) and kMaX-DeepLab (+0.6% PQ). Additionally, MaskConver\nwith a MobileNet backbone reaches 37.2% PQ, improving over Panoptic-DeepLab by\n+6.4% under the same FLOPs/latency constraints. A further optimized version of\nMaskConver achieves 29.7% PQ, while running in real-time on mobile devices. The\ncode and model weights will be publicly available",
        "date": "2023-12-11T00:52:26+00:00",
        "link": "http://arxiv.org/pdf/2312.06052v1"
    },
    {
        "title": "CAD: Photorealistic 3D Generation via Adversarial Distillation",
        "authors": [
            "Ziyu Wan",
            "Despoina Paschalidou",
            "Ian Huang",
            "Hongyu Liu",
            "Bokui Shen",
            "Xiaoyu Xiang",
            "Jing Liao",
            "Leonidas Guibas"
        ],
        "abstract": "The increased demand for 3D data in AR/VR, robotics and gaming applications,\ngave rise to powerful generative pipelines capable of synthesizing high-quality\n3D objects. Most of these models rely on the Score Distillation Sampling (SDS)\nalgorithm to optimize a 3D representation such that the rendered image\nmaintains a high likelihood as evaluated by a pre-trained diffusion model.\nHowever, finding a correct mode in the high-dimensional distribution produced\nby the diffusion model is challenging and often leads to issues such as\nover-saturation, over-smoothing, and Janus-like artifacts. In this paper, we\npropose a novel learning paradigm for 3D synthesis that utilizes pre-trained\ndiffusion models. Instead of focusing on mode-seeking, our method directly\nmodels the distribution discrepancy between multi-view renderings and diffusion\npriors in an adversarial manner, which unlocks the generation of high-fidelity\nand photorealistic 3D content, conditioned on a single image and prompt.\nMoreover, by harnessing the latent space of GANs and expressive diffusion model\npriors, our method facilitates a wide variety of 3D applications including\nsingle-view reconstruction, high diversity generation and continuous 3D\ninterpolation in the open domain. The experiments demonstrate the superiority\nof our pipeline compared to previous works in terms of generation quality and\ndiversity.",
        "date": "2023-12-11T18:59:58+00:00",
        "link": "http://arxiv.org/pdf/2312.06663v1"
    },
    {
        "title": "Photorealistic Video Generation with Diffusion Models",
        "authors": [
            "Agrim Gupta",
            "Lijun Yu",
            "Kihyuk Sohn",
            "Xiuye Gu",
            "Meera Hahn",
            "Li Fei-Fei",
            "Irfan Essa",
            "Lu Jiang",
            "José Lezama"
        ],
        "abstract": "We present W.A.L.T, a transformer-based approach for photorealistic video\ngeneration via diffusion modeling. Our approach has two key design decisions.\nFirst, we use a causal encoder to jointly compress images and videos within a\nunified latent space, enabling training and generation across modalities.\nSecond, for memory and training efficiency, we use a window attention\narchitecture tailored for joint spatial and spatiotemporal generative modeling.\nTaken together these design decisions enable us to achieve state-of-the-art\nperformance on established video (UCF-101 and Kinetics-600) and image\n(ImageNet) generation benchmarks without using classifier free guidance.\nFinally, we also train a cascade of three models for the task of text-to-video\ngeneration consisting of a base latent video diffusion model, and two video\nsuper-resolution diffusion models to generate videos of $512 \\times 896$\nresolution at $8$ frames per second.",
        "date": "2023-12-11T18:59:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06662v1"
    },
    {
        "title": "UpFusion: Novel View Diffusion from Unposed Sparse View Observations",
        "authors": [
            "Bharath Raj Nagoor Kani",
            "Hsin-Ying Lee",
            "Sergey Tulyakov",
            "Shubham Tulsiani"
        ],
        "abstract": "We propose UpFusion, a system that can perform novel view synthesis and infer\n3D representations for an object given a sparse set of reference images without\ncorresponding pose information. Current sparse-view 3D inference methods\ntypically rely on camera poses to geometrically aggregate information from\ninput views, but are not robust in-the-wild when such information is\nunavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by\nlearning to implicitly leverage the available images as context in a\nconditional generative model for synthesizing novel views. We incorporate two\ncomplementary forms of conditioning into diffusion models for leveraging the\ninput views: a) via inferring query-view aligned features using a scene-level\ntransformer, b) via intermediate attentional layers that can directly observe\nthe input image tokens. We show that this mechanism allows generating\nhigh-fidelity novel views while improving the synthesis quality given\nadditional (unposed) images. We evaluate our approach on the Co3Dv2 and Google\nScanned Objects datasets and demonstrate the benefits of our method over\npose-reliant sparse-view methods as well as single-view methods that cannot\nleverage additional views. Finally, we also show that our learned model can\ngeneralize beyond the training categories and even allow reconstruction from\nself-captured images of generic objects in-the-wild.",
        "date": "2023-12-11T18:59:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06661v1"
    },
    {
        "title": "EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM",
        "authors": [
            "Chong Zhou",
            "Xiangtai Li",
            "Chen Change Loy",
            "Bo Dai"
        ],
        "abstract": "This paper presents EdgeSAM, an accelerated variant of the Segment Anything\nModel (SAM), optimized for efficient execution on edge devices with minimal\ncompromise in performance. Our approach involves distilling the original\nViT-based SAM image encoder into a purely CNN-based architecture, better suited\nfor edge devices. We carefully benchmark various distillation strategies and\ndemonstrate that task-agnostic encoder distillation fails to capture the full\nknowledge embodied in SAM. To overcome this bottleneck, we include both the\nprompt encoder and mask decoder in the distillation process, with box and point\nprompts in the loop, so that the distilled model can accurately capture the\nintricate dynamics between user input and mask generation. To mitigate dataset\nbias issues stemming from point prompt distillation, we incorporate a\nlightweight module within the encoder. EdgeSAM achieves a 40-fold speed\nincrease compared to the original SAM, and it also outperforms MobileSAM, being\n14 times as fast when deployed on edge devices while enhancing the mIoUs on\nCOCO and LVIS by 2.3 and 3.2 respectively. It is also the first SAM variant\nthat can run at over 30 FPS on an iPhone 14. Code and models are available at\nhttps://github.com/chongzhou96/EdgeSAM.",
        "date": "2023-12-11T18:59:52+00:00",
        "link": "http://arxiv.org/pdf/2312.06660v1"
    },
    {
        "title": "Learning Naturally Aggregated Appearance for Efficient 3D Editing",
        "authors": [
            "Ka Leong Cheng",
            "Qiuyu Wang",
            "Zifan Shi",
            "Kecheng Zheng",
            "Yinghao Xu",
            "Hao Ouyang",
            "Qifeng Chen",
            "Yujun Shen"
        ],
        "abstract": "Neural radiance fields, which represent a 3D scene as a color field and a\ndensity field, have demonstrated great progress in novel view synthesis yet are\nunfavorable for editing due to the implicitness. In view of such a deficiency,\nwe propose to replace the color field with an explicit 2D appearance\naggregation, also called canonical image, with which users can easily customize\ntheir 3D editing via 2D image processing. To avoid the distortion effect and\nfacilitate convenient editing, we complement the canonical image with a\nprojection field that maps 3D points onto 2D pixels for texture lookup. This\nfield is carefully initialized with a pseudo canonical camera model and\noptimized with offset regularity to ensure naturalness of the aggregated\nappearance. Extensive experimental results on three datasets suggest that our\nrepresentation, dubbed AGAP, well supports various ways of 3D editing (e.g.,\nstylization, interactive drawing, and content extraction) with no need of\nre-optimization for each case, demonstrating its generalizability and\nefficiency. Project page is available at https://felixcheng97.github.io/AGAP/.",
        "date": "2023-12-11T18:59:31+00:00",
        "link": "http://arxiv.org/pdf/2312.06657v1"
    },
    {
        "title": "Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior",
        "authors": [
            "Fangfu Liu",
            "Diankun Wu",
            "Yi Wei",
            "Yongming Rao",
            "Yueqi Duan"
        ],
        "abstract": "Recently, 3D content creation from text prompts has demonstrated remarkable\nprogress by utilizing 2D and 3D diffusion models. While 3D diffusion models\nensure great multi-view consistency, their ability to generate high-quality and\ndiverse 3D assets is hindered by the limited 3D data. In contrast, 2D diffusion\nmodels find a distillation approach that achieves excellent generalization and\nrich details without any 3D data. However, 2D lifting methods suffer from\ninherent view-agnostic ambiguity thereby leading to serious multi-face Janus\nissues, where text prompts fail to provide sufficient guidance to learn\ncoherent 3D results. Instead of retraining a costly viewpoint-aware model, we\nstudy how to fully exploit easily accessible coarse 3D knowledge to enhance the\nprompts and guide 2D lifting optimization for refinement. In this paper, we\npropose Sherpa3D, a new text-to-3D framework that achieves high-fidelity,\ngeneralizability, and geometric consistency simultaneously. Specifically, we\ndesign a pair of guiding strategies derived from the coarse 3D prior generated\nby the 3D diffusion model: a structural guidance for geometric fidelity and a\nsemantic guidance for 3D coherence. Employing the two types of guidance, the 2D\ndiffusion model enriches the 3D content with diversified and high-quality\nresults. Extensive experiments show the superiority of our Sherpa3D over the\nstate-of-the-art text-to-3D methods in terms of quality and 3D consistency.",
        "date": "2023-12-11T18:59:18+00:00",
        "link": "http://arxiv.org/pdf/2312.06655v1"
    },
    {
        "title": "LightSim: Neural Lighting Simulation for Urban Scenes",
        "authors": [
            "Ava Pun",
            "Gary Sun",
            "Jingkang Wang",
            "Yun Chen",
            "Ze Yang",
            "Sivabalan Manivasagam",
            "Wei-Chiu Ma",
            "Raquel Urtasun"
        ],
        "abstract": "Different outdoor illumination conditions drastically alter the appearance of\nurban scenes, and they can harm the performance of image-based robot perception\nsystems if not seen during training. Camera simulation provides a\ncost-effective solution to create a large dataset of images captured under\ndifferent lighting conditions. Towards this goal, we propose LightSim, a neural\nlighting camera simulation system that enables diverse, realistic, and\ncontrollable data generation. LightSim automatically builds lighting-aware\ndigital twins at scale from collected raw sensor data and decomposes the scene\ninto dynamic actors and static background with accurate geometry, appearance,\nand estimated scene lighting. These digital twins enable actor insertion,\nmodification, removal, and rendering from a new viewpoint, all in a\nlighting-aware manner. LightSim then combines physically-based and learnable\ndeferred rendering to perform realistic relighting of modified scenes, such as\naltering the sun location and modifying the shadows or changing the sun\nbrightness, producing spatially- and temporally-consistent camera videos. Our\nexperiments show that LightSim generates more realistic relighting results than\nprior work. Importantly, training perception models on data generated by\nLightSim can significantly improve their performance.",
        "date": "2023-12-11T18:59:13+00:00",
        "link": "http://arxiv.org/pdf/2312.06654v1"
    },
    {
        "title": "Adaptive Human Trajectory Prediction via Latent Corridors",
        "authors": [
            "Neerja Thakkar",
            "Karttikeya Mangalam",
            "Andrea Bajcsy",
            "Jitendra Malik"
        ],
        "abstract": "Human trajectory prediction is typically posed as a zero-shot generalization\nproblem: a predictor is learnt on a dataset of human motion in training scenes,\nand then deployed on unseen test scenes. While this paradigm has yielded\ntremendous progress, it fundamentally assumes that trends in human behavior\nwithin the deployment scene are constant over time. As such, current prediction\nmodels are unable to adapt to scene-specific transient human behaviors, such as\ncrowds temporarily gathering to see buskers, pedestrians hurrying through the\nrain and avoiding puddles, or a protest breaking out. We formalize the problem\nof scene-specific adaptive trajectory prediction and propose a new adaptation\napproach inspired by prompt tuning called latent corridors. By augmenting the\ninput of any pre-trained human trajectory predictor with learnable image\nprompts, the predictor can improve in the deployment scene by inferring trends\nfrom extremely small amounts of new data (e.g., 2 humans observed for 30\nseconds). With less than 0.1% additional model parameters, we see up to 23.9%\nADE improvement in MOTSynth simulated data and 16.4% ADE in MOT and Wildtrack\nreal pedestrian data. Qualitatively, we observe that latent corridors imbue\npredictors with an awareness of scene geometry and scene-specific human\nbehaviors that non-adaptive predictors struggle to capture. The project website\ncan be found at https://neerja.me/atp_latent_corridors/.",
        "date": "2023-12-11T18:59:12+00:00",
        "link": "http://arxiv.org/pdf/2312.06653v1"
    },
    {
        "title": "Nuvo: Neural UV Mapping for Unruly 3D Representations",
        "authors": [
            "Pratul P. Srinivasan",
            "Stephan J. Garbin",
            "Dor Verbin",
            "Jonathan T. Barron",
            "Ben Mildenhall"
        ],
        "abstract": "Existing UV mapping algorithms are designed to operate on well-behaved\nmeshes, instead of the geometry representations produced by state-of-the-art 3D\nreconstruction and generation techniques. As such, applying these methods to\nthe volume densities recovered by neural radiance fields and related techniques\n(or meshes triangulated from such fields) results in texture atlases that are\ntoo fragmented to be useful for tasks such as view synthesis or appearance\nediting. We present a UV mapping method designed to operate on geometry\nproduced by 3D reconstruction and generation techniques. Instead of computing a\nmapping defined on a mesh's vertices, our method Nuvo uses a neural field to\nrepresent a continuous UV mapping, and optimizes it to be a valid and\nwell-behaved mapping for just the set of visible points, i.e. only points that\naffect the scene's appearance. We show that our model is robust to the\nchallenges posed by ill-behaved geometry, and that it produces editable UV\nmappings that can represent detailed appearance.",
        "date": "2023-12-11T18:58:38+00:00",
        "link": "http://arxiv.org/pdf/2312.05283v1"
    },
    {
        "title": "4M: Massively Multimodal Masked Modeling",
        "authors": [
            "David Mizrahi",
            "Roman Bachmann",
            "Oğuzhan Fatih Kar",
            "Teresa Yeo",
            "Mingfei Gao",
            "Afshin Dehghan",
            "Amir Zamir"
        ],
        "abstract": "Current machine learning models for vision are often highly specialized and\nlimited to a single modality and task. In contrast, recent large language\nmodels exhibit a wide range of capabilities, hinting at a possibility for\nsimilarly versatile models in computer vision. In this paper, we take a step in\nthis direction and propose a multimodal training scheme called 4M. It consists\nof training a single unified Transformer encoder-decoder using a masked\nmodeling objective across a wide range of input/output modalities - including\ntext, images, geometric, and semantic modalities, as well as neural network\nfeature maps. 4M achieves scalability by unifying the representation space of\nall modalities through mapping them into discrete tokens and performing\nmultimodal masked modeling on a small randomized subset of tokens.\n  4M leads to models that exhibit several key capabilities: (1) they can\nperform a diverse set of vision tasks out of the box, (2) they excel when\nfine-tuned for unseen downstream tasks or new input modalities, and (3) they\ncan function as a generative model that can be conditioned on arbitrary\nmodalities, enabling a wide variety of expressive multimodal editing\ncapabilities with remarkable flexibility.\n  Through experimental analyses, we demonstrate the potential of 4M for\ntraining versatile and scalable foundation models for vision tasks, setting the\nstage for further exploration in multimodal learning for vision and other\ndomains.",
        "date": "2023-12-11T18:57:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06647v1"
    },
    {
        "title": "Beyond Classification: Definition and Density-based Estimation of Calibration in Object Detection",
        "authors": [
            "Teodora Popordanoska",
            "Aleksei Tiulpin",
            "Matthew B. Blaschko"
        ],
        "abstract": "Despite their impressive predictive performance in various computer vision\ntasks, deep neural networks (DNNs) tend to make overly confident predictions,\nwhich hinders their widespread use in safety-critical applications. While there\nhave been recent attempts to calibrate DNNs, most of these efforts have\nprimarily been focused on classification tasks, thus neglecting DNN-based\nobject detectors. Although several recent works addressed calibration for\nobject detection and proposed differentiable penalties, none of them are\nconsistent estimators of established concepts in calibration. In this work, we\ntackle the challenge of defining and estimating calibration error specifically\nfor this task. In particular, we adapt the definition of classification\ncalibration error to handle the nuances associated with object detection, and\npredictions in structured output spaces more generally. Furthermore, we propose\na consistent and differentiable estimator of the detection calibration error,\nutilizing kernel density estimation. Our experiments demonstrate the\neffectiveness of our estimator against competing train-time and post-hoc\ncalibration methods, while maintaining similar detection performance.",
        "date": "2023-12-11T18:57:05+00:00",
        "link": "http://arxiv.org/pdf/2312.06645v1"
    },
    {
        "title": "AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes",
        "authors": [
            "Zehao Wen",
            "Zichen Liu",
            "Srinath Sridhar",
            "Rao Fu"
        ],
        "abstract": "We introduce AnyHome, a framework that translates open-vocabulary\ndescriptions, ranging from simple labels to elaborate paragraphs, into\nwell-structured and textured 3D indoor scenes at a house-scale. Inspired by\ncognition theories, AnyHome employs an amodal structured representation to\ncapture 3D spatial cues from textual narratives and then uses egocentric\ninpainting to enrich these scenes. To this end, we begin by using specially\ndesigned template prompts for Large Language Models (LLMs), which enable\nprecise control over the textual input. We then utilize intermediate\nrepresentations to maintain the spatial structure's consistency, ensuring that\nthe 3D scenes align closely with the textual description. Then, we apply a\nScore Distillation Sampling process to refine the placement of objects. Lastly,\nan egocentric inpainting process is incorporated to enhance the realism and\nappearance of the scenes. AnyHome stands out due to its hierarchical structured\nrepresentation combined with the versatility of open-vocabulary text\ninterpretation. This allows for extensive customization of indoor scenes at\nvarious levels of granularity. We demonstrate that AnyHome can reliably\ngenerate a range of diverse indoor scenes, characterized by their detailed\nspatial structures and textures, all corresponding to the free-form textual\ninputs.",
        "date": "2023-12-11T18:56:37+00:00",
        "link": "http://arxiv.org/pdf/2312.06644v1"
    },
    {
        "title": "Gaze Detection and Analysis for Initiating Joint Activity in Industrial Human-Robot Collaboration",
        "authors": [
            "Pooja Prajod",
            "Matteo Lavit Nicora",
            "Marta Mondellini",
            "Giovanni Tauro",
            "Rocco Vertechy",
            "Matteo Malosio",
            "Elisabeth André"
        ],
        "abstract": "Collaborative robots (cobots) are widely used in industrial applications, yet\nextensive research is still needed to enhance human-robot collaborations and\noperator experience. A potential approach to improve the collaboration\nexperience involves adapting cobot behavior based on natural cues from the\noperator. Inspired by the literature on human-human interactions, we conducted\na wizard-of-oz study to examine whether a gaze towards the cobot can serve as a\ntrigger for initiating joint activities in collaborative sessions. In this\nstudy, 37 participants engaged in an assembly task while their gaze behavior\nwas analyzed. We employ a gaze-based attention recognition model to identify\nwhen the participants look at the cobot. Our results indicate that in most\ncases (84.88\\%), the joint activity is preceded by a gaze towards the cobot.\nFurthermore, during the entire assembly cycle, the participants tend to look at\nthe cobot around the time of the joint activity. To the best of our knowledge,\nthis is the first study to analyze the natural gaze behavior of participants\nworking on a joint activity with a robot during a collaborative assembly task.",
        "date": "2023-12-11T18:56:03+00:00",
        "link": "http://arxiv.org/pdf/2312.06643v1"
    },
    {
        "title": "CorresNeRF: Image Correspondence Priors for Neural Radiance Fields",
        "authors": [
            "Yixing Lao",
            "Xiaogang Xu",
            "Zhipeng Cai",
            "Xihui Liu",
            "Hengshuang Zhao"
        ],
        "abstract": "Neural Radiance Fields (NeRFs) have achieved impressive results in novel view\nsynthesis and surface reconstruction tasks. However, their performance suffers\nunder challenging scenarios with sparse input views. We present CorresNeRF, a\nnovel method that leverages image correspondence priors computed by\noff-the-shelf methods to supervise NeRF training. We design adaptive processes\nfor augmentation and filtering to generate dense and high-quality\ncorrespondences. The correspondences are then used to regularize NeRF training\nvia the correspondence pixel reprojection and depth loss terms. We evaluate our\nmethods on novel view synthesis and surface reconstruction tasks with\ndensity-based and SDF-based NeRF models on different datasets. Our method\noutperforms previous methods in both photometric and geometric metrics. We show\nthat this simple yet effective technique of using correspondence priors can be\napplied as a plug-and-play module across different NeRF variants. The project\npage is at https://yxlao.github.io/corres-nerf.",
        "date": "2023-12-11T18:55:29+00:00",
        "link": "http://arxiv.org/pdf/2312.06642v1"
    },
    {
        "title": "Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution",
        "authors": [
            "Shangchen Zhou",
            "Peiqing Yang",
            "Jianyi Wang",
            "Yihang Luo",
            "Chen Change Loy"
        ],
        "abstract": "Text-based diffusion models have exhibited remarkable success in generation\nand editing, showing great promise for enhancing visual content with their\ngenerative prior. However, applying these models to video super-resolution\nremains challenging due to the high demands for output fidelity and temporal\nconsistency, which is complicated by the inherent randomness in diffusion\nmodels. Our study introduces Upscale-A-Video, a text-guided latent diffusion\nframework for video upscaling. This framework ensures temporal coherence\nthrough two key mechanisms: locally, it integrates temporal layers into U-Net\nand VAE-Decoder, maintaining consistency within short sequences; globally,\nwithout training, a flow-guided recurrent latent propagation module is\nintroduced to enhance overall video stability by propagating and fusing latent\nacross the entire sequences. Thanks to the diffusion paradigm, our model also\noffers greater flexibility by allowing text prompts to guide texture creation\nand adjustable noise levels to balance restoration and generation, enabling a\ntrade-off between fidelity and quality. Extensive experiments show that\nUpscale-A-Video surpasses existing methods in both synthetic and real-world\nbenchmarks, as well as in AI-generated videos, showcasing impressive visual\nrealism and temporal consistency.",
        "date": "2023-12-11T18:54:52+00:00",
        "link": "http://arxiv.org/pdf/2312.06640v1"
    },
    {
        "title": "Harmonic Mobile Manipulation",
        "authors": [
            "Ruihan Yang",
            "Yejin Kim",
            "Aniruddha Kembhavi",
            "Xiaolong Wang",
            "Kiana Ehsani"
        ],
        "abstract": "Recent advancements in robotics have enabled robots to navigate complex\nscenes or manipulate diverse objects independently. However, robots are still\nimpotent in many household tasks requiring coordinated behaviors such as\nopening doors. The factorization of navigation and manipulation, while\neffective for some tasks, fails in scenarios requiring coordinated actions. To\naddress this challenge, we introduce, HarmonicMM, an end-to-end learning method\nthat optimizes both navigation and manipulation, showing notable improvement\nover existing techniques in everyday tasks. This approach is validated in\nsimulated and real-world environments and adapts to novel unseen settings\nwithout additional tuning. Our contributions include a new benchmark for mobile\nmanipulation and the successful deployment in a real unseen apartment,\ndemonstrating the potential for practical indoor robot deployment in daily\nlife. More results are on our project site:\nhttps://rchalyang.github.io/HarmonicMM/",
        "date": "2023-12-11T18:54:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06639v1"
    },
    {
        "title": "TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation",
        "authors": [
            "Rongkun Zheng",
            "Lu Qi",
            "Xi Chen",
            "Yi Wang",
            "Kun Wang",
            "Yu Qiao",
            "Hengshuang Zhao"
        ],
        "abstract": "Training on large-scale datasets can boost the performance of video instance\nsegmentation while the annotated datasets for VIS are hard to scale up due to\nthe high labor cost. What we possess are numerous isolated filed-specific\ndatasets, thus, it is appealing to jointly train models across the aggregation\nof datasets to enhance data volume and diversity. However, due to the\nheterogeneity in category space, as mask precision increases with the data\nvolume, simply utilizing multiple datasets will dilute the attention of models\non different taxonomies. Thus, increasing the data scale and enriching taxonomy\nspace while improving classification precision is important. In this work, we\nanalyze that providing extra taxonomy information can help models concentrate\non specific taxonomy, and propose our model named Taxonomy-aware Multi-dataset\nJoint Training for Video Instance Segmentation (TMT-VIS) to address this vital\nchallenge. Specifically, we design a two-stage taxonomy aggregation module that\nfirst compiles taxonomy information from input videos and then aggregates these\ntaxonomy priors into instance queries before the transformer decoder. We\nconduct extensive experimental evaluations on four popular and challenging\nbenchmarks, including YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and UVO. Our\nmodel shows significant improvement over the baseline solutions, and sets new\nstate-of-the-art records on all benchmarks. These appealing and encouraging\nresults demonstrate the effectiveness and generality of our approach. The code\nis available at\nhttps://github.com/rkzheng99/TMT-VIS(https://github.com/rkzheng99/TMT-VIS)",
        "date": "2023-12-11T18:50:09+00:00",
        "link": "http://arxiv.org/pdf/2312.06630v1"
    },
    {
        "title": "AttenScribble: Attentive Similarity Learning for Scribble-Supervised Medical Image Segmentation",
        "authors": [
            "Mu Tian",
            "Qinzhu Yang",
            "Yi Gao"
        ],
        "abstract": "The success of deep networks in medical image segmentation relies heavily on\nmassive labeled training data. However, acquiring dense annotations is a\ntime-consuming process. Weakly-supervised methods normally employ less\nexpensive forms of supervision, among which scribbles started to gain\npopularity lately thanks to its flexibility. However, due to lack of shape and\nboundary information, it is extremely challenging to train a deep network on\nscribbles that generalizes on unlabeled pixels. In this paper, we present a\nstraightforward yet effective scribble supervised learning framework. Inspired\nby recent advances of transformer based segmentation, we create a pluggable\nspatial self-attention module which could be attached on top of any internal\nfeature layers of arbitrary fully convolutional network (FCN) backbone. The\nmodule infuses global interaction while keeping the efficiency of convolutions.\nDescended from this module, we construct a similarity metric based on\nnormalized and symmetrized attention. This attentive similarity leads to a\nnovel regularization loss that imposes consistency between segmentation\nprediction and visual affinity. This attentive similarity loss optimizes the\nalignment of FCN encoders, attention mapping and model prediction. Ultimately,\nthe proposed FCN+Attention architecture can be trained end-to-end guided by a\ncombination of three learning objectives: partial segmentation loss, a\ncustomized masked conditional random fields and the proposed attentive\nsimilarity loss. Extensive experiments on public datasets (ACDC and CHAOS)\nshowed that our framework not just out-performs existing state-of-the-art, but\nalso delivers close performance to fully-supervised benchmark. Code will be\navailable upon publication.",
        "date": "2023-12-11T18:42:18+00:00",
        "link": "http://arxiv.org/pdf/2312.06614v1"
    },
    {
        "title": "Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism",
        "authors": [
            "Georgios Milis",
            "Panagiotis P. Filntisis",
            "Anastasios Roussos",
            "Petros Maragos"
        ],
        "abstract": "Recent advances in deep learning for sequential data have given rise to fast\nand powerful models that produce realistic videos of talking humans. The state\nof the art in talking face generation focuses mainly on lip-syncing, being\nconditioned on audio clips. However, having the ability to synthesize talking\nhumans from text transcriptions rather than audio is particularly beneficial\nfor many applications and is expected to receive more and more attention,\nfollowing the recent breakthroughs in large language models. For that, most\nmethods implement a cascaded 2-stage architecture of a text-to-speech module\nfollowed by an audio-driven talking face generator, but this ignores the highly\ncomplex interplay between audio and visual streams that occurs during speaking.\nIn this paper, we propose the first, to the best of our knowledge, text-driven\naudiovisual speech synthesizer that uses Transformers and does not follow a\ncascaded approach. Our method, which we call NEUral Text to ARticulate Talk\n(NEUTART), is a talking face generator that uses a joint audiovisual feature\nspace, as well as speech-informed 3D facial reconstructions and a lip-reading\nloss for visual supervision. The proposed model produces photorealistic talking\nface videos with human-like articulation and well-synced audiovisual streams.\nOur experiments on audiovisual datasets as well as in-the-wild videos reveal\nstate-of-the-art generation quality both in terms of objective metrics and\nhuman evaluation.",
        "date": "2023-12-11T18:41:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06613v1"
    },
    {
        "title": "DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection",
        "authors": [
            "Haoyang He",
            "Jiangning Zhang",
            "Hongxu Chen",
            "Xuhai Chen",
            "Zhishan Li",
            "Xu Chen",
            "Yabiao Wang",
            "Chengjie Wang",
            "Lei Xie"
        ],
        "abstract": "Reconstruction-based approaches have achieved remarkable outcomes in anomaly\ndetection. The exceptional image reconstruction capabilities of recently\npopular diffusion models have sparked research efforts to utilize them for\nenhanced reconstruction of anomalous images. Nonetheless, these methods might\nface challenges related to the preservation of image categories and pixel-wise\nstructural integrity in the more practical multi-class setting. To solve the\nabove problems, we propose a Difusion-based Anomaly Detection (DiAD) framework\nfor multi-class anomaly detection, which consists of a pixel-space autoencoder,\na latent-space Semantic-Guided (SG) network with a connection to the stable\ndiffusion's denoising network, and a feature-space pre-trained feature\nextractor. Firstly, The SG network is proposed for reconstructing anomalous\nregions while preserving the original image's semantic information. Secondly,\nwe introduce Spatial-aware Feature Fusion (SFF) block to maximize\nreconstruction accuracy when dealing with extensively reconstructed areas.\nThirdly, the input and reconstructed images are processed by a pre-trained\nfeature extractor to generate anomaly maps based on features extracted at\ndifferent scales. Experiments on MVTec-AD and VisA datasets demonstrate the\neffectiveness of our approach which surpasses the state-of-the-art methods,\ne.g., achieving 96.8/52.6 and 97.2/99.0 (AUROC/AP) for localization and\ndetection respectively on multi-class MVTec-AD dataset. Code will be available\nat https://lewandofskee.github.io/projects/diad.",
        "date": "2023-12-11T18:38:28+00:00",
        "link": "http://arxiv.org/pdf/2312.06607v1"
    },
    {
        "title": "Early Action Recognition with Action Prototypes",
        "authors": [
            "Guglielmo Camporese",
            "Alessandro Bergamo",
            "Xunyu Lin",
            "Joseph Tighe",
            "Davide Modolo"
        ],
        "abstract": "Early action recognition is an important and challenging problem that enables\nthe recognition of an action from a partially observed video stream where the\nactivity is potentially unfinished or even not started. In this work, we\npropose a novel model that learns a prototypical representation of the full\naction for each class and uses it to regularize the architecture and the visual\nrepresentations of the partial observations. Our model is very simple in design\nand also efficient. We decompose the video into short clips, where a visual\nencoder extracts features from each clip independently. Later, a decoder\naggregates together in an online fashion features from all the clips for the\nfinal class prediction. During training, for each partial observation, the\nmodel is jointly trained to both predict the label as well as the action\nprototypical representation which acts as a regularizer. We evaluate our method\non multiple challenging real-world datasets and outperform the current\nstate-of-the-art by a significant margin. For example, on early recognition\nobserving only the first 10% of each video, our method improves the SOTA by\n+2.23 Top-1 accuracy on Something-Something-v2, +3.55 on UCF-101, +3.68 on\nSSsub21, and +5.03 on EPIC-Kitchens-55, where prior work used either\nmulti-modal inputs (e.g. optical-flow) or batched inference. Finally, we also\npresent exhaustive ablation studies to motivate the design choices we made, as\nwell as gather insights regarding what our model is learning semantically.",
        "date": "2023-12-11T18:31:13+00:00",
        "link": "http://arxiv.org/pdf/2312.06598v1"
    },
    {
        "title": "Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops",
        "authors": [
            "Aditya Prakash",
            "Arjun Gupta",
            "Saurabh Gupta"
        ],
        "abstract": "Objects undergo varying amounts of perspective distortion as they move across\na camera's field of view. Models for predicting 3D from a single image often\nwork with crops around the object of interest and ignore the location of the\nobject in the camera's field of view. We note that ignoring this location\ninformation further exaggerates the inherent ambiguity in making 3D inferences\nfrom 2D images and can prevent models from even fitting to the training data.\nTo mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding\n(KPE), which incorporates information about the location of crops in the image\nand camera intrinsics. Experiments on three popular 3D-from-a-single-image\nbenchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes,\nand predicting 3D shapes of articulated objects on ARCTIC, show the benefits of\nKPE.",
        "date": "2023-12-11T18:28:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06594v1"
    },
    {
        "title": "Flexible visual prompts for in-context learning in computer vision",
        "authors": [
            "Thomas Foster",
            "Ioana Croitoru",
            "Robert Dorfman",
            "Christoffer Edlund",
            "Thomas Varsavsky",
            "Jon Almazán"
        ],
        "abstract": "In this work, we address in-context learning (ICL) for the task of image\nsegmentation, introducing a novel approach that adapts a modern Video Object\nSegmentation (VOS) technique for visual in-context learning. This adaptation is\ninspired by the VOS method's ability to efficiently and flexibly learn objects\nfrom a few examples. Through evaluations across a range of support set sizes\nand on diverse segmentation datasets, our method consistently surpasses\nexisting techniques. Notably, it excels with data containing classes not\nencountered during training. Additionally, we propose a technique for support\nset selection, which involves choosing the most relevant images to include in\nthis set. By employing support set selection, the performance increases for all\ntested methods without the need for additional training or prompt tuning. The\ncode can be found at https://github.com/v7labs/XMem_ICL/.",
        "date": "2023-12-11T18:27:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06592v1"
    },
    {
        "title": "QuickQuakeBuildings: Post-earthquake SAR-Optical Dataset for Quick Damaged-building Detection",
        "authors": [
            "Yao Sun",
            "Yi Wang",
            "Michael Eineder"
        ],
        "abstract": "Quick and automated earthquake-damaged building detection from post-event\nsatellite imagery is crucial, yet it is challenging due to the scarcity of\ntraining data required to develop robust algorithms. This letter presents the\nfirst dataset dedicated to detecting earthquake-damaged buildings from\npost-event very high resolution (VHR) Synthetic Aperture Radar (SAR) and\noptical imagery. Utilizing open satellite imagery and annotations acquired\nafter the 2023 Turkey-Syria earthquakes, we deliver a dataset of coregistered\nbuilding footprints and satellite image patches of both SAR and optical data,\nencompassing more than four thousand buildings. The task of damaged building\ndetection is formulated as a binary image classification problem, that can also\nbe treated as an anomaly detection problem due to extreme class imbalance. We\nprovide baseline methods and results to serve as references for comparison.\nResearchers can utilize this dataset to expedite algorithm development,\nfacilitating the rapid detection of damaged buildings in response to future\nevents. The dataset and codes together with detailed explanations are made\npublicly available at\n\\url{https://github.com/ya0-sun/PostEQ-SARopt-BuildingDamage}.",
        "date": "2023-12-11T18:19:36+00:00",
        "link": "http://arxiv.org/pdf/2312.06587v1"
    },
    {
        "title": "3D Hand Pose Estimation in Egocentric Images in the Wild",
        "authors": [
            "Aditya Prakash",
            "Ruisen Tu",
            "Matthew Chang",
            "Saurabh Gupta"
        ],
        "abstract": "We present WildHands, a method for 3D hand pose estimation in egocentric\nimages in the wild. This is challenging due to (a) lack of 3D hand pose\nannotations for images in the wild, and (b) a form of perspective\ndistortion-induced shape ambiguity that arises in the analysis of crops around\nhands. For the former, we use auxiliary supervision on in-the-wild data in the\nform of segmentation masks & grasp labels in addition to 3D supervision\navailable in lab datasets. For the latter, we provide spatial cues about the\nlocation of the hand crop in the camera's field of view. Our approach achieves\nthe best 3D hand pose on the ARCTIC leaderboard and outperforms FrankMocap, a\npopular and robust approach for estimating hand pose in the wild, by 45.3% when\nevaluated on 2D hand pose on our EPIC-HandKps dataset.",
        "date": "2023-12-11T18:15:47+00:00",
        "link": "http://arxiv.org/pdf/2312.06583v1"
    },
    {
        "title": "EasyVolcap: Accelerating Neural Volumetric Video Research",
        "authors": [
            "Zhen Xu",
            "Tao Xie",
            "Sida Peng",
            "Haotong Lin",
            "Qing Shuai",
            "Zhiyuan Yu",
            "Guangzhao He",
            "Jiaming Sun",
            "Hujun Bao",
            "Xiaowei Zhou"
        ],
        "abstract": "Volumetric video is a technology that digitally records dynamic events such\nas artistic performances, sporting events, and remote conversations. When\nacquired, such volumography can be viewed from any viewpoint and timestamp on\nflat screens, 3D displays, or VR headsets, enabling immersive viewing\nexperiences and more flexible content creation in a variety of applications\nsuch as sports broadcasting, video conferencing, gaming, and movie productions.\nWith the recent advances and fast-growing interest in neural scene\nrepresentations for volumetric video, there is an urgent need for a unified\nopen-source library to streamline the process of volumetric video capturing,\nreconstruction, and rendering for both researchers and non-professional users\nto develop various algorithms and applications of this emerging technology. In\nthis paper, we present EasyVolcap, a Python & Pytorch library for accelerating\nneural volumetric video research with the goal of unifying the process of\nmulti-view data processing, 4D scene reconstruction, and efficient dynamic\nvolumetric video rendering. Our source code is available at\nhttps://github.com/zju3dv/EasyVolcap.",
        "date": "2023-12-11T17:59:46+00:00",
        "link": "http://arxiv.org/pdf/2312.06575v1"
    },
    {
        "title": "ControlNet-XS: Designing an Efficient and Effective Architecture for Controlling Text-to-Image Diffusion Models",
        "authors": [
            "Denis Zavadski",
            "Johann-Friedrich Feiden",
            "Carsten Rother"
        ],
        "abstract": "The field of image synthesis has made tremendous strides forward in the last\nyears. Besides defining the desired output image with text-prompts, an\nintuitive approach is to additionally use spatial guidance in form of an image,\nsuch as a depth map. For this, a recent and highly popular approach is to use a\ncontrolling network, such as ControlNet, in combination with a pre-trained\nimage generation model, such as Stable Diffusion. When evaluating the design of\nexisting controlling networks, we observe that they all suffer from the same\nproblem of a delay in information flowing between the generation and\ncontrolling process. This, in turn, means that the controlling network must\nhave generative capabilities. In this work we propose a new controlling\narchitecture, called ControlNet-XS, which does not suffer from this problem,\nand hence can focus on the given task of learning to control. In contrast to\nControlNet, our model needs only a fraction of parameters, and hence is about\ntwice as fast during inference and training time. Furthermore, the generated\nimages are of higher quality and the control is of higher fidelity. All code\nand pre-trained models will be made publicly available.",
        "date": "2023-12-11T17:58:06+00:00",
        "link": "http://arxiv.org/pdf/2312.06573v1"
    },
    {
        "title": "Inferring Hybrid Neural Fluid Fields from Videos",
        "authors": [
            "Hong-Xing Yu",
            "Yang Zheng",
            "Yuan Gao",
            "Yitong Deng",
            "Bo Zhu",
            "Jiajun Wu"
        ],
        "abstract": "We study recovering fluid density and velocity from sparse multiview videos.\nExisting neural dynamic reconstruction methods predominantly rely on optical\nflows; therefore, they cannot accurately estimate the density and uncover the\nunderlying velocity due to the inherent visual ambiguities of fluid velocity,\nas fluids are often shapeless and lack stable visual features. The challenge is\nfurther pronounced by the turbulent nature of fluid flows, which calls for\nproperly designed fluid velocity representations. To address these challenges,\nwe propose hybrid neural fluid fields (HyFluid), a neural approach to jointly\ninfer fluid density and velocity fields. Specifically, to deal with visual\nambiguities of fluid velocity, we introduce a set of physics-based losses that\nenforce inferring a physically plausible velocity field, which is\ndivergence-free and drives the transport of density. To deal with the turbulent\nnature of fluid velocity, we design a hybrid neural velocity representation\nthat includes a base neural velocity field that captures most irrotational\nenergy and a vortex particle-based velocity that models residual turbulent\nvelocity. We show that our method enables recovering vortical flow details. Our\napproach opens up possibilities for various learning and reconstruction\napplications centered around 3D incompressible flow, including fluid\nre-simulation and editing, future prediction, and neural dynamic scene\ncomposition. Project website: https://kovenyu.com/HyFluid/",
        "date": "2023-12-11T17:46:25+00:00",
        "link": "http://arxiv.org/pdf/2312.06561v1"
    },
    {
        "title": "HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models",
        "authors": [
            "Xiaogang Peng",
            "Yiming Xie",
            "Zizhao Wu",
            "Varun Jampani",
            "Deqing Sun",
            "Huaizu Jiang"
        ],
        "abstract": "We address the problem of generating realistic 3D human-object interactions\n(HOIs) driven by textual prompts. Instead of a single model, our key insight is\nto take a modular design and decompose the complex task into simpler sub-tasks.\nWe first develop a dual-branch diffusion model (HOI-DM) to generate both human\nand object motions conditioning on the input text, and encourage coherent\nmotions by a cross-attention communication module between the human and object\nmotion generation branches. We also develop an affordance prediction diffusion\nmodel (APDM) to predict the contacting area between the human and object during\nthe interactions driven by the textual prompt. The APDM is independent of the\nresults by the HOI-DM and thus can correct potential errors by the latter.\nMoreover, it stochastically generates the contacting points to diversify the\ngenerated motions. Finally, we incorporate the estimated contacting points into\nthe classifier-guidance to achieve accurate and close contact between humans\nand objects. To train and evaluate our approach, we annotate BEHAVE dataset\nwith text descriptions. Experimental results demonstrate that our approach is\nable to produce realistic HOIs with various interactions and different types of\nobjects.",
        "date": "2023-12-11T17:41:17+00:00",
        "link": "http://arxiv.org/pdf/2312.06553v1"
    },
    {
        "title": "Grounded Question-Answering in Long Egocentric Videos",
        "authors": [
            "Shangzhe Di",
            "Weidi Xie"
        ],
        "abstract": "Existing approaches to video understanding, mainly designed for short videos\nfrom a third-person perspective, are limited in their applicability in certain\nfields, such as robotics. In this paper, we delve into open-ended\nquestion-answering (QA) in long, egocentric videos, which allows individuals or\nrobots to inquire about their own past visual experiences. This task presents\nunique challenges, including the complexity of temporally grounding queries\nwithin extensive video content, the high resource demands for precise data\nannotation, and the inherent difficulty of evaluating open-ended answers due to\ntheir ambiguous nature. Our proposed approach tackles these challenges by (i)\nintegrating query grounding and answering within a unified model to reduce\nerror propagation; (ii) employing large language models for efficient and\nscalable data synthesis; and (iii) introducing a close-ended QA task for\nevaluation, to manage answer ambiguity. Extensive experiments demonstrate the\neffectiveness of our method, which also achieves state-of-the-art performance\non the QAEgo4D and Ego4D-NLQ benchmarks. We plan to publicly release the codes,\nmodel, and constructed datasets for future research.",
        "date": "2023-12-11T16:31:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06505v1"
    },
    {
        "title": "Detecting Events in Crowds Through Changes in Geometrical Dimensions of Pedestrians",
        "authors": [
            "Matheus Schreiner Homrich da Silva",
            "Paulo Brossard de Souza Pinto Neto",
            "Rodolfo Migon Favaretto",
            "Soraia Raupp Musse"
        ],
        "abstract": "Security is an important topic in our contemporary world, and the ability to\nautomate the detection of any events of interest that can take place in a crowd\nis of great interest to a population. We hypothesize that the detection of\nevents in videos is correlated with significant changes in pedestrian\nbehaviors. In this paper, we examine three different scenarios of crowd\nbehavior, containing both the cases where an event triggers a change in the\nbehavior of the crowd and two video sequences where the crowd and its motion\nremain mostly unchanged. With both the videos and the tracking of the\nindividual pedestrians (performed in a pre-processed phase), we use Geomind, a\nsoftware we developed to extract significant data about the scene, in\nparticular, the geometrical features, personalities, and emotions of each\nperson. We then examine the output, seeking a significant change in the way\neach person acts as a function of the time, that could be used as a basis to\nidentify events or to model realistic crowd actions. When applied to the games\narea, our method can use the detected events to find some sort of pattern to be\nthen used in agent simulation. Results indicate that our hypothesis seems valid\nin the sense that the visually observed events could be automatically detected\nusing GeoMind.",
        "date": "2023-12-11T16:18:56+00:00",
        "link": "http://arxiv.org/pdf/2312.06495v1"
    },
    {
        "title": "STDiff: Spatio-temporal Diffusion for Continuous Stochastic Video Prediction",
        "authors": [
            "Xi Ye",
            "Guillaume-Alexandre Bilodeau"
        ],
        "abstract": "Predicting future frames of a video is challenging because it is difficult to\nlearn the uncertainty of the underlying factors influencing their contents. In\nthis paper, we propose a novel video prediction model, which has\ninfinite-dimensional latent variables over the spatio-temporal domain.\nSpecifically, we first decompose the video motion and content information, then\ntake a neural stochastic differential equation to predict the temporal motion\ninformation, and finally, an image diffusion model autoregressively generates\nthe video frame by conditioning on the predicted motion feature and the\nprevious frame. The better expressiveness and stronger stochasticity learning\ncapability of our model lead to state-of-the-art video prediction performances.\nAs well, our model is able to achieve temporal continuous prediction, i.e.,\npredicting in an unsupervised way the future video frames with an arbitrarily\nhigh frame rate. Our code is available at\n\\url{https://github.com/XiYe20/STDiffProject}.",
        "date": "2023-12-11T16:12:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06486v1"
    },
    {
        "title": "Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation",
        "authors": [
            "Xiaoyi Bao",
            "Jie Qin",
            "Siyang Sun",
            "Yun Zheng",
            "Xingang Wang"
        ],
        "abstract": "For few-shot semantic segmentation, the primary task is to extract\nclass-specific intrinsic information from limited labeled data. However, the\nsemantic ambiguity and inter-class similarity of previous methods limit the\naccuracy of pixel-level foreground-background classification. To alleviate\nthese issues, we propose the Relevant Intrinsic Feature Enhancement Network\n(RiFeNet). To improve the semantic consistency of foreground instances, we\npropose an unlabeled branch as an efficient data utilization method, which\nteaches the model how to extract intrinsic features robust to intra-class\ndifferences. Notably, during testing, the proposed unlabeled branch is excluded\nwithout extra unlabeled data and computation. Furthermore, we extend the\ninter-class variability between foreground and background by proposing a novel\nmulti-level prototype generation and interaction module. The different-grained\ncomplementarity between global and local prototypes allows for better\ndistinction between similar categories. The qualitative and quantitative\nperformance of RiFeNet surpasses the state-of-the-art methods on PASCAL-5i and\nCOCO benchmarks.",
        "date": "2023-12-11T16:02:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06474v1"
    },
    {
        "title": "Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation",
        "authors": [
            "Qi Yang",
            "Xing Nie",
            "Tong Li",
            "Pengfei Gao",
            "Ying Guo",
            "Cheng Zhen",
            "Pengfei Yan",
            "Shiming Xiang"
        ],
        "abstract": "Recently, an audio-visual segmentation (AVS) task has been introduced, aiming\nto group pixels with sounding objects within a given video. This task\nnecessitates a first-ever audio-driven pixel-level understanding of the scene,\nposing significant challenges. In this paper, we propose an innovative\naudio-visual transformer framework, termed COMBO, an acronym for COoperation of\nMulti-order Bilateral relatiOns. For the first time, our framework explores\nthree types of bilateral entanglements within AVS: pixel entanglement, modality\nentanglement, and temporal entanglement. Regarding pixel entanglement, we\nemploy a Siam-Encoder Module (SEM) that leverages prior knowledge to generate\nmore precise visual features from the foundational model. For modality\nentanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to\nalign corresponding visual and auditory signals bi-directionally. As for\ntemporal entanglement, we introduce an innovative adaptive inter-frame\nconsistency loss according to the inherent rules of temporal. Comprehensive\nexperiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou\non MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that\nCOMBO surpasses previous state-of-the-art methods. Code and more results will\nbe publicly available at https://combo-avs.github.io/.",
        "date": "2023-12-11T15:51:38+00:00",
        "link": "http://arxiv.org/pdf/2312.06462v1"
    },
    {
        "title": "ASF-YOLO: A Novel YOLO Model with Attentional Scale Sequence Fusion for Cell Instance Segmentation",
        "authors": [
            "Ming Kang",
            "Chee-Ming Ting",
            "Fung Fung Ting",
            "Raphaël C. -W. Phan"
        ],
        "abstract": "We propose a novel Attentional Scale Sequence Fusion based You Only Look Once\n(YOLO) framework (ASF-YOLO) which combines spatial and scale features for\naccurate and fast cell instance segmentation. Built on the YOLO segmentation\nframework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhance\nthe multi-scale information extraction capability of the network, and the\nTriple Feature Encoder (TPE) module to fuse feature maps of different scales to\nincrease detailed information. We further introduce a Channel and Position\nAttention Mechanism (CPAM) to integrate both the SSFF and TPE modules, which\nfocus on informative channels and spatial position-related small objects for\nimproved detection and segmentation performance. Experimental validations on\ntwo cell datasets show remarkable segmentation accuracy and speed of the\nproposed ASF-YOLO model. It achieves a box mAP of 0.91, mask mAP of 0.887, and\nan inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset,\noutperforming the state-of-the-art methods. The source code is available at\nhttps://github.com/mkang315/ASF-YOLO.",
        "date": "2023-12-11T15:47:12+00:00",
        "link": "http://arxiv.org/pdf/2312.06458v1"
    },
    {
        "title": "Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images",
        "authors": [
            "Bao Li",
            "Zhenyu Liu",
            "Lizhi Shao",
            "Bensheng Qiu",
            "Hong Bu",
            "Jie Tian"
        ],
        "abstract": "Directly predicting human epidermal growth factor receptor 2 (HER2) status\nfrom widely available hematoxylin and eosin (HE)-stained whole slide images\n(WSIs) can reduce technical costs and expedite treatment selection. Accurately\npredicting HER2 requires large collections of multi-site WSIs. Federated\nlearning enables collaborative training of these WSIs without gigabyte-size\nWSIs transportation and data privacy concerns. However, federated learning\nencounters challenges in addressing label imbalance in multi-site WSIs from the\nreal world. Moreover, existing WSI classification methods cannot simultaneously\nexploit local context information and long-range dependencies in the site-end\nfeature representation of federated learning. To address these issues, we\npresent a point transformer with federated learning for multi-site HER2 status\nprediction from HE-stained WSIs. Our approach incorporates two novel designs.\nWe propose a dynamic label distribution strategy and an auxiliary classifier,\nwhich helps to establish a well-initialized model and mitigate label\ndistribution variations across sites. Additionally, we propose a farthest\ncosine sampling based on cosine distance. It can sample the most distinctive\nfeatures and capture the long-range dependencies. Extensive experiments and\nanalysis show that our method achieves state-of-the-art performance at four\nsites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can\ngeneralize to two unseen sites with 229 WSIs.",
        "date": "2023-12-11T15:41:05+00:00",
        "link": "http://arxiv.org/pdf/2312.06454v1"
    },
    {
        "title": "Semantic Image Synthesis for Abdominal CT",
        "authors": [
            "Yan Zhuang",
            "Benjamin Hou",
            "Tejas Sudharshan Mathai",
            "Pritam Mukherjee",
            "Boah Kim",
            "Ronald M. Summers"
        ],
        "abstract": "As a new emerging and promising type of generative models, diffusion models\nhave proven to outperform Generative Adversarial Networks (GANs) in multiple\ntasks, including image synthesis. In this work, we explore semantic image\nsynthesis for abdominal CT using conditional diffusion models, which can be\nused for downstream applications such as data augmentation. We systematically\nevaluated the performance of three diffusion models, as well as to other\nstate-of-the-art GAN-based approaches, and studied the different conditioning\nscenarios for the semantic mask. Experimental results demonstrated that\ndiffusion models were able to synthesize abdominal CT images with better\nquality. Additionally, encoding the mask and the input separately is more\neffective than na\\\"ive concatenating.",
        "date": "2023-12-11T15:39:41+00:00",
        "link": "http://arxiv.org/pdf/2312.06453v1"
    },
    {
        "title": "DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior",
        "authors": [
            "Tianyu Huang",
            "Yihan Zeng",
            "Zhilu Zhang",
            "Wan Xu",
            "Hang Xu",
            "Songcen Xu",
            "Rynson W. H. Lau",
            "Wangmeng Zuo"
        ],
        "abstract": "3D generation has raised great attention in recent years. With the success of\ntext-to-image diffusion models, the 2D-lifting technique becomes a promising\nroute to controllable 3D generation. However, these methods tend to present\ninconsistent geometry, which is also known as the Janus problem. We observe\nthat the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D\ndiffusion models and overfitting of the optimization objective. To address it,\nwe propose a two-stage 2D-lifting framework, namely DreamControl, which\noptimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained\nobjects with control-based score distillation. Specifically, adaptive viewpoint\nsampling and boundary integrity metric are proposed to ensure the consistency\nof generated priors. The priors are then regarded as input conditions to\nmaintain reasonable geometries, in which conditional LoRA and weighted score\nare further proposed to optimize detailed textures. DreamControl can generate\nhigh-quality 3D content in terms of both geometry consistency and texture\nfidelity. Moreover, our control-based optimization guidance is applicable to\nmore downstream tasks, including user-guided generation and 3D animation. The\nproject page is available at https://github.com/tyhuang0428/DreamControl.",
        "date": "2023-12-11T15:12:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06439v1"
    },
    {
        "title": "VisionTraj: A Noise-Robust Trajectory Recovery Framework based on Large-scale Camera Network",
        "authors": [
            "Zhishuai Li",
            "Ziyue Li",
            "Xiaoru Hu",
            "Guoqing Du",
            "Yunhao Nie",
            "Feng Zhu",
            "Lei Bai",
            "Rui Zhao"
        ],
        "abstract": "Trajectory recovery based on the snapshots from the city-wide multi-camera\nnetwork facilitates urban mobility sensing and driveway optimization. The\nstate-of-the-art solutions devoted to such a vision-based scheme typically\nincorporate predefined rules or unsupervised iterative feedback, struggling\nwith multi-fold challenges such as lack of open-source datasets for training\nthe whole pipeline, and the vulnerability to the noises from visual inputs. In\nresponse to the dilemma, this paper proposes VisionTraj, the first\nlearning-based model that reconstructs vehicle trajectories from snapshots\nrecorded by road network cameras. Coupled with it, we elaborate on two rational\nvision-trajectory datasets, which produce extensive trajectory data along with\ncorresponding visual snapshots, enabling supervised vision-trajectory interplay\nextraction. Following the data creation, based on the results from the\noff-the-shelf multi-modal vehicle clustering, we first re-formulate the\ntrajectory recovery problem as a generative task and introduce the canonical\nTransformer as the autoregressive backbone. Then, to identify clustering noises\n(e.g., false positives) with the bound on the snapshots' spatiotemporal\ndependencies, a GCN-based soft-denoising module is conducted based on the fine-\nand coarse-grained Re-ID clusters. Additionally, we harness strong semantic\ninformation extracted from the tracklet to provide detailed insights into the\nvehicle's entry and exit actions during trajectory recovery. The denoising and\ntracklet components can also act as plug-and-play modules to boost baselines.\nExperimental results on the two hand-crafted datasets show that the proposed\nVisionTraj achieves a maximum +11.5% improvement against the sub-best model.",
        "date": "2023-12-11T14:52:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06428v1"
    },
    {
        "title": "Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It",
        "authors": [
            "Adam Lilja",
            "Junsheng Fu",
            "Erik Stenborg",
            "Lars Hammarstrand"
        ],
        "abstract": "Data leakage is a critical issue when training and evaluating any method\nbased on supervised learning. The state-of-the-art methods for online mapping\nare based on supervised learning and are trained predominantly using two\ndatasets: nuScenes and Argoverse 2. These datasets revisit the same geographic\nlocations across training, validation, and test sets. Specifically, over $80$%\nof nuScenes and $40$% of Argoverse 2 validation and test samples are located\nless than $5$ m from a training sample. This allows methods to localize within\na memorized implicit map during testing and leads to inflated performance\nnumbers being reported. To reveal the true performance in unseen environments,\nwe introduce geographical splits of the data. Experimental results show\nsignificantly lower performance numbers, for some methods dropping with more\nthan $45$ mAP, when retraining and reevaluating existing online mapping models\nwith the proposed split. Additionally, a reassessment of prior design choices\nreveals diverging conclusions from those based on the original split. Notably,\nthe impact of the lifting method and the support from auxiliary tasks (e.g.,\ndepth supervision) on performance appears less substantial or follows a\ndifferent trajectory than previously perceived. Geographical splits can be\nfound https://github.com/LiljaAdam/geographical-splits",
        "date": "2023-12-11T14:43:23+00:00",
        "link": "http://arxiv.org/pdf/2312.06420v1"
    },
    {
        "title": "PointVoxel: A Simple and Effective Pipeline for Multi-View Multi-Modal 3D Human Pose Estimation",
        "authors": [
            "Zhiyu Pan",
            "Zhicheng Zhong",
            "Wenxuan Guo",
            "Yifan Chen",
            "Jianjiang Feng",
            "Jie Zhou"
        ],
        "abstract": "Recently, several methods have been proposed to estimate 3D human pose from\nmulti-view images and achieved impressive performance on public datasets\ncollected in relatively easy scenarios. However, there are limited approaches\nfor extracting 3D human skeletons from multimodal inputs (e.g., RGB and\npointcloud) that can enhance the accuracy of predicting 3D poses in challenging\nsituations. We fill this gap by introducing a pipeline called PointVoxel that\nfuses multi-view RGB and pointcloud inputs to obtain 3D human poses. We\ndemonstrate that volumetric representation is an effective architecture for\nintegrating these different modalities. Moreover, in order to overcome the\nchallenges of annotating 3D human pose labels in difficult scenarios, we\ndevelop a synthetic dataset generator for pretraining and design an\nunsupervised domain adaptation strategy so that we can obtain a well-trained 3D\nhuman pose estimator without using any manual annotations. We evaluate our\napproach on four datasets (two public datasets, one synthetic dataset, and one\nchallenging dataset named BasketBall collected by ourselves), showing promising\nresults. The code and dataset will be released soon.",
        "date": "2023-12-11T14:30:11+00:00",
        "link": "http://arxiv.org/pdf/2312.06409v1"
    },
    {
        "title": "Compound Text-Guided Prompt Tuning via Image-Adaptive Cues",
        "authors": [
            "Hao Tan",
            "Jun Li",
            "Yizhuang Zhou",
            "Jun Wan",
            "Zhen Lei",
            "Xiangyu Zhang"
        ],
        "abstract": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable\ngeneralization capabilities to downstream tasks. However, existing prompt\ntuning based frameworks need to parallelize learnable textual inputs for all\ncategories, suffering from massive GPU memory consumption when there is a large\nnumber of categories in the target dataset. Moreover, previous works require to\ninclude category names within prompts, exhibiting subpar performance when\ndealing with ambiguous category names. To address these shortcomings, we\npropose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces\nresource demand while achieving superior performance. We introduce text\nsupervision to the optimization of prompts, which enables two benefits: 1)\nreleasing the model reliance on the pre-defined category names during\ninference, thereby enabling more flexible prompt generation; 2) reducing the\nnumber of inputs to the text encoder, which decreases GPU memory consumption\nsignificantly. Specifically, we found that compound text supervisions, i.e.,\ncategory-wise and content-wise, is highly effective, since they provide\ninter-class separability and capture intra-class variations, respectively.\nMoreover, we condition the prompt generation on visual features through a\nmodule called Bonder, which facilitates the alignment between prompts and\nvisual features. Extensive experiments on few-shot recognition and domain\ngeneralization demonstrate that TGP-T achieves superior performance with\nconsistently lower training costs. It reduces GPU memory usage by 93% and\nattains a 2.5% performance gain on 16-shot ImageNet. The code is available at\nhttps://github.com/EricTan7/TGP-T.",
        "date": "2023-12-11T14:17:02+00:00",
        "link": "http://arxiv.org/pdf/2312.06401v1"
    },
    {
        "title": "DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers",
        "authors": [
            "Aaron Mir",
            "Eduardo Alonso",
            "Esther Mondragón"
        ],
        "abstract": "We propose a novel talking head synthesis pipeline called \"DiT-Head\", which\nis based on diffusion transformers and uses audio as a condition to drive the\ndenoising process of a diffusion model. Our method is scalable and can\ngeneralise to multiple identities while producing high-quality results. We\ntrain and evaluate our proposed approach and compare it against existing\nmethods of talking head synthesis. We show that our model can compete with\nthese methods in terms of visual quality and lip-sync accuracy. Our results\nhighlight the potential of our proposed approach to be used for a wide range of\napplications, including virtual assistants, entertainment, and education. For a\nvideo demonstration of the results and our user study, please refer to our\nsupplementary material.",
        "date": "2023-12-11T14:09:56+00:00",
        "link": "http://arxiv.org/pdf/2312.06400v1"
    },
    {
        "title": "NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos",
        "authors": [
            "Jinxi Li",
            "Ziyang Song",
            "Bo Yang"
        ],
        "abstract": "In this paper, we aim to model 3D scene dynamics from multi-view videos.\nUnlike the majority of existing works which usually focus on the common task of\nnovel view synthesis within the training time period, we propose to\nsimultaneously learn the geometry, appearance, and physical velocity of 3D\nscenes only from video frames, such that multiple desirable applications can be\nsupported, including future frame extrapolation, unsupervised 3D semantic scene\ndecomposition, and dynamic motion transfer. Our method consists of three major\ncomponents, 1) the keyframe dynamic radiance field, 2) the interframe velocity\nfield, and 3) a joint keyframe and interframe optimization module which is the\ncore of our framework to effectively train both networks. To validate our\nmethod, we further introduce two dynamic 3D datasets: 1) Dynamic Object\ndataset, and 2) Dynamic Indoor Scene dataset. We conduct extensive experiments\non multiple datasets, demonstrating the superior performance of our method over\nall baselines, particularly in the critical tasks of future frame extrapolation\nand unsupervised 3D semantic scene decomposition.",
        "date": "2023-12-11T14:07:31+00:00",
        "link": "http://arxiv.org/pdf/2312.06398v1"
    },
    {
        "title": "ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation",
        "authors": [
            "Cédric Rommel",
            "Victor Letzelter",
            "Nermin Samet",
            "Renaud Marlet",
            "Matthieu Cord",
            "Patrick Pérez",
            "Eduardo Valle"
        ],
        "abstract": "Monocular 3D human pose estimation (3D-HPE) is an inherently ambiguous task,\nas a 2D pose in an image might originate from different possible 3D poses. Yet,\nmost 3D-HPE methods rely on regression models, which assume a one-to-one\nmapping between inputs and outputs. In this work, we provide theoretical and\nempirical evidence that, because of this ambiguity, common regression models\nare bound to predict topologically inconsistent poses, and that traditional\nevaluation metrics, such as the MPJPE, P-MPJPE and PCK, are insufficient to\nassess this aspect. As a solution, we propose ManiPose, a novel\nmanifold-constrained multi-hypothesis model capable of proposing multiple\ncandidate 3D poses for each 2D input, together with their corresponding\nplausibility. Unlike previous multi-hypothesis approaches, our solution is\ncompletely supervised and does not rely on complex generative models, thus\ngreatly facilitating its training and usage. Furthermore, by constraining our\nmodel to lie within the human pose manifold, we can guarantee the consistency\nof all hypothetical poses predicted with our approach, which was not possible\nin previous works. We illustrate the usefulness of ManiPose in a synthetic\n1D-to-2D lifting setting and demonstrate on real-world datasets that it\noutperforms state-of-the-art models in pose consistency by a large margin,\nwhile still reaching competitive MPJPE performance.",
        "date": "2023-12-11T13:50:10+00:00",
        "link": "http://arxiv.org/pdf/2312.06386v1"
    },
    {
        "title": "Ternary Spike: Learning Ternary Spikes for Spiking Neural Networks",
        "authors": [
            "Yufei Guo",
            "Yuanpei Chen",
            "Xiaode Liu",
            "Weihang Peng",
            "Yuhan Zhang",
            "Xuhui Huang",
            "Zhe Ma"
        ],
        "abstract": "The Spiking Neural Network (SNN), as one of the biologically inspired neural\nnetwork infrastructures, has drawn increasing attention recently. It adopts\nbinary spike activations to transmit information, thus the multiplications of\nactivations and weights can be substituted by additions, which brings high\nenergy efficiency. However, in the paper, we theoretically and experimentally\nprove that the binary spike activation map cannot carry enough information,\nthus causing information loss and resulting in accuracy decreasing. To handle\nthe problem, we propose a ternary spike neuron to transmit information. The\nternary spike neuron can also enjoy the event-driven and multiplication-free\noperation advantages of the binary spike neuron but will boost the information\ncapacity. Furthermore, we also embed a trainable factor in the ternary spike\nneuron to learn the suitable spike amplitude, thus our SNN will adopt different\nspike amplitudes along layers, which can better suit the phenomenon that the\nmembrane potential distributions are different along layers. To retain the\nefficiency of the vanilla ternary spike, the trainable ternary spike SNN will\nbe converted to a standard one again via a re-parameterization technique in the\ninference. Extensive experiments with several popular network structures over\nstatic and dynamic datasets show that the ternary spike can consistently\noutperform state-of-the-art methods. Our code is open-sourced at\nhttps://github.com/yfguo91/Ternary-Spike.",
        "date": "2023-12-11T13:28:54+00:00",
        "link": "http://arxiv.org/pdf/2312.06372v1"
    },
    {
        "title": "Intraoperative 2D/3D Image Registration via Differentiable X-ray Rendering",
        "authors": [
            "Vivek Gopalakrishnan",
            "Neel Dey",
            "Polina Golland"
        ],
        "abstract": "Surgical decisions are informed by aligning rapid portable 2D intraoperative\nimages (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,\nCT). 2D/3D image registration often fails in practice: conventional\noptimization methods are prohibitively slow and susceptible to local minima,\nwhile neural networks trained on small datasets fail on new patients or require\nimpractical landmark supervision. We present DiffPose, a self-supervised\napproach that leverages patient-specific simulation and differentiable\nphysics-based rendering to achieve accurate 2D/3D registration without relying\non manually labeled data. Preoperatively, a CNN is trained to regress the pose\nof a randomly oriented synthetic X-ray rendered from the preoperative CT. The\nCNN then initializes rapid intraoperative test-time optimization that uses the\ndifferentiable X-ray renderer to refine the solution. Our work further proposes\nseveral geometrically principled methods for sampling camera poses from\n$\\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving\nregistration in the tangent space $\\mathfrak{se}(3)$ with geodesic and\nmultiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy\nacross surgical datasets at intraoperative speeds, improving upon existing\nunsupervised methods by an order of magnitude and even outperforming supervised\nbaselines. Our code is available at https://github.com/eigenvivek/DiffPose.",
        "date": "2023-12-11T13:05:54+00:00",
        "link": "http://arxiv.org/pdf/2312.06358v1"
    },
    {
        "title": "PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization",
        "authors": [
            "Xu Peng",
            "Junwei Zhu",
            "Boyuan Jiang",
            "Ying Tai",
            "Donghao Luo",
            "Jiangning Zhang",
            "Wei Lin",
            "Taisong Jin",
            "Chengjie Wang",
            "Rongrong Ji"
        ],
        "abstract": "Recent advancements in personalized image generation using diffusion models\nhave been noteworthy. However, existing methods suffer from inefficiencies due\nto the requirement for subject-specific fine-tuning. This computationally\nintensive process hinders efficient deployment, limiting practical usability.\nMoreover, these methods often grapple with identity distortion and limited\nexpression diversity. In light of these challenges, we propose PortraitBooth,\nan innovative approach designed for high efficiency, robust identity\npreservation, and expression-editable text-to-image generation, without the\nneed for fine-tuning. PortraitBooth leverages subject embeddings from a face\nrecognition model for personalized image generation without fine-tuning. It\neliminates computational overhead and mitigates identity distortion. The\nintroduced dynamic identity preservation strategy further ensures close\nresemblance to the original image identity. Moreover, PortraitBooth\nincorporates emotion-aware cross-attention control for diverse facial\nexpressions in generated images, supporting text-driven expression editing. Its\nscalability enables efficient and high-quality image creation, including\nmulti-subject generation. Extensive results demonstrate superior performance\nover other state-of-the-art methods in both single and multiple image\ngeneration scenarios.",
        "date": "2023-12-11T13:03:29+00:00",
        "link": "http://arxiv.org/pdf/2312.06354v1"
    },
    {
        "title": "NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations",
        "authors": [
            "Yuichi Inoue",
            "Yuki Yada",
            "Kotaro Tanahashi",
            "Yu Yamaguchi"
        ],
        "abstract": "Visual Question Answering (VQA) is one of the most important tasks in\nautonomous driving, which requires accurate recognition and complex situation\nevaluations. However, datasets annotated in a QA format, which guarantees\nprecise language generation and scene recognition from driving scenes, have not\nbeen established yet. In this work, we introduce Markup-QA, a novel dataset\nannotation technique in which QAs are enclosed within markups. This approach\nfacilitates the simultaneous evaluation of a model's capabilities in sentence\ngeneration and VQA. Moreover, using this annotation methodology, we designed\nthe NuScenes-MQA dataset. This dataset empowers the development of vision\nlanguage models, especially for autonomous driving tasks, by focusing on both\ndescriptive capabilities and precise QA. The dataset is available at\nhttps://github.com/turingmotors/NuScenes-MQA.",
        "date": "2023-12-11T12:58:54+00:00",
        "link": "http://arxiv.org/pdf/2312.06352v1"
    },
    {
        "title": "Evaluation of Large Language Models for Decision Making in Autonomous Driving",
        "authors": [
            "Kotaro Tanahashi",
            "Yuichi Inoue",
            "Yu Yamaguchi",
            "Hidetatsu Yaginuma",
            "Daiki Shiotsuka",
            "Hiroyuki Shimatani",
            "Kohei Iwamasa",
            "Yoshiaki Inoue",
            "Takafumi Yamaguchi",
            "Koki Igari",
            "Tsukasa Horinouchi",
            "Kento Tokuhiro",
            "Yugo Tokuchi",
            "Shunsuke Aoki"
        ],
        "abstract": "Various methods have been proposed for utilizing Large Language Models (LLMs)\nin autonomous driving. One strategy of using LLMs for autonomous driving\ninvolves inputting surrounding objects as text prompts to the LLMs, along with\ntheir coordinate and velocity information, and then outputting the subsequent\nmovements of the vehicle. When using LLMs for such purposes, capabilities such\nas spatial recognition and planning are essential. In particular, two\nfoundational capabilities are required: (1) spatial-aware decision making,\nwhich is the ability to recognize space from coordinate information and make\ndecisions to avoid collisions, and (2) the ability to adhere to traffic rules.\nHowever, quantitative research has not been conducted on how accurately\ndifferent types of LLMs can handle these problems. In this study, we\nquantitatively evaluated these two abilities of LLMs in the context of\nautonomous driving. Furthermore, to conduct a Proof of Concept (POC) for the\nfeasibility of implementing these abilities in actual vehicles, we developed a\nsystem that uses LLMs to drive a vehicle.",
        "date": "2023-12-11T12:56:40+00:00",
        "link": "http://arxiv.org/pdf/2312.06351v1"
    },
    {
        "title": "Semantic Connectivity-Driven Pseudo-labeling for Cross-domain Segmentation",
        "authors": [
            "Dong Zhao",
            "Ruizhi Yang",
            "Shuang Wang",
            "Qi Zang",
            "Yang Hu",
            "Licheng Jiao",
            "Nicu Sebe",
            "Zhun Zhong"
        ],
        "abstract": "Presently, self-training stands as a prevailing approach in cross-domain\nsemantic segmentation, enhancing model efficacy by training with pixels\nassigned with reliable pseudo-labels. However, we find two critical limitations\nin this paradigm. (1) The majority of reliable pixels exhibit a speckle-shaped\npattern and are primarily located in the central semantic region. This presents\nchallenges for the model in accurately learning semantics. (2) Category noise\nin speckle pixels is difficult to locate and correct, leading to error\naccumulation in self-training. To address these limitations, we propose a novel\napproach called Semantic Connectivity-driven pseudo-labeling (SeCo). This\napproach formulates pseudo-labels at the connectivity level and thus can\nfacilitate learning structured and low-noise semantics. Specifically, SeCo\ncomprises two key components: Pixel Semantic Aggregation (PSA) and Semantic\nConnectivity Correction (SCC). Initially, PSA divides semantics into 'stuff'\nand 'things' categories and aggregates speckled pseudo-labels into semantic\nconnectivity through efficient interaction with the Segment Anything Model\n(SAM). This enables us not only to obtain accurate boundaries but also\nsimplifies noise localization. Subsequently, SCC introduces a simple\nconnectivity classification task, which enables locating and correcting\nconnectivity noise with the guidance of loss distribution. Extensive\nexperiments demonstrate that SeCo can be flexibly applied to various\ncross-domain semantic segmentation tasks, including traditional unsupervised,\nsource-free, and black-box domain adaptation, significantly improving the\nperformance of existing state-of-the-art methods. The code is available at\nhttps://github.com/DZhaoXd/SeCo.",
        "date": "2023-12-11T12:29:51+00:00",
        "link": "http://arxiv.org/pdf/2312.06331v1"
    },
    {
        "title": "Navigating Open Set Scenarios for Skeleton-based Action Recognition",
        "authors": [
            "Kunyu Peng",
            "Cheng Yin",
            "Junwei Zheng",
            "Ruiping Liu",
            "David Schneider",
            "Jiaming Zhang",
            "Kailun Yang",
            "M. Saquib Sarfraz",
            "Rainer Stiefelhagen",
            "Alina Roitberg"
        ],
        "abstract": "In real-world scenarios, human actions often fall outside the distribution of\ntraining data, making it crucial for models to recognize known actions and\nreject unknown ones. However, using pure skeleton data in such open-set\nconditions poses challenges due to the lack of visual background cues and the\ndistinct sparse structure of body pose sequences. In this paper, we tackle the\nunexplored Open-Set Skeleton-based Action Recognition (OS-SAR) task and\nformalize the benchmark on three skeleton-based datasets. We assess the\nperformance of seven established open-set approaches on our task and identify\ntheir limits and critical generalization issues when dealing with skeleton\ninformation. To address these challenges, we propose a distance-based\ncross-modality ensemble method that leverages the cross-modal alignment of\nskeleton joints, bones, and velocities to achieve superior open-set recognition\nperformance. We refer to the key idea as CrossMax - an approach that utilizes a\nnovel cross-modality mean max discrepancy suppression mechanism to align latent\nspaces during training and a cross-modality distance-based logits refinement\nmethod during testing. CrossMax outperforms existing approaches and\nconsistently yields state-of-the-art results across all datasets and backbones.\nThe benchmark, code, and models will be released at\nhttps://github.com/KPeng9510/OS-SAR.",
        "date": "2023-12-11T12:29:32+00:00",
        "link": "http://arxiv.org/pdf/2312.06330v1"
    },
    {
        "title": "Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models",
        "authors": [
            "Yubin Wang",
            "Xinyang Jiang",
            "De Cheng",
            "Dongsheng Li",
            "Cairong Zhao"
        ],
        "abstract": "Prompt learning has become a prevalent strategy for adapting vision-language\nfoundation models to downstream tasks. As large language models (LLMs) have\nemerged, recent studies have explored the use of category-related descriptions\nas input to enhance prompt effectiveness. Nevertheless, conventional\ndescriptions fall short of structured information that effectively represents\nthe interconnections among entities or attributes linked to a particular\ncategory. To address this limitation and prioritize harnessing structured\nknowledge, this paper advocates for leveraging LLMs to build a graph for each\ndescription to model the entities and attributes describing the category, as\nwell as their correlations. Preexisting prompt tuning methods exhibit\ninadequacies in managing this structured knowledge. Consequently, we propose a\nnovel approach called Hierarchical Prompt Tuning (HPT), which enables\nsimultaneous modeling of both structured and conventional linguistic knowledge.\nSpecifically, we introduce a relationship-guided attention module to capture\npair-wise associations among entities and attributes for low-level prompt\nlearning. In addition, by incorporating high-level and global-level prompts\nmodeling overall semantics, the proposed hierarchical structure forges\ncross-level interlinks and empowers the model to handle more complex and\nlong-term relationships. Extensive experiments demonstrate that our HPT shows\nstrong effectiveness and generalizes much better than existing SOTA methods.\nOur code is available at https://github.com/Vill-Lab/2024-AAAI-HPT.",
        "date": "2023-12-11T12:14:06+00:00",
        "link": "http://arxiv.org/pdf/2312.06323v1"
    },
    {
        "title": "SemiSAM: Exploring SAM for Enhancing Semi-Supervised Medical Image Segmentation with Extremely Limited Annotations",
        "authors": [
            "Yichi Zhang",
            "Yuan Cheng",
            "Yuan Qi"
        ],
        "abstract": "Semi-supervised learning has attracted much attention due to its less\ndependence on acquiring abundant annotations from experts compared to fully\nsupervised methods, which is especially important for medical image\nsegmentation which typically requires intensive pixel/voxel-wise labeling by\ndomain experts. Although semi-supervised methods can improve the performance by\nutilizing unlabeled data, there are still gaps between fully supervised methods\nunder extremely limited annotation scenarios. In this paper, we propose a\nsimple yet efficient strategy to explore the usage of the Segment Anything\nModel (SAM) for enhancing semi-supervised medical image segmentation.\nConcretely, the segmentation model trained with domain knowledge provides\ninformation for localization and generating input prompts to the SAM. Then the\ngenerated pseudo-labels of SAM are utilized as additional supervision to assist\nin the learning procedure of the semi-supervised framework. Experimental\nresults demonstrate that SAM's assistance significantly enhances the\nperformance of existing semi-supervised frameworks, especially when only one or\na few labeled images are available.",
        "date": "2023-12-11T12:03:30+00:00",
        "link": "http://arxiv.org/pdf/2312.06316v1"
    },
    {
        "title": "Attribute Annotation and Bias Evaluation in Visual Datasets for Autonomous Driving",
        "authors": [
            "David Fernández Llorca",
            "Pedro Frau",
            "Ignacio Parra",
            "Rubén Izquierdo",
            "Emilia Gómez"
        ],
        "abstract": "This paper addresses the often overlooked issue of fairness in the autonomous\ndriving domain, particularly in vision-based perception and prediction systems,\nwhich play a pivotal role in the overall functioning of Autonomous Vehicles\n(AVs). We focus our analysis on biases present in some of the most commonly\nused visual datasets for training person and vehicle detection systems. We\nintroduce an annotation methodology and a specialised annotation tool, both\ndesigned to annotate protected attributes of agents in visual datasets. We\nvalidate our methodology through an inter-rater agreement analysis and provide\nthe distribution of attributes across all datasets. These include annotations\nfor the attributes age, sex, skin tone, group, and means of transport for more\nthan 90K people, as well as vehicle type, colour, and car type for over 50K\nvehicles. Generally, diversity is very low for most attributes, with some\ngroups, such as children, wheelchair users, or personal mobility vehicle users,\nbeing extremely underrepresented in the analysed datasets. The study\ncontributes significantly to efforts to consider fairness in the evaluation of\nperception and prediction systems for AVs. This paper follows reproducibility\nprinciples. The annotation tool, scripts and the annotated attributes can be\naccessed publicly at https://github.com/ec-jrc/humaint_annotator.",
        "date": "2023-12-11T11:27:01+00:00",
        "link": "http://arxiv.org/pdf/2312.06306v1"
    },
    {
        "title": "RCA-NOC: Relative Contrastive Alignment for Novel Object Captioning",
        "authors": [
            "Jiashuo Fan",
            "Yaoyuan Liang",
            "Leyao Liu",
            "Shaolun Huang",
            "Lei Zhang"
        ],
        "abstract": "In this paper, we introduce a novel approach to novel object captioning which\nemploys relative contrastive learning to learn visual and semantic alignment.\nOur approach maximizes compatibility between regions and object tags in a\ncontrastive manner. To set up a proper contrastive learning objective, for each\nimage, we augment tags by leveraging the relative nature of positive and\nnegative pairs obtained from foundation models such as CLIP. We then use the\nrank of each augmented tag in a list as a relative relevance label to contrast\neach top-ranked tag with a set of lower-ranked tags. This learning objective\nencourages the top-ranked tags to be more compatible with their image and text\ncontext than lower-ranked tags, thus improving the discriminative ability of\nthe learned multi-modality representation. We evaluate our approach on two\ndatasets and show that our proposed RCA-NOC approach outperforms\nstate-of-the-art methods by a large margin, demonstrating its effectiveness in\nimproving vision-language representation for novel object captioning.",
        "date": "2023-12-11T11:06:32+00:00",
        "link": "http://arxiv.org/pdf/2312.06299v1"
    },
    {
        "title": "Cataract-1K: Cataract Surgery Dataset for Scene Segmentation, Phase Recognition, and Irregularity Detection",
        "authors": [
            "Negin Ghamsarian",
            "Yosuf El-Shabrawi",
            "Sahar Nasirihaghighi",
            "Doris Putzgruber-Adamitsch",
            "Martin Zinkernagel",
            "Sebastian Wolf",
            "Klaus Schoeffmann",
            "Raphael Sznitman"
        ],
        "abstract": "In recent years, the landscape of computer-assisted interventions and\npost-operative surgical video analysis has been dramatically reshaped by\ndeep-learning techniques, resulting in significant advancements in surgeons'\nskills, operation room management, and overall surgical outcomes. However, the\nprogression of deep-learning-powered surgical technologies is profoundly\nreliant on large-scale datasets and annotations. Particularly, surgical scene\nunderstanding and phase recognition stand as pivotal pillars within the realm\nof computer-assisted surgery and post-operative assessment of cataract surgery\nvideos. In this context, we present the largest cataract surgery video dataset\nthat addresses diverse requisites for constructing computerized surgical\nworkflow analysis and detecting post-operative irregularities in cataract\nsurgery. We validate the quality of annotations by benchmarking the performance\nof several state-of-the-art neural network architectures for phase recognition\nand surgical scene segmentation. Besides, we initiate the research on domain\nadaptation for instrument segmentation in cataract surgery by evaluating\ncross-domain instrument segmentation performance in cataract surgery videos.\nThe dataset and annotations will be publicly available upon acceptance of the\npaper.",
        "date": "2023-12-11T10:53:05+00:00",
        "link": "http://arxiv.org/pdf/2312.06295v1"
    },
    {
        "title": "Compensation Sampling for Improved Convergence in Diffusion Models",
        "authors": [
            "Hui Lu",
            "Albert ali Salah",
            "Ronald Poppe"
        ],
        "abstract": "Diffusion models achieve remarkable quality in image generation, but at a\ncost. Iterative denoising requires many time steps to produce high fidelity\nimages. We argue that the denoising process is crucially limited by an\naccumulation of the reconstruction error due to an initial inaccurate\nreconstruction of the target data. This leads to lower quality outputs, and\nslower convergence. To address this issue, we propose compensation sampling to\nguide the generation towards the target domain. We introduce a compensation\nterm, implemented as a U-Net, which adds negligible computation overhead during\ntraining and, optionally, inference. Our approach is flexible and we\ndemonstrate its application in unconditional generation, face inpainting, and\nface de-occlusion using benchmark datasets CIFAR-10, CelebA, CelebA-HQ,\nFFHQ-256, and FSG. Our approach consistently yields state-of-the-art results in\nterms of image quality, while accelerating the denoising process to converge\nduring training by up to an order of magnitude.",
        "date": "2023-12-11T10:39:01+00:00",
        "link": "http://arxiv.org/pdf/2312.06285v1"
    },
    {
        "title": "DG-TTA: Out-of-domain medical image segmentation through Domain Generalization and Test-Time Adaptation",
        "authors": [
            "Christian Weihsbach",
            "Christian N. Kruse",
            "Alexander Bigalke",
            "Mattias P. Heinrich"
        ],
        "abstract": "Applying pre-trained medical segmentation models on out-of-domain images\noften yields predictions of insufficient quality. Several strategies have been\nproposed to maintain model performance, such as finetuning or unsupervised- and\nsource-free domain adaptation. These strategies set restrictive requirements\nfor data availability. In this study, we propose to combine domain\ngeneralization and test-time adaptation to create a highly effective approach\nfor reusing pre-trained models in unseen target domains. Domain-generalized\npre-training on source data is used to obtain the best initial performance in\nthe target domain. We introduce the MIND descriptor previously used in image\nregistration tasks as a further technique to achieve generalization and present\nsuperior performance for small-scale datasets compared to existing approaches.\nAt test-time, high-quality segmentation for every single unseen scan is ensured\nby optimizing the model weights for consistency given different image\naugmentations. That way, our method enables separate use of source and target\ndata and thus removes current data availability barriers. Moreover, the\npresented method is highly modular as it does not require specific model\narchitectures or prior knowledge of involved domains and labels. We demonstrate\nthis by integrating it into the nnUNet, which is currently the most popular and\naccurate framework for medical image segmentation. We employ multiple datasets\ncovering abdominal, cardiac, and lumbar spine scans and compose several\nout-of-domain scenarios in this study. We demonstrate that our method, combined\nwith pre-trained whole-body CT models, can effectively segment MR images with\nhigh accuracy in all of the aforementioned scenarios. Open-source code can be\nfound here: https://github.com/multimodallearning/DG-TTA",
        "date": "2023-12-11T10:26:21+00:00",
        "link": "http://arxiv.org/pdf/2312.06275v1"
    },
    {
        "title": "Regroup Median Loss for Combating Label Noise",
        "authors": [
            "Fengpeng Li",
            "Kemou Li",
            "Jinyu Tian",
            "Jiantao Zhou"
        ],
        "abstract": "The deep model training procedure requires large-scale datasets of annotated\ndata. Due to the difficulty of annotating a large number of samples, label\nnoise caused by incorrect annotations is inevitable, resulting in low model\nperformance and poor model generalization. To combat label noise, current\nmethods usually select clean samples based on the small-loss criterion and use\nthese samples for training. Due to some noisy samples similar to clean ones,\nthese small-loss criterion-based methods are still affected by label noise. To\naddress this issue, in this work, we propose Regroup Median Loss (RML) to\nreduce the probability of selecting noisy samples and correct losses of noisy\nsamples. RML randomly selects samples with the same label as the training\nsamples based on a new loss processing method. Then, we combine the stable mean\nloss and the robust median loss through a proposed regrouping strategy to\nobtain robust loss estimation for noisy samples. To further improve the model\nperformance against label noise, we propose a new sample selection strategy and\nbuild a semi-supervised method based on RML. Compared to state-of-the-art\nmethods, for both the traditionally trained and semi-supervised models, RML\nachieves a significant improvement on synthetic and complex real-world\ndatasets. The source code of the paper has been released.",
        "date": "2023-12-11T10:19:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06273v1"
    },
    {
        "title": "U-MixFormer: UNet-like Transformer with Mix-Attention for Efficient Semantic Segmentation",
        "authors": [
            "Seul-Ki Yeom",
            "Julian von Klitzing"
        ],
        "abstract": "Semantic segmentation has witnessed remarkable advancements with the\nadaptation of the Transformer architecture. Parallel to the strides made by the\nTransformer, CNN-based U-Net has seen significant progress, especially in\nhigh-resolution medical imaging and remote sensing. This dual success inspired\nus to merge the strengths of both, leading to the inception of a U-Net-based\nvision transformer decoder tailored for efficient contextual encoding. Here, we\npropose a novel transformer decoder, U-MixFormer, built upon the U-Net\nstructure, designed for efficient semantic segmentation. Our approach\ndistinguishes itself from the previous transformer methods by leveraging\nlateral connections between the encoder and decoder stages as feature queries\nfor the attention modules, apart from the traditional reliance on skip\nconnections. Moreover, we innovatively mix hierarchical feature maps from\nvarious encoder and decoder stages to form a unified representation for keys\nand values, giving rise to our unique mix-attention module. Our approach\ndemonstrates state-of-the-art performance across various configurations.\nExtensive experiments show that U-MixFormer outperforms SegFormer, FeedFormer,\nand SegNeXt by a large margin. For example, U-MixFormer-B0 surpasses\nSegFormer-B0 and FeedFormer-B0 with 3.8% and 2.0% higher mIoU and 27.3% and\n21.8% less computation and outperforms SegNext with 3.3% higher mIoU with\nMSCAN-T encoder on ADE20K. Code available at\nhttps://github.com/julian-klitzing/u-mixformer.",
        "date": "2023-12-11T10:19:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06272v1"
    },
    {
        "title": "Adaptive Annotation Distribution for Weakly Supervised Point Cloud Semantic Segmentation",
        "authors": [
            "Zhiyi Pan",
            "Nan Zhang",
            "Wei Gao",
            "Shan Liu",
            "Ge Li"
        ],
        "abstract": "Weakly supervised point cloud semantic segmentation has attracted a lot of\nattention due to its ability to alleviate the heavy reliance on fine-grained\nannotations of point clouds. However, in practice, sparse annotation usually\nexhibits a distinct non-uniform distribution in point cloud, which poses\nchallenges for weak supervision. To address these issues, we propose an\nadaptive annotation distribution method for weakly supervised point cloud\nsemantic segmentation. Specifically, we introduce the probability density\nfunction into the gradient sampling approximation analysis and investigate the\nimpact of sparse annotations distributions. Based on our analysis, we propose a\nlabel-aware point cloud downsampling strategy to increase the proportion of\nannotations involved in the training stage. Furthermore, we design the\nmultiplicative dynamic entropy as the gradient calibration function to mitigate\nthe gradient bias caused by non-uniformly distributed sparse annotations and\nexplicitly reduce the epistemic uncertainty. Without any prior restrictions and\nadditional information, our proposed method achieves comprehensive performance\nimprovements at multiple label rates with different annotation distributions on\nS3DIS, ScanNetV2 and SemanticKITTI.",
        "date": "2023-12-11T09:57:09+00:00",
        "link": "http://arxiv.org/pdf/2312.06259v1"
    },
    {
        "title": "UIEDP:Underwater Image Enhancement with Diffusion Prior",
        "authors": [
            "Dazhao Du",
            "Enhan Li",
            "Lingyu Si",
            "Fanjiang Xu",
            "Jianwei Niu",
            "Fuchun Sun"
        ],
        "abstract": "Underwater image enhancement (UIE) aims to generate clear images from\nlow-quality underwater images. Due to the unavailability of clear reference\nimages, researchers often synthesize them to construct paired datasets for\ntraining deep models. However, these synthesized images may sometimes lack\nquality, adversely affecting training outcomes. To address this issue, we\npropose UIE with Diffusion Prior (UIEDP), a novel framework treating UIE as a\nposterior distribution sampling process of clear images conditioned on degraded\nunderwater inputs. Specifically, UIEDP combines a pre-trained diffusion model\ncapturing natural image priors with any existing UIE algorithm, leveraging the\nlatter to guide conditional generation. The diffusion prior mitigates the\ndrawbacks of inferior synthetic images, resulting in higher-quality image\ngeneration. Extensive experiments have demonstrated that our UIEDP yields\nsignificant improvements across various metrics, especially no-reference image\nquality assessment. And the generated enhanced images also exhibit a more\nnatural appearance.",
        "date": "2023-12-11T09:24:52+00:00",
        "link": "http://arxiv.org/pdf/2312.06240v1"
    },
    {
        "title": "Invariant Representation Learning via Decoupling Style and Spurious Features",
        "authors": [
            "Ruimeng Li",
            "Yuanhao Pu",
            "Zhaoyi Li",
            "Hong Xie",
            "Defu Lian"
        ],
        "abstract": "This paper considers the out-of-distribution (OOD) generalization problem\nunder the setting that both style distribution shift and spurious features\nexist and domain labels are missing. This setting frequently arises in\nreal-world applications and is underlooked because previous approaches mainly\nhandle either of these two factors. The critical challenge is decoupling style\nand spurious features in the absence of domain labels. To address this\nchallenge, we first propose a structural causal model (SCM) for the image\ngeneration process, which captures both style distribution shift and spurious\nfeatures. The proposed SCM enables us to design a new framework called IRSS,\nwhich can gradually separate style distribution and spurious features from\nimages by introducing adversarial neural networks and multi-environment\noptimization, thus achieving OOD generalization. Moreover, it does not require\nadditional supervision (e.g., domain labels) other than the images and their\ncorresponding labels. Experiments on benchmark datasets demonstrate that IRSS\noutperforms traditional OOD methods and solves the problem of Invariant risk\nminimization (IRM) degradation, enabling the extraction of invariant features\nunder distribution shift.",
        "date": "2023-12-11T09:14:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06226v1"
    },
    {
        "title": "Medical Vision Language Pretraining: A survey",
        "authors": [
            "Prashant Shrestha",
            "Sanskar Amgain",
            "Bidur Khanal",
            "Cristian A. Linte",
            "Binod Bhattarai"
        ],
        "abstract": "Medical Vision Language Pretraining (VLP) has recently emerged as a promising\nsolution to the scarcity of labeled data in the medical domain. By leveraging\npaired/unpaired vision and text datasets through self-supervised learning,\nmodels can be trained to acquire vast knowledge and learn robust feature\nrepresentations. Such pretrained models have the potential to enhance multiple\ndownstream medical tasks simultaneously, reducing the dependency on labeled\ndata. However, despite recent progress and its potential, there is no such\ncomprehensive survey paper that has explored the various aspects and\nadvancements in medical VLP. In this paper, we specifically review existing\nworks through the lens of different pretraining objectives, architectures,\ndownstream evaluation tasks, and datasets utilized for pretraining and\ndownstream tasks. Subsequently, we delve into current challenges in medical\nVLP, discussing existing and potential solutions, and conclude by highlighting\nfuture directions. To the best of our knowledge, this is the first survey\nfocused on medical VLP.",
        "date": "2023-12-11T09:14:13+00:00",
        "link": "http://arxiv.org/pdf/2312.06224v1"
    },
    {
        "title": "CSOT: Curriculum and Structure-Aware Optimal Transport for Learning with Noisy Labels",
        "authors": [
            "Wanxing Chang",
            "Ye Shi",
            "Jingya Wang"
        ],
        "abstract": "Learning with noisy labels (LNL) poses a significant challenge in training a\nwell-generalized model while avoiding overfitting to corrupted labels. Recent\nadvances have achieved impressive performance by identifying clean labels and\ncorrecting corrupted labels for training. However, the current approaches rely\nheavily on the model's predictions and evaluate each sample independently\nwithout considering either the global and local structure of the sample\ndistribution. These limitations typically result in a suboptimal solution for\nthe identification and correction processes, which eventually leads to models\noverfitting to incorrect labels. In this paper, we propose a novel optimal\ntransport (OT) formulation, called Curriculum and Structure-aware Optimal\nTransport (CSOT). CSOT concurrently considers the inter- and intra-distribution\nstructure of the samples to construct a robust denoising and relabeling\nallocator. During the training process, the allocator incrementally assigns\nreliable labels to a fraction of the samples with the highest confidence. These\nlabels have both global discriminability and local coherence. Notably, CSOT is\na new OT formulation with a nonconvex objective function and curriculum\nconstraints, so it is not directly compatible with classical OT solvers. Here,\nwe develop a lightspeed computational method that involves a scaling iteration\nwithin a generalized conditional gradient framework to solve CSOT efficiently.\nExtensive experiments demonstrate the superiority of our method over the\ncurrent state-of-the-arts in LNL. Code is available at\nhttps://github.com/changwxx/CSOT-for-LNL.",
        "date": "2023-12-11T09:12:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06221v1"
    },
    {
        "title": "The Journey, Not the Destination: How Data Guides Diffusion Models",
        "authors": [
            "Kristian Georgiev",
            "Joshua Vendrow",
            "Hadi Salman",
            "Sung Min Park",
            "Aleksander Madry"
        ],
        "abstract": "Diffusion models trained on large datasets can synthesize photo-realistic\nimages of remarkable quality and diversity. However, attributing these images\nback to the training data-that is, identifying specific training examples which\ncaused an image to be generated-remains a challenge. In this paper, we propose\na framework that: (i) provides a formal notion of data attribution in the\ncontext of diffusion models, and (ii) allows us to counterfactually validate\nsuch attributions. Then, we provide a method for computing these attributions\nefficiently. Finally, we apply our method to find (and evaluate) such\nattributions for denoising diffusion probabilistic models trained on CIFAR-10\nand latent diffusion models trained on MS COCO. We provide code at\nhttps://github.com/MadryLab/journey-TRAK .",
        "date": "2023-12-11T08:39:43+00:00",
        "link": "http://arxiv.org/pdf/2312.06205v1"
    },
    {
        "title": "Towards Transferable Adversarial Attacks with Centralized Perturbation",
        "authors": [
            "Shangbo Wu",
            "Yu-an Tan",
            "Yajie Wang",
            "Ruinan Ma",
            "Wencong Ma",
            "Yuanzhang Li"
        ],
        "abstract": "Adversarial transferability enables black-box attacks on unknown victim deep\nneural networks (DNNs), rendering attacks viable in real-world scenarios.\nCurrent transferable attacks create adversarial perturbation over the entire\nimage, resulting in excessive noise that overfit the source model.\nConcentrating perturbation to dominant image regions that are model-agnostic is\ncrucial to improving adversarial efficacy. However, limiting perturbation to\nlocal regions in the spatial domain proves inadequate in augmenting\ntransferability. To this end, we propose a transferable adversarial attack with\nfine-grained perturbation optimization in the frequency domain, creating\ncentralized perturbation. We devise a systematic pipeline to dynamically\nconstrain perturbation optimization to dominant frequency coefficients. The\nconstraint is optimized in parallel at each iteration, ensuring the directional\nalignment of perturbation optimization with model prediction. Our approach\nallows us to centralize perturbation towards sample-specific important\nfrequency features, which are shared by DNNs, effectively mitigating source\nmodel overfitting. Experiments demonstrate that by dynamically centralizing\nperturbation on dominating frequency coefficients, crafted adversarial examples\nexhibit stronger transferability, and allowing them to bypass various defenses.",
        "date": "2023-12-11T08:25:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06199v1"
    },
    {
        "title": "Optimized View and Geometry Distillation from Multi-view Diffuser",
        "authors": [
            "Youjia Zhang",
            "Junqing Yu",
            "Zikai Song",
            "Wei Yang"
        ],
        "abstract": "Generating multi-view images from a single input view using image-conditioned\ndiffusion models is a recent advancement and has shown considerable potential.\nHowever, issues such as the lack of consistency in synthesized views and\nover-smoothing in extracted geometry persist. Previous methods integrate\nmulti-view consistency modules or impose additional supervisory to enhance view\nconsistency while compromising on the flexibility of camera positioning and\nlimiting the versatility of view synthesis. In this study, we consider the\nradiance field optimized during geometry extraction as a more rigid consistency\nprior, compared to volume and ray aggregation used in previous works. We\nfurther identify and rectify a critical bias in the traditional radiance field\noptimization process through score distillation from a multi-view diffuser. We\nintroduce an Unbiased Score Distillation (USD) that utilizes unconditioned\nnoises from a 2D diffusion model, greatly refining the radiance field fidelity.\nwe leverage the rendered views from the optimized radiance field as the basis\nand develop a two-step specialization process of a 2D diffusion model, which is\nadept at conducting object-specific denoising and generating high-quality\nmulti-view images. Finally, we recover faithful geometry and texture directly\nfrom the refined multi-view images. Empirical evaluations demonstrate that our\noptimized geometry and view distillation technique generates comparable results\nto the state-of-the-art models trained on extensive datasets, all while\nmaintaining freedom in camera positioning.",
        "date": "2023-12-11T08:22:24+00:00",
        "link": "http://arxiv.org/pdf/2312.06198v1"
    },
    {
        "title": "DisControlFace: Disentangled Control for Personalized Facial Image Editing",
        "authors": [
            "Haozhe Jia",
            "Yan Li",
            "Hengfei Cui",
            "Di Xu",
            "Changpeng Yang",
            "Yuwang Wang",
            "Tao Yu"
        ],
        "abstract": "In this work, we focus on exploring explicit fine-grained control of\ngenerative facial image editing, all while generating faithful and consistent\npersonalized facial appearances. We identify the key challenge of this task as\nthe exploration of disentangled conditional control in the generation process,\nand accordingly propose a novel diffusion-based framework, named\nDisControlFace, comprising two decoupled components. Specifically, we leverage\nan off-the-shelf diffusion reconstruction model as the backbone and freeze its\npre-trained weights, which helps to reduce identity shift and recover\nediting-unrelated details of the input image. Furthermore, we construct a\nparallel control network that is compatible with the reconstruction backbone to\ngenerate spatial control conditions based on estimated explicit face\nparameters. Finally, we further reformulate the training pipeline into a\nmasked-autoencoding form to effectively achieve disentangled training of our\nDisControlFace. Our DisControlNet can perform robust editing on any facial\nimage through training on large-scale 2D in-the-wild portraits and also\nsupports low-cost fine-tuning with few additional images to further learn\ndiverse personalized priors of a specific person. Extensive experiments\ndemonstrate that DisControlFace can generate realistic facial images\ncorresponding to various face control conditions, while significantly improving\nthe preservation of the personalized facial details.",
        "date": "2023-12-11T08:16:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06193v1"
    },
    {
        "title": "NutritionVerse-Synth: An Open Access Synthetically Generated 2D Food Scene Dataset for Dietary Intake Estimation",
        "authors": [
            "Saeejith Nair",
            "Chi-en Amy Tai",
            "Yuhao Chen",
            "Alexander Wong"
        ],
        "abstract": "Manually tracking nutritional intake via food diaries is error-prone and\nburdensome. Automated computer vision techniques show promise for dietary\nmonitoring but require large and diverse food image datasets. To address this\nneed, we introduce NutritionVerse-Synth (NV-Synth), a large-scale synthetic\nfood image dataset. NV-Synth contains 84,984 photorealistic meal images\nrendered from 7,082 dynamically plated 3D scenes. Each scene is captured from\n12 viewpoints and includes perfect ground truth annotations such as RGB, depth,\nsemantic, instance, and amodal segmentation masks, bounding boxes, and detailed\nnutritional information per food item. We demonstrate the diversity of NV-Synth\nacross foods, compositions, viewpoints, and lighting. As the largest\nopen-source synthetic food dataset, NV-Synth highlights the value of\nphysics-based simulations for enabling scalable and controllable generation of\ndiverse photorealistic meal images to overcome data limitations and drive\nadvancements in automated dietary assessment using computer vision. In addition\nto the dataset, the source code for our data generation framework is also made\npublicly available at https://saeejithnair.github.io/nvsynth.",
        "date": "2023-12-11T08:15:49+00:00",
        "link": "http://arxiv.org/pdf/2312.06192v1"
    },
    {
        "title": "SP-DiffDose: A Conditional Diffusion Model for Radiation Dose Prediction Based on Multi-Scale Fusion of Anatomical Structures, Guided by SwinTransformer and Projector",
        "authors": [
            "Linjie Fu",
            "Xia Li",
            "Xiuding Cai",
            "Yingkai Wang",
            "Xueyao Wang",
            "Yu Yao",
            "Yali Shen"
        ],
        "abstract": "Radiation therapy serves as an effective and standard method for cancer\ntreatment. Excellent radiation therapy plans always rely on high-quality dose\ndistribution maps obtained through repeated trial and error by experienced\nexperts. However, due to individual differences and complex clinical\nsituations, even seasoned expert teams may need help to achieve the best\ntreatment plan every time quickly. Many automatic dose distribution prediction\nmethods have been proposed recently to accelerate the radiation therapy\nplanning process and have achieved good results. However, these results suffer\nfrom over-smoothing issues, with the obtained dose distribution maps needing\nmore high-frequency details, limiting their clinical application. To address\nthese limitations, we propose a dose prediction diffusion model based on\nSwinTransformer and a projector, SP-DiffDose. To capture the direct correlation\nbetween anatomical structure and dose distribution maps, SP-DiffDose uses a\nstructural encoder to extract features from anatomical images, then employs a\nconditional diffusion process to blend noise and anatomical images at multiple\nscales and gradually map them to dose distribution maps. To enhance the dose\nprediction distribution for organs at risk, SP-DiffDose utilizes\nSwinTransformer in the deeper layers of the network to capture features at\ndifferent scales in the image. To learn good representations from the fused\nfeatures, SP-DiffDose passes the fused features through a designed projector,\nimproving dose prediction accuracy. Finally, we evaluate SP-DiffDose on an\ninternal dataset. The results show that SP-DiffDose outperforms existing\nmethods on multiple evaluation metrics, demonstrating the superiority and\ngeneralizability of our method.",
        "date": "2023-12-11T08:07:41+00:00",
        "link": "http://arxiv.org/pdf/2312.06187v1"
    },
    {
        "title": "Recent Advances in Deterministic Human Motion Prediction: A Review",
        "authors": [
            "Tenghao Deng",
            "Yan Sun"
        ],
        "abstract": "In recent years, with the continuous advancement of deep learning and the\nemergence of large-scale human motion datasets, human motion prediction\ntechnology has gradually gained prominence in various fields such as\nhuman-computer interaction, autonomous driving, sports analysis, and personnel\ntracking. This article introduces common model architectures in this domain\nalong with their respective advantages and disadvantages. It also\nsystematically summarizes recent research innovations, focusing on in-depth\ndiscussions of relevant papers in these areas, thereby highlighting\nforward-looking insights into the field's development. Furthermore, this paper\nprovides a comprehensive overview of existing methods, commonly used datasets,\nand evaluation metrics in this field. Finally, it discusses some of the current\nlimitations in the field and proposes potential future research directions to\naddress these challenges and promote further advancements in human motion\nprediction.",
        "date": "2023-12-11T07:54:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06184v1"
    },
    {
        "title": "Dynamic Weighted Combiner for Mixed-Modal Image Retrieval",
        "authors": [
            "Fuxiang Huang",
            "Lei Zhang",
            "Xiaowei Fu",
            "Suqi Song"
        ],
        "abstract": "Mixed-Modal Image Retrieval (MMIR) as a flexible search paradigm has\nattracted wide attention. However, previous approaches always achieve limited\nperformance, due to two critical factors are seriously overlooked. 1) The\ncontribution of image and text modalities is different, but incorrectly treated\nequally. 2) There exist inherent labeling noises in describing users'\nintentions with text in web datasets from diverse real-world scenarios, giving\nrise to overfitting. We propose a Dynamic Weighted Combiner (DWC) to tackle the\nabove challenges, which includes three merits. First, we propose an Editable\nModality De-equalizer (EMD) by taking into account the contribution disparity\nbetween modalities, containing two modality feature editors and an adaptive\nweighted combiner. Second, to alleviate labeling noises and data bias, we\npropose a dynamic soft-similarity label generator (SSG) to implicitly improve\nnoisy supervision. Finally, to bridge modality gaps and facilitate similarity\nlearning, we propose a CLIP-based mutual enhancement module alternately trained\nby a mixed-modality contrastive loss. Extensive experiments verify that our\nproposed model significantly outperforms state-of-the-art methods on real-world\ndatasets. The source code is available at\n\\url{https://github.com/fuxianghuang1/DWC}.",
        "date": "2023-12-11T07:36:45+00:00",
        "link": "http://arxiv.org/pdf/2312.06179v1"
    },
    {
        "title": "Jointly Explicit and Implicit Cross-Modal Interaction Network for Anterior Chamber Inflammation Diagnosis",
        "authors": [
            "Qian Shao",
            "Ye Dai",
            "Haochao Ying",
            "Kan Xu",
            "Jinhong Wang",
            "Wei Chi",
            "Jian Wu"
        ],
        "abstract": "Uveitis demands the precise diagnosis of anterior chamber inflammation (ACI)\nfor optimal treatment. However, current diagnostic methods only rely on a\nlimited single-modal disease perspective, which leads to poor performance. In\nthis paper, we investigate a promising yet challenging way to fuse multimodal\ndata for ACI diagnosis. Notably, existing fusion paradigms focus on empowering\nimplicit modality interactions (i.e., self-attention and its variants), but\nneglect to inject explicit modality interactions, especially from clinical\nknowledge and imaging property. To this end, we propose a jointly Explicit and\nimplicit Cross-Modal Interaction Network (EiCI-Net) for Anterior Chamber\nInflammation Diagnosis that uses anterior segment optical coherence tomography\n(AS-OCT) images, slit-lamp images, and clinical data jointly. Specifically, we\nfirst develop CNN-Based Encoders and Tabular Processing Module (TPM) to extract\nefficient feature representations in different modalities. Then, we devise an\nExplicit Cross-Modal Interaction Module (ECIM) to generate attention maps as a\nkind of explicit clinical knowledge based on the tabular feature maps, then\nintegrated them into the slit-lamp feature maps, allowing the CNN-Based Encoder\nto focus on more effective informativeness of the slit-lamp images. After that,\nthe Implicit Cross-Modal Interaction Module (ICIM), a transformer-based\nnetwork, further implicitly enhances modality interactions. Finally, we\nconstruct a considerable real-world dataset from our collaborative hospital and\nconduct sufficient experiments to demonstrate the superior performance of our\nproposed EiCI-Net compared with the state-of-the-art classification methods in\nvarious metrics.",
        "date": "2023-12-11T07:20:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06171v1"
    },
    {
        "title": "Bag of Tricks: Semi-Supervised Cross-domain Crater Detection with Poor Data Quality",
        "authors": [
            "Yifan Liu",
            "Tiecheng Song"
        ],
        "abstract": "With the development of spaceflight and the exploration of extraterrestrial\nplanets, exoplanet crater detection has gradually gained attention. However,\nwith the current scarcity of relevant datasets, high sample background\ncomplexity, and large inter-domain differences, few existing detection models\ncan achieve good robustness and generalization across domains by training on\ndata with more background interference. To obtain a better robust model with\nbetter cross-domain generalization in the presence of poor data quality, we\npropose the SCPQ model, in which we first propose a method for fusing shallow\ninformation using attention mechanism (FSIAM), which utilizes feature maps\nfused with deep convolved feature maps after fully extracting the global\nsensory field of shallow information via the attention mechanism module, which\ncan fully fit the data to obtain a better sense of the domain in the presence\nof poor data, and thus better multiscale adaptability. Secondly, we propose a\npseudo-label and data augment strategy (PDAS) and a smooth hard example mining\n(SHEM) loss function to improve cross-domain performance. PDAS adopts\nhigh-quality pseudo-labeled data from the target domain to the finetune model,\nand adopts different strong and weak data enhancement strategies for different\ndomains, which mitigates the different distribution of information inherent in\nthe source and target domains, and obtains a better generalization effect.\nMeanwhile, our proposed SHEM loss function can solve the problem of poor\nrobustness of hard examples due to partial background interference learning\nduring the training process. The SHEM loss function can smooth this\ninterference and has generalization while learning hard examples. Experimental\nresults show that we achieved better performance on the DACD dataset and\nimproved the Recall of cross-domain detection by 24.04\\% over baseline.",
        "date": "2023-12-11T07:16:49+00:00",
        "link": "http://arxiv.org/pdf/2312.06169v1"
    },
    {
        "title": "Implicit Shape Modeling for Anatomical Structure Refinement of Volumetric Medical Images",
        "authors": [
            "Minghui Zhang",
            "Hanxiao Zhang",
            "Xin You",
            "Yun Gu"
        ],
        "abstract": "Shape modeling of volumetric medical images is a critical task for\nquantitative analysis and surgical plans in computer-aided diagnosis. To\nrelieve the burden of expert clinicians, the reconstructed shapes are widely\nacquired from deep learning models, e.g. Convolutional Neural Networks (CNNs),\nfollowed by marching cube algorithm. However, automatically obtaining\nreconstructed shapes can not always achieve perfect results due to the limited\nresolution of images and lack of shape prior constraints. In this paper, we\ndesign a unified framework for the refinement of medical image segmentation on\ntop of an implicit neural network. Specifically, To learn a sharable shape\nprior from different instances within the same category in the training phase,\nthe physical information of volumetric medical images are firstly utilized to\nconstruct the Physical-Informed Continuous Coordinate Transform (PICCT). PICCT\ntransforms the input data in an aligned manner fed into the implicit shape\nmodeling. To better learn shape representation, we introduce implicit shape\nconstraints on top of the signed distance function (SDF) into the implicit\nshape modeling of both instances and latent template. For the inference phase,\na template interaction module (TIM) is proposed to refine initial results\nproduced by CNNs via deforming deep implicit templates with latent codes.\nExperimental results on three datasets demonstrated the superiority of our\napproach in shape refinement. The Chamfer Distance/Earth Mover's Distance\nachieved by the proposed method are 0.232/0.087 on the Liver dataset,\n0.128/0.069 on the Pancreas dataset, and 0.417/0.100 on the Lung Lobe dataset.",
        "date": "2023-12-11T07:09:32+00:00",
        "link": "http://arxiv.org/pdf/2312.06164v1"
    },
    {
        "title": "Adversarial Camera Patch: An Effective and Robust Physical-World Attack on Object Detectors",
        "authors": [
            "Kalibinuer Tiliwalidi"
        ],
        "abstract": "Nowadays, the susceptibility of deep neural networks (DNNs) has garnered\nsignificant attention. Researchers are exploring patch-based physical attacks,\nyet traditional approaches, while effective, often result in conspicuous\npatches covering target objects. This leads to easy detection by human\nobservers. Recently, novel camera-based physical attacks have emerged,\nleveraging camera patches to execute stealthy attacks. These methods circumvent\ntarget object modifications by introducing perturbations directly to the camera\nlens, achieving a notable breakthrough in stealthiness. However, prevailing\ncamera-based strategies necessitate the deployment of multiple patches on the\ncamera lens, which introduces complexity. To address this issue, we propose an\nAdversarial Camera Patch (ADCP).",
        "date": "2023-12-11T06:56:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06163v1"
    },
    {
        "title": "Textual Prompt Guided Image Restoration",
        "authors": [
            "Qiuhai Yan",
            "Aiwen Jiang",
            "Kang Chen",
            "Long Peng",
            "Qiaosi Yi",
            "Chunjie Zhang"
        ],
        "abstract": "Image restoration has always been a cutting-edge topic in the academic and\nindustrial fields of computer vision. Since degradation signals are often\nrandom and diverse, \"all-in-one\" models that can do blind image restoration\nhave been concerned in recent years. Early works require training specialized\nheaders and tails to handle each degradation of concern, which are manually\ncumbersome. Recent works focus on learning visual prompts from data\ndistribution to identify degradation type. However, the prompts employed in\nmost of models are non-text, lacking sufficient emphasis on the importance of\nhuman-in-the-loop. In this paper, an effective textual prompt guided image\nrestoration model has been proposed. In this model, task-specific BERT is\nfine-tuned to accurately understand user's instructions and generating textual\nprompt guidance. Depth-wise multi-head transposed attentions and gated\nconvolution modules are designed to bridge the gap between textual prompts and\nvisual features. The proposed model has innovatively introduced semantic\nprompts into low-level visual domain. It highlights the potential to provide a\nnatural, precise, and controllable way to perform image restoration tasks.\nExtensive experiments have been done on public denoising, dehazing and\nderaining datasets. The experiment results demonstrate that, compared with\npopular state-of-the-art methods, the proposed model can obtain much more\nsuperior performance, achieving accurate recognition and removal of degradation\nwithout increasing model's complexity. Related source codes and data will be\npublicly available on github site\nhttps://github.com/MoTong-AI-studio/TextPromptIR.",
        "date": "2023-12-11T06:56:41+00:00",
        "link": "http://arxiv.org/pdf/2312.06162v1"
    },
    {
        "title": "Adaptive Feature Selection for No-Reference Image Quality Assessment using Contrastive Mitigating Semantic Noise Sensitivity",
        "authors": [
            "Xudong Li",
            "Timin Gao",
            "Xiawu Zheng",
            "Runze Hu",
            "Jingyuan Zheng",
            "Yunhang Shen",
            "Ke Li",
            "Yutao Liu",
            "Pingyang Dai",
            "Yan Zhang",
            "Rongrong Ji"
        ],
        "abstract": "The current state-of-the-art No-Reference Image Quality Assessment (NR-IQA)\nmethods typically use feature extraction in upstream backbone networks, which\nassumes that all extracted features are relevant. However, we argue that not\nall features are beneficial, and some may even be harmful, necessitating\ncareful selection. Empirically, we find that many image pairs with small\nfeature spatial distances can have vastly different quality scores. To address\nthis issue, we propose a Quality-Aware Feature Matching IQA metric(QFM-IQM)\nthat employs contrastive learning to remove harmful features from the upstream\ntask. Specifically, our approach enhances the semantic noise distinguish\ncapabilities of neural networks by comparing image pairs with similar semantic\nfeatures but varying quality scores and adaptively adjusting the upstream\ntask's features by introducing disturbance. Furthermore, we utilize a\ndistillation framework to expand the dataset and improve the model's\ngeneralization ability. Our approach achieves superior performance to the\nstate-of-the-art NR-IQA methods on 8 standard NR-IQA datasets, achieving PLCC\nvalues of 0.932 (vs. 0.908 in TID2013) and 0.913 (vs. 0.894 in LIVEC).",
        "date": "2023-12-11T06:50:27+00:00",
        "link": "http://arxiv.org/pdf/2312.06158v1"
    },
    {
        "title": "BACTrack: Building Appearance Collection for Aerial Tracking",
        "authors": [
            "Xincong Liu",
            "Tingfa Xu",
            "Ying Wang",
            "Zhinong Yu",
            "Xiaoying Yuan",
            "Haolin Qin",
            "Jianan Li"
        ],
        "abstract": "Siamese network-based trackers have shown remarkable success in aerial\ntracking. Most previous works, however, usually perform template matching only\nbetween the initial template and the search region and thus fail to deal with\nrapidly changing targets that often appear in aerial tracking. As a remedy,\nthis work presents Building Appearance Collection Tracking (BACTrack). This\nsimple yet effective tracking framework builds a dynamic collection of target\ntemplates online and performs efficient multi-template matching to achieve\nrobust tracking. Specifically, BACTrack mainly comprises a Mixed-Temporal\nTransformer (MTT) and an appearance discriminator. The former is responsible\nfor efficiently building relationships between the search region and multiple\ntarget templates in parallel through a mixed-temporal attention mechanism. At\nthe same time, the appearance discriminator employs an online adaptive\ntemplate-update strategy to ensure that the collected multiple templates remain\nreliable and diverse, allowing them to closely follow rapid changes in the\ntarget's appearance and suppress background interference during tracking.\nExtensive experiments show that our BACTrack achieves top performance on four\nchallenging aerial tracking benchmarks while maintaining an impressive speed of\nover 87 FPS on a single GPU. Speed tests on embedded platforms also validate\nour potential suitability for deployment on UAV platforms.",
        "date": "2023-12-11T05:55:59+00:00",
        "link": "http://arxiv.org/pdf/2312.06136v1"
    },
    {
        "title": "ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and Implicit Style Prompt Bank",
        "authors": [
            "Zhanjie Zhang",
            "Quanwei Zhang",
            "Guangyuan Li",
            "Wei Xing",
            "Lei Zhao",
            "Jiakai Sun",
            "Zehua Lan",
            "Junsheng Luan",
            "Yiling Huang",
            "Huaizhong Lin"
        ],
        "abstract": "Artistic style transfer aims to repaint the content image with the learned\nartistic style. Existing artistic style transfer methods can be divided into\ntwo categories: small model-based approaches and pre-trained large-scale\nmodel-based approaches. Small model-based approaches can preserve the content\nstrucuture, but fail to produce highly realistic stylized images and introduce\nartifacts and disharmonious patterns; Pre-trained large-scale model-based\napproaches can generate highly realistic stylized images but struggle with\npreserving the content structure. To address the above issues, we propose\nArtBank, a novel artistic style transfer framework, to generate highly\nrealistic stylized images while preserving the content structure of the content\nimages. Specifically, to sufficiently dig out the knowledge embedded in\npre-trained large-scale models, an Implicit Style Prompt Bank (ISPB), a set of\ntrainable parameter matrices, is designed to learn and store knowledge from the\ncollection of artworks and behave as a visual prompt to guide pre-trained\nlarge-scale models to generate highly realistic stylized images while\npreserving content structure. Besides, to accelerate training the above ISPB,\nwe propose a novel Spatial-Statistical-based self-Attention Module (SSAM). The\nqualitative and quantitative experiments demonstrate the superiority of our\nproposed method over state-of-the-art artistic style transfer methods.",
        "date": "2023-12-11T05:53:40+00:00",
        "link": "http://arxiv.org/pdf/2312.06135v1"
    },
    {
        "title": "M3SOT: Multi-frame, Multi-field, Multi-space 3D Single Object Tracking",
        "authors": [
            "Jiaming Liu",
            "Yue Wu",
            "Maoguo Gong",
            "Qiguang Miao",
            "Wenping Ma",
            "Can Qin"
        ],
        "abstract": "3D Single Object Tracking (SOT) stands a forefront task of computer vision,\nproving essential for applications like autonomous driving. Sparse and occluded\ndata in scene point clouds introduce variations in the appearance of tracked\nobjects, adding complexity to the task. In this research, we unveil M3SOT, a\nnovel 3D SOT framework, which synergizes multiple input frames (template sets),\nmultiple receptive fields (continuous contexts), and multiple solution spaces\n(distinct tasks) in ONE model. Remarkably, M3SOT pioneers in modeling\ntemporality, contexts, and tasks directly from point clouds, revisiting a\nperspective on the key factors influencing SOT. To this end, we design a\ntransformer-based network centered on point cloud targets in the search area,\naggregating diverse contextual representations and propagating target cues by\nemploying historical frames. As M3SOT spans varied processing perspectives,\nwe've streamlined the network-trimming its depth and optimizing its\nstructure-to ensure a lightweight and efficient deployment for SOT\napplications. We posit that, backed by practical construction, M3SOT sidesteps\nthe need for complex frameworks and auxiliary components to deliver sterling\nresults. Extensive experiments on benchmarks such as KITTI, nuScenes, and Waymo\nOpen Dataset demonstrate that M3SOT achieves state-of-the-art performance at 38\nFPS. Our code and models are available at\nhttps://github.com/ywu0912/TeamCode.git.",
        "date": "2023-12-11T04:49:47+00:00",
        "link": "http://arxiv.org/pdf/2312.06117v1"
    },
    {
        "title": "Stellar: Systematic Evaluation of Human-Centric Personalized Text-to-Image Methods",
        "authors": [
            "Panos Achlioptas",
            "Alexandros Benetatos",
            "Iordanis Fostiropoulos",
            "Dimitris Skourtis"
        ],
        "abstract": "In this work, we systematically study the problem of personalized\ntext-to-image generation, where the output image is expected to portray\ninformation about specific human subjects. E.g., generating images of oneself\nappearing at imaginative places, interacting with various items, or engaging in\nfictional activities. To this end, we focus on text-to-image systems that input\na single image of an individual to ground the generation process along with\ntext describing the desired visual context. Our first contribution is to fill\nthe literature gap by curating high-quality, appropriate data for this task.\nNamely, we introduce a standardized dataset (Stellar) that contains\npersonalized prompts coupled with images of individuals that is an order of\nmagnitude larger than existing relevant datasets and where rich semantic\nground-truth annotations are readily available. Having established Stellar to\npromote cross-systems fine-grained comparisons further, we introduce a rigorous\nensemble of specialized metrics that highlight and disentangle fundamental\nproperties such systems should obey. Besides being intuitive, our new metrics\ncorrelate significantly more strongly with human judgment than currently used\nmetrics on this task. Last but not least, drawing inspiration from the recent\nworks of ELITE and SDXL, we derive a simple yet efficient, personalized\ntext-to-image baseline that does not require test-time fine-tuning for each\nsubject and which sets quantitatively and in human trials a new SoTA. For more\ninformation, please visit our project's website:\nhttps://stellar-gen-ai.github.io/.",
        "date": "2023-12-11T04:47:39+00:00",
        "link": "http://arxiv.org/pdf/2312.06116v1"
    },
    {
        "title": "SimMining-3D: Altitude-Aware 3D Object Detection in Complex Mining Environments: A Novel Dataset and ROS-Based Automatic Annotation Pipeline",
        "authors": [
            "Mehala Balamurali",
            "Ehsan Mihankhah"
        ],
        "abstract": "Accurate and efficient object detection is crucial for safe and efficient\noperation of earth-moving equipment in mining. Traditional 2D image-based\nmethods face limitations in dynamic and complex mine environments. To overcome\nthese challenges, 3D object detection using point cloud data has emerged as a\ncomprehensive approach. However, training models for mining scenarios is\nchallenging due to sensor height variations, viewpoint changes, and the need\nfor diverse annotated datasets. This paper presents novel contributions to\naddress these challenges. We introduce a synthetic dataset SimMining 3D [1]\nspecifically designed for 3D object detection in mining environments. The\ndataset captures objects and sensors positioned at various heights within mine\nbenches, accurately reflecting authentic mining scenarios. An automatic\nannotation pipeline through ROS interface reduces manual labor and accelerates\ndataset creation. We propose evaluation metrics accounting for sensor-to-object\nheight variations and point cloud density, enabling accurate model assessment\nin mining scenarios. Real data tests validate our models effectiveness in\nobject prediction. Our ablation study emphasizes the importance of altitude and\nheight variation augmentations in improving accuracy and reliability. The\npublicly accessible synthetic dataset [1] serves as a benchmark for supervised\nlearning and advances object detection techniques in mining with complimentary\npointwise annotations for each scene. In conclusion, our work bridges the gap\nbetween synthetic and real data, addressing the domain shift challenge in 3D\nobject detection for mining. We envision robust object detection systems\nenhancing safety and efficiency in mining and related domains.",
        "date": "2023-12-11T04:33:45+00:00",
        "link": "http://arxiv.org/pdf/2312.06113v1"
    },
    {
        "title": "Converting and Smoothing False Negatives for Vision-Language Pre-training",
        "authors": [
            "Jaeseok Byun",
            "Dohoon Kim",
            "Taesup Moon"
        ],
        "abstract": "We consider the critical issue of false negatives in Vision-Language\nPre-training (VLP), a challenge that arises from the inherent many-to-many\ncorrespondence of image-text pairs in large-scale web-crawled datasets. The\npresence of false negatives can impede achieving optimal performance and even\nlead to learning failures. To address this challenge, we propose a method\ncalled COSMO (COnverting and SMOoothing false negatives) that manages the false\nnegative issues, especially powerful in hard negative sampling. Building upon\nthe recently developed GRouped mIni-baTch sampling (GRIT) strategy, our\napproach consists of two pivotal components: 1) an efficient connection mining\nprocess that identifies and converts false negatives into positives, and 2)\nlabel smoothing for the image-text contrastive loss (ITC). Our comprehensive\nexperiments verify the effectiveness of COSMO across multiple downstream tasks,\nemphasizing the crucial role of addressing false negatives in VLP, potentially\neven surpassing the importance of addressing false positives. In addition, the\ncompatibility of COSMO with the recent BLIP-family model is also demonstrated.",
        "date": "2023-12-11T04:33:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06112v1"
    },
    {
        "title": "Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models",
        "authors": [
            "Haoran Wei",
            "Lingyu Kong",
            "Jinyue Chen",
            "Liang Zhao",
            "Zheng Ge",
            "Jinrong Yang",
            "Jianjian Sun",
            "Chunrui Han",
            "Xiangyu Zhang"
        ],
        "abstract": "Modern Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary\n-- CLIP, which can cover most common vision tasks. However, for some special\nvision task that needs dense and fine-grained vision perception, e.g.,\ndocument-level OCR or chart understanding, especially in non-English scenarios,\nthe CLIP-style vocabulary may encounter low efficiency in tokenizing the vision\nknowledge and even suffer out-of-vocabulary problem. Accordingly, we propose\nVary, an efficient and effective method to scale up the vision vocabulary of\nLVLMs. The procedures of Vary are naturally divided into two folds: the\ngeneration and integration of a new vision vocabulary. In the first phase, we\ndevise a vocabulary network along with a tiny decoder-only transformer to\nproduce the desired vocabulary via autoregression. In the next, we scale up the\nvanilla vision vocabulary by merging the new one with the original one (CLIP),\nenabling the LVLMs can quickly garner new features. Compared to the popular\nBLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while\nenjoying more excellent fine-grained perception and understanding ability.\nSpecifically, Vary is competent in new document parsing features (OCR or\nmarkdown conversion) while achieving 78.2% ANLS in DocVQA and 36.2% in MMVet.\nOur code will be publicly available on the homepage.",
        "date": "2023-12-11T04:26:17+00:00",
        "link": "http://arxiv.org/pdf/2312.06109v1"
    },
    {
        "title": "AUGCAL: Improving Sim2Rreal Adaptation by Uncertainty Calibration on Augmented Synthetic Images",
        "authors": [
            "Prithvijit Chattopadhyay",
            "Bharat Goyal",
            "Boglarka Ecsedi",
            "Viraj Prabhu",
            "Judy Hoffman"
        ],
        "abstract": "Synthetic data (SIM) drawn from simulators have emerged as a popular\nalternative for training models where acquiring annotated real-world images is\ndifficult. However, transferring models trained on synthetic images to\nreal-world applications can be challenging due to appearance disparities. A\ncommonly employed solution to counter this SIM2REAL gap is unsupervised domain\nadaptation, where models are trained using labeled SIM data and unlabeled REAL\ndata. Mispredictions made by such SIM2REAL adapted models are often associated\nwith miscalibration - stemming from overconfident predictions on real data. In\nthis paper, we introduce AUGCAL, a simple training-time patch for unsupervised\nadaptation that improves SIM2REAL adapted models by - (1) reducing overall\nmiscalibration, (2) reducing overconfidence in incorrect predictions and (3)\nimproving confidence score reliability by better guiding misclassification\ndetection - all while retaining or improving SIM2REAL performance. Given a base\nSIM2REAL adaptation algorithm, at training time, AUGCAL involves replacing\nvanilla SIM images with strongly augmented views (AUG intervention) and\nadditionally optimizing for a training time calibration loss on augmented SIM\npredictions (CAL intervention). We motivate AUGCAL using a brief analytical\njustification of how to reduce miscalibration on unlabeled REAL data. Through\nour experiments, we empirically show the efficacy of AUGCAL across multiple\nadaptation methods, backbones, tasks and shifts.",
        "date": "2023-12-11T04:24:11+00:00",
        "link": "http://arxiv.org/pdf/2312.06106v1"
    },
    {
        "title": "Hundred-Kilobyte Lookup Tables for Efficient Single-Image Super-Resolution",
        "authors": [
            "Binxiao Huang",
            "Jason Chun Lok Li",
            "Jie Ran",
            "Boyu Li",
            "Jiajun Zhou",
            "Dahai Yu",
            "Ngai Wong"
        ],
        "abstract": "Conventional super-resolution (SR) schemes make heavy use of convolutional\nneural networks (CNNs), which involve intensive multiply-accumulate (MAC)\noperations, and require specialized hardware such as graphics processing units.\nThis contradicts the regime of edge AI that often runs on devices strained by\npower, computing, and storage resources. Such a challenge has motivated a\nseries of lookup table (LUT)-based SR schemes that employ simple LUT readout\nand largely elude CNN computation. Nonetheless, the multi-megabyte LUTs in\nexisting methods still prohibit on-chip storage and necessitate off-chip memory\ntransport. This work tackles this storage hurdle and innovates hundred-kilobyte\nLUT (HKLUT) models amenable to on-chip cache. Utilizing an asymmetric\ntwo-branch multistage network coupled with a suite of specialized kernel\npatterns, HKLUT demonstrates an uncompromising performance and superior\nhardware efficiency over existing LUT schemes.",
        "date": "2023-12-11T04:07:34+00:00",
        "link": "http://arxiv.org/pdf/2312.06101v1"
    },
    {
        "title": "MATK: The Meme Analytical Tool Kit",
        "authors": [
            "Ming Shan Hee",
            "Aditi Kumaresan",
            "Nguyen Khoi Hoang",
            "Nirmalendu Prakash",
            "Rui Cao",
            "Roy Ka-Wei Lee"
        ],
        "abstract": "The rise of social media platforms has brought about a new digital culture\ncalled memes. Memes, which combine visuals and text, can strongly influence\npublic opinions on social and cultural issues. As a result, people have become\ninterested in categorizing memes, leading to the development of various\ndatasets and multimodal models that show promising results in this field.\nHowever, there is currently a lack of a single library that allows for the\nreproduction, evaluation, and comparison of these models using fair benchmarks\nand settings. To fill this gap, we introduce the Meme Analytical Tool Kit\n(MATK), an open-source toolkit specifically designed to support existing memes\ndatasets and cutting-edge multimodal models. MATK aims to assist researchers\nand engineers in training and reproducing these multimodal models for meme\nclassification tasks, while also providing analysis techniques to gain insights\ninto their strengths and weaknesses. To access MATK, please visit\n\\url{https://github.com/Social-AI-Studio/MATK}.",
        "date": "2023-12-11T03:36:59+00:00",
        "link": "http://arxiv.org/pdf/2312.06094v1"
    },
    {
        "title": "PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using Large Language Models",
        "authors": [
            "Nirmalendu Prakash",
            "Han Wang",
            "Nguyen Khoi Hoang",
            "Ming Shan Hee",
            "Roy Ka-Wei Lee"
        ],
        "abstract": "The proliferation of social media has given rise to a new form of\ncommunication: memes. Memes are multimodal and often contain a combination of\ntext and visual elements that convey meaning, humor, and cultural significance.\nWhile meme analysis has been an active area of research, little work has been\ndone on unsupervised multimodal topic modeling of memes, which is important for\ncontent moderation, social media analysis, and cultural studies. We propose\n\\textsf{PromptMTopic}, a novel multimodal prompt-based model designed to learn\ntopics from both text and visual modalities by leveraging the language modeling\ncapabilities of large language models. Our model effectively extracts and\nclusters topics learned from memes, considering the semantic interaction\nbetween the text and visual modalities. We evaluate our proposed model through\nextensive experiments on three real-world meme datasets, which demonstrate its\nsuperiority over state-of-the-art topic modeling baselines in learning\ndescriptive topics in memes. Additionally, our qualitative analysis shows that\n\\textsf{PromptMTopic} can identify meaningful and culturally relevant topics\nfrom memes. Our work contributes to the understanding of the topics and themes\nof memes, a crucial form of communication in today's society.\\\\\n\\red{\\textbf{Disclaimer: This paper contains sensitive content that may be\ndisturbing to some readers.}}",
        "date": "2023-12-11T03:36:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06093v1"
    },
    {
        "title": "Robust Geometry and Reflectance Disentanglement for 3D Face Reconstruction from Sparse-view Images",
        "authors": [
            "Daisheng Jin",
            "Jiangbei Hu",
            "Baixin Xu",
            "Yuxin Dai",
            "Chen Qian",
            "Ying He"
        ],
        "abstract": "This paper presents a novel two-stage approach for reconstructing human faces\nfrom sparse-view images, a task made challenging by the unique geometry and\ncomplex skin reflectance of each individual. Our method focuses on decomposing\nkey facial attributes, including geometry, diffuse reflectance, and specular\nreflectance, from ambient light. Initially, we create a general facial template\nfrom a diverse collection of individual faces, capturing essential geometric\nand reflectance characteristics. Guided by this template, we refine each\nspecific face model in the second stage, which further considers the\ninteraction between geometry and reflectance, as well as the subsurface\nscattering effects on facial skin. Our method enables the reconstruction of\nhigh-quality facial representations from as few as three images, offering\nimproved geometric accuracy and reflectance detail. Through comprehensive\nevaluations and comparisons, our method demonstrates superiority over existing\ntechniques. Our method effectively disentangles geometry and reflectance\ncomponents, leading to enhanced quality in synthesizing new views and opening\nup possibilities for applications such as relighting and reflectance editing.\nWe will make the code publicly available.",
        "date": "2023-12-11T03:14:58+00:00",
        "link": "http://arxiv.org/pdf/2312.06085v1"
    },
    {
        "title": "Oracle Character Recognition using Unsupervised Discriminative Consistency Network",
        "authors": [
            "Mei Wang",
            "Weihong Deng",
            "Sen Su"
        ],
        "abstract": "Ancient history relies on the study of ancient characters. However,\nreal-world scanned oracle characters are difficult to collect and annotate,\nposing a major obstacle for oracle character recognition (OrCR). Besides,\nserious abrasion and inter-class similarity also make OrCR more challenging. In\nthis paper, we propose a novel unsupervised domain adaptation method for OrCR,\nwhich enables to transfer knowledge from labeled handprinted oracle characters\nto unlabeled scanned data. We leverage pseudo-labeling to incorporate the\nsemantic information into adaptation and constrain augmentation consistency to\nmake the predictions of scanned samples consistent under different\nperturbations, leading to the model robustness to abrasion, stain and\ndistortion. Simultaneously, an unsupervised transition loss is proposed to\nlearn more discriminative features on the scanned domain by optimizing both\nbetween-class and within-class transition probability. Extensive experiments\nshow that our approach achieves state-of-the-art result on Oracle-241 dataset\nand substantially outperforms the recently proposed structure-texture\nseparation network by 15.1%.",
        "date": "2023-12-11T02:52:27+00:00",
        "link": "http://arxiv.org/pdf/2312.06075v1"
    },
    {
        "title": "A dynamic interactive learning framework for automated 3D medical image segmentation",
        "authors": [
            "Mu Tian",
            "Xiaohui Chen",
            "Yi Gao"
        ],
        "abstract": "Many deep learning based automated medical image segmentation systems, in\nreality, face difficulties in deployment due to the cost of massive data\nannotation and high latency in model iteration. We propose a dynamic\ninteractive learning framework that addresses these challenges by integrating\ninteractive segmentation into end-to-end weak supervised learning with\nstreaming tasks. We develop novel replay and label smoothing schemes that\novercome catastrophic forgetting and improve online learning robustness. For\neach image, our multi-round interactive segmentation module simultaneously\noptimizes both front-end predictions and deep learning segmenter. In each\nround, a 3D \"proxy mask\" is propagated from sparse user inputs based on image\nregistration, serving as weak supervision that enable knowledge distillation\nfrom the unknown ground truth. In return, the trained segmenter explicitly\nguides next step's user interventions according to a spatial residual map from\nconsecutive front or back-end predictions. Evaluation on 3D segmentation tasks\n(NCI-ISBI2013 and BraTS2015) shows that our framework generates online learning\nperformances that match offline training benchmark. In addition, with a 62%\nreduction in total annotation efforts, our framework produces competitive dice\nscores comparing to online and offline learning which equipped with full ground\ntruth. Furthermore, such a framework, with its flexibility and responsiveness,\ncould be deployed behind hospital firewall that guarantees data security and\neasy maintenance.",
        "date": "2023-12-11T02:39:08+00:00",
        "link": "http://arxiv.org/pdf/2312.06072v1"
    },
    {
        "title": "Probabilistic Precipitation Downscaling with Optical Flow-Guided Diffusion",
        "authors": [
            "Prakhar Srivastava",
            "Ruihan Yang",
            "Gavin Kerrigan",
            "Gideon Dresdner",
            "Jeremy McGibbon",
            "Christopher Bretherton",
            "Stephan Mandt"
        ],
        "abstract": "In climate science and meteorology, local precipitation predictions are\nlimited by the immense computational costs induced by the high spatial\nresolution that simulation methods require. A common workaround is statistical\ndownscaling (aka superresolution), where a low-resolution prediction is\nsuper-resolved using statistical approaches. While traditional computer vision\ntasks mainly focus on human perception or mean squared error, applications in\nweather and climate require capturing the conditional distribution of\nhigh-resolution patterns given low-resolution patterns so that reliable\nensemble averages can be taken. Our approach relies on extending recent video\ndiffusion models to precipitation superresolution: an optical flow on the\nhigh-resolution output induces temporally coherent predictions, whereas a\ntemporally-conditioned diffusion model generates residuals that capture the\ncorrect noise characteristics and high-frequency patterns. We test our approach\non X-SHiELD, an established large-scale climate simulation dataset, and compare\nagainst two state-of-the-art baselines, focusing on CRPS, MSE, precipitation\ndistributions, as well as an illustrative case -- the complex terrain of\nCalifornia. Our approach sets a new standard for data-driven precipitation\ndownscaling.",
        "date": "2023-12-11T02:38:07+00:00",
        "link": "http://arxiv.org/pdf/2312.06071v1"
    },
    {
        "title": "Mining Gaze for Contrastive Learning toward Computer-Assisted Diagnosis",
        "authors": [
            "Zihao Zhao",
            "Sheng Wang",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "abstract": "Obtaining large-scale radiology reports can be difficult for medical images\ndue to various reasons, limiting the effectiveness of contrastive pre-training\nin the medical image domain and underscoring the need for alternative methods.\nIn this paper, we propose eye-tracking as an alternative to text reports, as it\nallows for the passive collection of gaze signals without disturbing\nradiologist's routine diagnosis process. By tracking the gaze of radiologists\nas they read and diagnose medical images, we can understand their visual\nattention and clinical reasoning. When a radiologist has similar gazes for two\nmedical images, it may indicate semantic similarity for diagnosis, and these\nimages should be treated as positive pairs when pre-training a\ncomputer-assisted diagnosis (CAD) network through contrastive learning.\nAccordingly, we introduce the Medical contrastive Gaze Image Pre-training\n(McGIP) as a plug-and-play module for contrastive learning frameworks. McGIP\nuses radiologist's gaze to guide contrastive pre-training. We evaluate our\nmethod using two representative types of medical images and two common types of\ngaze data. The experimental results demonstrate the practicality of McGIP,\nindicating its high potential for various clinical scenarios and applications.",
        "date": "2023-12-11T02:27:45+00:00",
        "link": "http://arxiv.org/pdf/2312.06069v1"
    },
    {
        "title": "Contrastive Multi-view Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks",
        "authors": [
            "Renxiang Guan",
            "Zihao Li",
            "Xianju Li",
            "Chang Tang",
            "Ruyi Feng"
        ],
        "abstract": "High-dimensional and complex spectral structures make the clustering of\nhyperspectral images (HSI) a challenging task. Subspace clustering is an\neffective approach for addressing this problem. However, current subspace\nclustering algorithms are primarily designed for a single view and do not fully\nexploit the spatial or textural feature information in HSI. In this study,\ncontrastive multi-view subspace clustering of HSI was proposed based on graph\nconvolutional networks. Pixel neighbor textural and spatial-spectral\ninformation were sent to construct two graph convolutional subspaces to learn\ntheir affinity matrices. To maximize the interaction between different views, a\ncontrastive learning algorithm was introduced to promote the consistency of\npositive samples and assist the model in extracting robust features. An\nattention-based fusion module was used to adaptively integrate these affinity\nmatrices, constructing a more discriminative affinity matrix. The model was\nevaluated using four popular HSI datasets: Indian Pines, Pavia University,\nHouston, and Xu Zhou. It achieved overall accuracies of 97.61%, 96.69%, 87.21%,\nand 97.65%, respectively, and significantly outperformed state-of-the-art\nclustering methods. In conclusion, the proposed model effectively improves the\nclustering accuracy of HSI.",
        "date": "2023-12-11T02:22:10+00:00",
        "link": "http://arxiv.org/pdf/2312.06068v1"
    },
    {
        "title": "PCRDiffusion: Diffusion Probabilistic Models for Point Cloud Registration",
        "authors": [
            "Yue Wu",
            "Yongzhe Yuan",
            "Xiaolong Fan",
            "Xiaoshui Huang",
            "Maoguo Gong",
            "Qiguang Miao"
        ],
        "abstract": "We propose a new framework that formulates point cloud registration as a\ndenoising diffusion process from noisy transformation to object transformation.\nDuring training stage, object transformation diffuses from ground-truth\ntransformation to random distribution, and the model learns to reverse this\nnoising process. In sampling stage, the model refines randomly generated\ntransformation to the output result in a progressive way. We derive the\nvariational bound in closed form for training and provide implementations of\nthe model. Our work provides the following crucial findings: (i) In contrast to\nmost existing methods, our framework, Diffusion Probabilistic Models for Point\nCloud Registration (PCRDiffusion) does not require repeatedly update source\npoint cloud to refine the predicted transformation. (ii) Point cloud\nregistration, one of the representative discriminative tasks, can be solved by\na generative way and the unified probabilistic formulation. Finally, we discuss\nand provide an outlook on the application of diffusion model in different\nscenarios for point cloud registration. Experimental results demonstrate that\nour model achieves competitive performance in point cloud registration. In\ncorrespondence-free and correspondence-based scenarios, PCRDifussion can both\nachieve exceeding 50\\% performance improvements.",
        "date": "2023-12-11T01:56:42+00:00",
        "link": "http://arxiv.org/pdf/2312.06063v1"
    },
    {
        "title": "CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models",
        "authors": [
            "Tuna Han Salih Meral",
            "Enis Simsar",
            "Federico Tombari",
            "Pinar Yanardag"
        ],
        "abstract": "Images produced by text-to-image diffusion models might not always faithfully\nrepresent the semantic intent of the provided text prompt, where the model\nmight overlook or entirely fail to produce certain objects. Existing solutions\noften require customly tailored functions for each of these problems, leading\nto sub-optimal results, especially for complex prompts. Our work introduces a\nnovel perspective by tackling this challenge in a contrastive context. Our\napproach intuitively promotes the segregation of objects in attention maps\nwhile also maintaining that pairs of related attributes are kept close to each\nother. We conduct extensive experiments across a wide variety of scenarios,\neach involving unique combinations of objects, attributes, and scenes. These\nexperiments effectively showcase the versatility, efficiency, and flexibility\nof our method in working with both latent and pixel-based diffusion models,\nincluding Stable Diffusion and Imagen. Moreover, we publicly share our source\ncode to facilitate further research.",
        "date": "2023-12-11T01:42:15+00:00",
        "link": "http://arxiv.org/pdf/2312.06059v1"
    },
    {
        "title": "MaskConver: Revisiting Pure Convolution Model for Panoptic Segmentation",
        "authors": [
            "Abdullah Rashwan",
            "Jiageng Zhang",
            "Ali Taalimi",
            "Fan Yang",
            "Xingyi Zhou",
            "Chaochao Yan",
            "Liang-Chieh Chen",
            "Yeqing Li"
        ],
        "abstract": "In recent years, transformer-based models have dominated panoptic\nsegmentation, thanks to their strong modeling capabilities and their unified\nrepresentation for both semantic and instance classes as global binary masks.\nIn this paper, we revisit pure convolution model and propose a novel panoptic\narchitecture named MaskConver. MaskConver proposes to fully unify things and\nstuff representation by predicting their centers. To that extent, it creates a\nlightweight class embedding module that can break the ties when multiple\ncenters co-exist in the same location. Furthermore, our study shows that the\ndecoder design is critical in ensuring that the model has sufficient context\nfor accurate detection and segmentation. We introduce a powerful ConvNeXt-UNet\ndecoder that closes the performance gap between convolution- and\ntransformerbased models. With ResNet50 backbone, our MaskConver achieves 53.6%\nPQ on the COCO panoptic val set, outperforming the modern convolution-based\nmodel, Panoptic FCN, by 9.3% as well as transformer-based models such as\nMask2Former (+1.7% PQ) and kMaX-DeepLab (+0.6% PQ). Additionally, MaskConver\nwith a MobileNet backbone reaches 37.2% PQ, improving over Panoptic-DeepLab by\n+6.4% under the same FLOPs/latency constraints. A further optimized version of\nMaskConver achieves 29.7% PQ, while running in real-time on mobile devices. The\ncode and model weights will be publicly available",
        "date": "2023-12-11T00:52:26+00:00",
        "link": "http://arxiv.org/pdf/2312.06052v1"
    },
    {
        "title": "SSPNet: Scale and Spatial Priors Guided Generalizable and Interpretable Pedestrian Attribute Recognition",
        "authors": [
            "Jifeng Shen",
            "Teng Guo",
            "Xin Zuo",
            "Heng Fan",
            "Wankou Yang"
        ],
        "abstract": "Global feature based Pedestrian Attribute Recognition (PAR) models are often\npoorly localized when using Grad-CAM for attribute response analysis, which has\na significant impact on the interpretability, generalizability and performance.\nPrevious researches have attempted to improve generalization and interpretation\nthrough meticulous model design, yet they often have neglected or underutilized\neffective prior information crucial for PAR. To this end, a novel Scale and\nSpatial Priors Guided Network (SSPNet) is proposed for PAR, which is mainly\ncomposed of the Adaptive Feature Scale Selection (AFSS) and Prior Location\nExtraction (PLE) modules. The AFSS module learns to provide reasonable scale\nprior information for different attribute groups, allowing the model to focus\non different levels of feature maps with varying semantic granularity. The PLE\nmodule reveals potential attribute spatial prior information, which avoids\nunnecessary attention on irrelevant areas and lowers the risk of model\nover-fitting. More specifically, the scale prior in AFSS is adaptively learned\nfrom different layers of feature pyramid with maximum accuracy, while the\nspatial priors in PLE can be revealed from part feature with different\ngranularity (such as image blocks, human pose keypoint and sparse sampling\npoints). Besides, a novel IoU based attribute localization metric is proposed\nfor Weakly-supervised Pedestrian Attribute Localization (WPAL) based on the\nimproved Grad-CAM for attribute response mask. The experimental results on the\nintra-dataset and cross-dataset evaluations demonstrate the effectiveness of\nour proposed method in terms of mean accuracy (mA). Furthermore, it also\nachieves superior performance on the PCS dataset for attribute localization in\nterms of IoU. Code will be released at https://github.com/guotengg/SSPNet.",
        "date": "2023-12-11T00:41:40+00:00",
        "link": "http://arxiv.org/pdf/2312.06049v1"
    },
    {
        "title": "Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism",
        "authors": [
            "Georgios Milis",
            "Panagiotis P. Filntisis",
            "Anastasios Roussos",
            "Petros Maragos"
        ],
        "abstract": "Recent advances in deep learning for sequential data have given rise to fast\nand powerful models that produce realistic videos of talking humans. The state\nof the art in talking face generation focuses mainly on lip-syncing, being\nconditioned on audio clips. However, having the ability to synthesize talking\nhumans from text transcriptions rather than audio is particularly beneficial\nfor many applications and is expected to receive more and more attention,\nfollowing the recent breakthroughs in large language models. For that, most\nmethods implement a cascaded 2-stage architecture of a text-to-speech module\nfollowed by an audio-driven talking face generator, but this ignores the highly\ncomplex interplay between audio and visual streams that occurs during speaking.\nIn this paper, we propose the first, to the best of our knowledge, text-driven\naudiovisual speech synthesizer that uses Transformers and does not follow a\ncascaded approach. Our method, which we call NEUral Text to ARticulate Talk\n(NEUTART), is a talking face generator that uses a joint audiovisual feature\nspace, as well as speech-informed 3D facial reconstructions and a lip-reading\nloss for visual supervision. The proposed model produces photorealistic talking\nface videos with human-like articulation and well-synced audiovisual streams.\nOur experiments on audiovisual datasets as well as in-the-wild videos reveal\nstate-of-the-art generation quality both in terms of objective metrics and\nhuman evaluation.",
        "date": "2023-12-11T18:41:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06613v1"
    },
    {
        "title": "Deep Photonic Reservoir Computer for Speech Recognition",
        "authors": [
            "Enrico Picco",
            "Alessandro Lupo",
            "Serge Massar"
        ],
        "abstract": "Speech recognition is a critical task in the field of artificial intelligence\nand has witnessed remarkable advancements thanks to large and complex neural\nnetworks, whose training process typically requires massive amounts of labeled\ndata and computationally intensive operations. An alternative paradigm,\nreservoir computing, is energy efficient and is well adapted to implementation\nin physical substrates, but exhibits limitations in performance when compared\nto more resource-intensive machine learning algorithms. In this work we address\nthis challenge by investigating different architectures of interconnected\nreservoirs, all falling under the umbrella of deep reservoir computing. We\npropose a photonic-based deep reservoir computer and evaluate its effectiveness\non different speech recognition tasks. We show specific design choices that aim\nto simplify the practical implementation of a reservoir computer while\nsimultaneously achieving high-speed processing of high-dimensional audio\nsignals. Overall, with the present work we hope to help the advancement of\nlow-power and high-performance neuromorphic hardware.",
        "date": "2023-12-11T17:43:58+00:00",
        "link": "http://arxiv.org/pdf/2312.06558v1"
    },
    {
        "title": "Towards Domain-Specific Cross-Corpus Speech Emotion Recognition Approach",
        "authors": [
            "Yan Zhao",
            "Yuan Zong",
            "Hailun Lian",
            "Cheng Lu",
            "Jingang Shi",
            "Wenming Zheng"
        ],
        "abstract": "Cross-corpus speech emotion recognition (SER) poses a challenge due to\nfeature distribution mismatch, potentially degrading the performance of\nestablished SER methods. In this paper, we tackle this challenge by proposing a\nnovel transfer subspace learning method called acoustic knowledgeguided\ntransfer linear regression (AKTLR). Unlike existing approaches, which often\noverlook domain-specific knowledge related to SER and simply treat cross-corpus\nSER as a generic transfer learning task, our AKTLR method is built upon a\nwell-designed acoustic knowledge-guided dual sparsity constraint mechanism.\nThis mechanism emphasizes the potential of minimalistic acoustic parameter\nfeature sets to alleviate classifier overadaptation, which is empirically\nvalidated acoustic knowledge in SER, enabling superior generalization in\ncross-corpus SER tasks compared to using large feature sets. Through this\nmechanism, we extend a simple transfer linear regression model to AKTLR. This\nextension harnesses its full capability to seek emotiondiscriminative and\ncorpus-invariant features from established acoustic parameter feature sets used\nfor describing speech signals across two scales: contributive acoustic\nparameter groups and constituent elements within each contributive group. Our\nproposed method is evaluated through extensive cross-corpus SER experiments on\nthree widely-used speech emotion corpora: EmoDB, eNTERFACE, and CASIA. The\nresults confirm the effectiveness and superior performance of our method,\noutperforming recent state-of-the-art transfer subspace learning and deep\ntransfer learning-based cross-corpus SER methods. Furthermore, our work\nprovides experimental evidence supporting the feasibility and superiority of\nincorporating domain-specific knowledge into the transfer learning model to\naddress cross-corpus SER tasks.",
        "date": "2023-12-11T15:53:57+00:00",
        "link": "http://arxiv.org/pdf/2312.06466v1"
    },
    {
        "title": "Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation",
        "authors": [
            "Qi Yang",
            "Xing Nie",
            "Tong Li",
            "Pengfei Gao",
            "Ying Guo",
            "Cheng Zhen",
            "Pengfei Yan",
            "Shiming Xiang"
        ],
        "abstract": "Recently, an audio-visual segmentation (AVS) task has been introduced, aiming\nto group pixels with sounding objects within a given video. This task\nnecessitates a first-ever audio-driven pixel-level understanding of the scene,\nposing significant challenges. In this paper, we propose an innovative\naudio-visual transformer framework, termed COMBO, an acronym for COoperation of\nMulti-order Bilateral relatiOns. For the first time, our framework explores\nthree types of bilateral entanglements within AVS: pixel entanglement, modality\nentanglement, and temporal entanglement. Regarding pixel entanglement, we\nemploy a Siam-Encoder Module (SEM) that leverages prior knowledge to generate\nmore precise visual features from the foundational model. For modality\nentanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to\nalign corresponding visual and auditory signals bi-directionally. As for\ntemporal entanglement, we introduce an innovative adaptive inter-frame\nconsistency loss according to the inherent rules of temporal. Comprehensive\nexperiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou\non MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that\nCOMBO surpasses previous state-of-the-art methods. Code and more results will\nbe publicly available at https://combo-avs.github.io/.",
        "date": "2023-12-11T15:51:38+00:00",
        "link": "http://arxiv.org/pdf/2312.06462v1"
    },
    {
        "title": "Deep Imbalanced Learning for Multimodal Emotion Recognition in Conversations",
        "authors": [
            "Tao Meng",
            "Yuntao Shou",
            "Wei Ai",
            "Nan Yin",
            "Keqin Li"
        ],
        "abstract": "The main task of Multimodal Emotion Recognition in Conversations (MERC) is to\nidentify the emotions in modalities, e.g., text, audio, image and video, which\nis a significant development direction for realizing machine intelligence.\nHowever, many data in MERC naturally exhibit an imbalanced distribution of\nemotion categories, and researchers ignore the negative impact of imbalanced\ndata on emotion recognition. To tackle this problem, we systematically analyze\nit from three aspects: data augmentation, loss sensitivity, and sampling\nstrategy, and propose the Class Boundary Enhanced Representation Learning\n(CBERL) model. Concretely, we first design a multimodal generative adversarial\nnetwork to address the imbalanced distribution of {emotion} categories in raw\ndata. Secondly, a deep joint variational autoencoder is proposed to fuse\ncomplementary semantic information across modalities and obtain discriminative\nfeature representations. Finally, we implement a multi-task graph neural\nnetwork with mask reconstruction and classification optimization to solve the\nproblem of overfitting and underfitting in class boundary learning, and achieve\ncross-modal emotion recognition. We have conducted extensive experiments on the\nIEMOCAP and MELD benchmark datasets, and the results show that CBERL has\nachieved a certain performance improvement in the effectiveness of emotion\nrecognition. Especially on the minority class fear and disgust emotion labels,\nour model improves the accuracy and F1 value by 10% to 20%.",
        "date": "2023-12-11T12:35:17+00:00",
        "link": "http://arxiv.org/pdf/2312.06337v1"
    },
    {
        "title": "Testing Speech Emotion Recognition Machine Learning Models",
        "authors": [
            "Anna Derington",
            "Hagen Wierstorf",
            "Ali Özkil",
            "Florian Eyben",
            "Felix Burkhardt",
            "Björn W. Schuller"
        ],
        "abstract": "Machine learning models for speech emotion recognition (SER) can be trained\nfor different tasks and are usually evaluated on the basis of a few available\ndatasets per task. Tasks could include arousal, valence, dominance, emotional\ncategories, or tone of voice. Those models are mainly evaluated in terms of\ncorrelation or recall, and always show some errors in their predictions. The\nerrors manifest themselves in model behaviour, which can be very different\nalong different dimensions even if the same recall or correlation is achieved\nby the model. This paper investigates behavior of speech emotion recognition\nmodels with a testing framework which requires models to fulfill conditions in\nterms of correctness, fairness, and robustness.",
        "date": "2023-12-11T10:15:35+00:00",
        "link": "http://arxiv.org/pdf/2312.06270v1"
    },
    {
        "title": "Transformer Attractors for Robust and Efficient End-to-End Neural Diarization",
        "authors": [
            "Lahiru Samarakoon",
            "Samuel J. Broughton",
            "Marc Harkönen",
            "Ivan Fung"
        ],
        "abstract": "End-to-end neural diarization with encoder-decoder based attractors\n(EEND-EDA) is a method to perform diarization in a single neural network. EDA\nhandles the diarization of a flexible number of speakers by using an LSTM-based\nencoder-decoder that generates a set of speaker-wise attractors in an\nautoregressive manner. In this paper, we propose to replace EDA with a\ntransformer-based attractor calculation (TA) module. TA is composed of a\nCombiner block and a Transformer decoder. The main function of the combiner\nblock is to generate conversational dependent (CD) embeddings by incorporating\nlearned conversational information into a global set of embeddings. These CD\nembeddings will then serve as the input for the transformer decoder. Results on\npublic datasets show that EEND-TA achieves 2.68% absolute DER improvement over\nEEND-EDA. EEND-TA inference is 1.28 times faster than that of EEND-EDA.",
        "date": "2023-12-11T09:49:47+00:00",
        "link": "http://arxiv.org/pdf/2312.06253v1"
    },
    {
        "title": "Music-PAW: Learning Music Representations via Hierarchical Part-whole Interaction and Contrast",
        "authors": [
            "Dong Yao",
            "Shengyu Zhang",
            "Zhou Zhao",
            "Jieming Zhu",
            "Liqun Deng",
            "Wenqiao Zhang",
            "Zhenhua Dong",
            "Ruiming Tang",
            "Xin Jiang"
        ],
        "abstract": "The excellent performance of recent self-supervised learning methods on\nvarious downstream tasks has attracted great attention from academia and\nindustry. Some recent research efforts have been devoted to self-supervised\nmusic representation learning. Nevertheless, most of them learn to represent\nequally-sized music clips in the waveform or a spectrogram. Despite being\neffective in some tasks, learning music representations in such a manner\nlargely neglect the inherent part-whole hierarchies of music. Due to the\nhierarchical nature of the auditory cortex [24], understanding the bottom-up\nstructure of music, i.e., how different parts constitute the whole at different\nlevels, is essential for music understanding and representation learning. This\nwork pursues hierarchical music representation learning and introduces the\nMusic-PAW framework, which enables feature interactions of cropped music clips\nwith part-whole hierarchies. From a technical perspective, we propose a\ntransformer-based part-whole interaction module to progressively reason the\nstructural relationships between part-whole music clips at adjacent levels.\nBesides, to create a multi-hierarchy representation space, we devise a\nhierarchical contrastive learning objective to align part-whole music\nrepresentations in adjacent hierarchies. The merits of audio representation\nlearning from part-whole hierarchies have been validated on various downstream\ntasks, including music classification (single-label and multi-label), cover\nsong identification and acoustic scene classification.",
        "date": "2023-12-11T08:17:58+00:00",
        "link": "http://arxiv.org/pdf/2312.06197v1"
    },
    {
        "title": "ROSE: A Recognition-Oriented Speech Enhancement Framework in Air Traffic Control Using Multi-Objective Learning",
        "authors": [
            "Xincheng Yu",
            "Dongyue Guo",
            "Jianwei Zhang",
            "Yi Lin"
        ],
        "abstract": "Radio speech echo is a specific phenomenon in the air traffic control (ATC)\ndomain, which degrades speech quality and further impacts automatic speech\nrecognition (ASR) accuracy. In this work, a recognition-oriented speech\nenhancement (ROSE) framework is proposed to improve speech intelligibility and\nalso advance ASR accuracy, which serves as a plug-and-play tool in ATC\nscenarios and does not require additional retraining of the ASR model.\nSpecifically, an encoder-decoder-based U-Net framework is proposed to eliminate\nthe radio speech echo based on the real-world collected corpus. By\nincorporating the SE-oriented and ASR-oriented loss, ROSE is implemented in a\nmulti-objective manner by learning shared representations across the two\noptimization objectives. An attention-based skip-fusion (ABSF) mechanism is\napplied to skip connections to refine the features. A channel and sequence\nattention (CSAtt) block is innovatively designed to guide the model to focus on\ninformative representations and suppress disturbing features. The experimental\nresults show that the ROSE significantly outperforms other state-of-the-art\nmethods for both the SE and ASR tasks. In addition, the proposed approach can\ncontribute to the desired performance improvements on public datasets.",
        "date": "2023-12-11T04:51:41+00:00",
        "link": "http://arxiv.org/pdf/2312.06118v1"
    },
    {
        "title": "EEND-DEMUX: End-to-End Neural Speaker Diarization via Demultiplexed Speaker Embeddings",
        "authors": [
            "Sung Hwan Mun",
            "Min Hyun Han",
            "Canyeong Moon",
            "Nam Soo Kim"
        ],
        "abstract": "In recent years, there have been studies to further improve the end-to-end\nneural speaker diarization (EEND) systems. This letter proposes the EEND-DEMUX\nmodel, a novel framework utilizing demultiplexed speaker embeddings. In this\nwork, we focus on disentangling speaker-relevant information in the latent\nspace and then transform each separated latent variable into its corresponding\nspeech activity. EEND-DEMUX can directly obtain separated speaker embeddings\nthrough the demultiplexing operation in the inference phase without an external\nspeaker diarization system, an embedding extractor, or a heuristic decoding\ntechnique. Furthermore, we employ a multi-head cross-attention mechanism to\ncapture the correlation between mixture and separated speaker embeddings\neffectively. We formulate three loss functions based on matching,\northogonality, and sparsity constraints to learn robust demultiplexed speaker\nembeddings. The experimental results on the LibriMix dataset show consistently\nimproved performance in both a fixed and flexible number of speakers scenarios.",
        "date": "2023-12-11T02:14:55+00:00",
        "link": "http://arxiv.org/pdf/2312.06065v1"
    },
    {
        "title": "Speaker-Text Retrieval via Contrastive Learning",
        "authors": [
            "Xuechen Liu",
            "Xin Wang",
            "Erica Cooper",
            "Xiaoxiao Miao",
            "Junichi Yamagishi"
        ],
        "abstract": "In this study, we introduce a novel cross-modal retrieval task involving\nspeaker descriptions and their corresponding audio samples. Utilizing\npre-trained speaker and text encoders, we present a simple learning framework\nbased on contrastive learning. Additionally, we explore the impact of\nincorporating speaker labels into the training process. Our findings establish\nthe effectiveness of linking speaker and text information for the task for both\nEnglish and Japanese languages, across diverse data configurations. Additional\nvisual analysis unveils potential nuanced associations between speaker\nclustering and retrieval performance.",
        "date": "2023-12-11T01:23:50+00:00",
        "link": "http://arxiv.org/pdf/2312.06055v1"
    }
]