[
    {
        "title": "Disentangling CO Chemistry in a Protoplanetary Disk Using Explanatory Machine Learning Techniques",
        "authors": [
            "Amina Diop",
            "Ilse Cleeves",
            "Dana Anderson",
            "Jamila Pegues",
            "Adele Plunkett"
        ],
        "abstract": "Molecular abundances in protoplanetary disks are highly sensitive to the\nlocal physical conditions, including gas temperature, gas density, radiation\nfield, and dust properties. Often multiple factors are intertwined, impacting\nthe abundances of both simple and complex species. We present a new approach to\nunderstanding these chemical and physical interdependencies using machine\nlearning. Specifically we explore the case of CO modeled under the conditions\nof a generic disk and build an explanatory regression model to study the\ndependence of CO spatial density on the gas density, gas temperature, cosmic\nray ionization rate, X-ray ionization rate, and UV flux. Our findings indicate\nthat combinations of parameters play a surprisingly powerful role in regulating\nCO compared to any singular physical parameter. Moreover, in general, we find\nthe conditions in the disk are destructive toward CO. CO depletion is further\nenhanced in an increased cosmic ray environment and in disks with higher\ninitial C/O ratios. These dependencies uncovered by our new approach are\nconsistent with previous studies, which are more modeling intensive and\ncomputationally expensive. Our work thus shows that machine learning can be a\npowerful tool not only for creating efficient predictive models, but also for\nenabling a deeper understanding of complex chemical processes.",
        "date": "2023-12-08T18:59:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05254v1"
    },
    {
        "title": "KBFormer: A Diffusion Model for Structured Entity Completion",
        "authors": [
            "Ouail Kitouni",
            "Niklas Nolte",
            "James Hensman",
            "Bhaskar Mitra"
        ],
        "abstract": "We develop a generative attention-based approach to modeling structured\nentities comprising different property types, such as numerical, categorical,\nstring, and composite. This approach handles such heterogeneous data through a\nmixed continuous-discrete diffusion process over the properties. Our flexible\nframework can model entities with arbitrary hierarchical properties, enabling\napplications to structured Knowledge Base (KB) entities and tabular data. Our\napproach obtains state-of-the-art performance on a majority of cases across 15\ndatasets. In addition, experiments with a device KB and a nuclear physics\ndataset demonstrate the model's ability to learn representations useful for\nentity completion in diverse settings. This has many downstream use cases,\nincluding modeling numerical properties with high accuracy - critical for\nscience applications, which also benefit from the model's inherent\nprobabilistic nature.",
        "date": "2023-12-08T18:59:14+00:00",
        "link": "http://arxiv.org/pdf/2312.05253v1"
    },
    {
        "title": "TaskMet: Task-Driven Metric Learning for Model Learning",
        "authors": [
            "Dishank Bansal",
            "Ricky T. Q. Chen",
            "Mustafa Mukadam",
            "Brandon Amos"
        ],
        "abstract": "Deep learning models are often deployed in downstream tasks that the training\nprocedure may not be aware of. For example, models solely trained to achieve\naccurate predictions may struggle to perform well on downstream tasks because\nseemingly small prediction errors may incur drastic task errors. The standard\nend-to-end learning approach is to make the task loss differentiable or to\nintroduce a differentiable surrogate that the model can be trained on. In these\nsettings, the task loss needs to be carefully balanced with the prediction loss\nbecause they may have conflicting objectives. We propose take the task loss\nsignal one level deeper than the parameters of the model and use it to learn\nthe parameters of the loss function the model is trained on, which can be done\nby learning a metric in the prediction space. This approach does not alter the\noptimal prediction model itself, but rather changes the model learning to\nemphasize the information important for the downstream task. This enables us to\nachieve the best of both worlds: a prediction model trained in the original\nprediction space while also being valuable for the desired downstream task. We\nvalidate our approach through experiments conducted in two main settings: 1)\ndecision-focused model learning scenarios involving portfolio optimization and\nbudget allocation, and 2) reinforcement learning in noisy environments with\ndistracting states. The source code to reproduce our experiments is available\nat https://github.com/facebookresearch/taskmet",
        "date": "2023-12-08T18:59:03+00:00",
        "link": "http://arxiv.org/pdf/2312.05250v1"
    },
    {
        "title": "Topology-Based Reconstruction Prevention for Decentralised Learning",
        "authors": [
            "Florine W. Dekker",
            "Zekeriya Erkin",
            "Mauro Conti"
        ],
        "abstract": "Decentralised learning has recently gained traction as an alternative to\nfederated learning in which both data and coordination are distributed over its\nusers. To preserve the confidentiality of users' data, decentralised learning\nrelies on differential privacy, multi-party computation, or a combination\nthereof. However, running multiple privacy-preserving summations in sequence\nmay allow adversaries to perform reconstruction attacks. Unfortunately, current\nreconstruction countermeasures either cannot trivially be adapted to the\ndistributed setting, or add excessive amounts of noise.\n  In this work, we first show that passive honest-but-curious adversaries can\nreconstruct other users' private data after several privacy-preserving\nsummations. For example, in subgraphs with 18 users, we show that only three\npassive honest-but-curious adversaries succeed at reconstructing private data\n11.0% of the time, requiring an average of 8.8 summations per adversary. The\nsuccess rate is independent of the size of the full network. We consider weak\nadversaries, who do not control the graph topology and can exploit neither the\nworkings of the summation protocol nor the specifics of users' data.\n  We develop a mathematical understanding of how reconstruction relates to\ntopology and propose the first topology-based decentralised defence against\nreconstruction attacks. Specifically, we show that reconstruction requires a\nnumber of adversaries linear in the length of the network's shortest cycle.\nConsequently, reconstructing private data from privacy-preserving summations is\nimpossible in acyclic networks.\n  Our work is a stepping stone for a formal theory of decentralised\nreconstruction defences based on topology. Such a theory would generalise our\ncountermeasure beyond summation, define confidentiality in terms of entropy,\nand describe the effects of (topology-aware) differential privacy.",
        "date": "2023-12-08T18:55:40+00:00",
        "link": "http://arxiv.org/pdf/2312.05248v1"
    },
    {
        "title": "The impact of heteroskedasticity on uplift modeling",
        "authors": [
            "Björn Bokelmann",
            "Stefan Lessmann"
        ],
        "abstract": "There are various applications, where companies need to decide to which\nindividuals they should best allocate treatment. To support such decisions,\nuplift models are applied to predict treatment effects on an individual level.\nBased on the predicted treatment effects, individuals can be ranked and\ntreatment allocation can be prioritized according to this ranking. An implicit\nassumption, which has not been doubted in the previous uplift modeling\nliterature, is that this treatment prioritization approach tends to bring\nindividuals with high treatment effects to the top and individuals with low\ntreatment effects to the bottom of the ranking. In our research, we show that\nheteroskedastictity in the training data can cause a bias of the uplift model\nranking: individuals with the highest treatment effects can get accumulated in\nlarge numbers at the bottom of the ranking. We explain theoretically how\nheteroskedasticity can bias the ranking of uplift models and show this process\nin a simulation and on real-world data. We argue that this problem of ranking\nbias due to heteroskedasticity might occur in many real-world applications and\nrequires modification of the treatment prioritization to achieve an efficient\ntreatment allocation.",
        "date": "2023-12-08T18:32:27+00:00",
        "link": "http://arxiv.org/pdf/2312.05234v1"
    },
    {
        "title": "Modeling Risk in Reinforcement Learning: A Literature Mapping",
        "authors": [
            "Leonardo Villalobos-Arias",
            "Derek Martin",
            "Abhijeet Krishnan",
            "Madeleine Gagné",
            "Colin M. Potts",
            "Arnav Jhala"
        ],
        "abstract": "Safe reinforcement learning deals with mitigating or avoiding unsafe\nsituations by reinforcement learning (RL) agents. Safe RL approaches are based\non specific risk representations for particular problems or domains. In order\nto analyze agent behaviors, compare safe RL approaches, and effectively\ntransfer techniques between application domains, it is necessary to understand\nthe types of risk specific to safe RL problems. We performed a systematic\nliterature mapping with the objective to characterize risk in safe RL. Based on\nthe obtained results, we present definitions, characteristics, and types of\nrisk that hold on multiple application domains. Our literature mapping covers\nliterature from the last 5 years (2017-2022), from a variety of knowledge areas\n(AI, finance, engineering, medicine) where RL approaches emphasize risk\nrepresentation and management. Our mapping covers 72 papers filtered\nsystematically from over thousands of papers on the topic. Our proposed notion\nof risk covers a variety of representations, disciplinary differences, common\ntraining exercises, and types of techniques. We encourage researchers to\ninclude explicit and detailed accounts of risk in future safe RL research\nreports, using this mapping as a starting point. With this information,\nresearchers and practitioners could draw stronger conclusions on the\neffectiveness of techniques on different problems.",
        "date": "2023-12-08T18:26:08+00:00",
        "link": "http://arxiv.org/pdf/2312.05231v1"
    },
    {
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
        "authors": [
            "Zhiting Hu",
            "Tianmin Shu"
        ],
        "abstract": "Despite their tremendous success in many applications, large language models\noften fall short of consistent reasoning and planning in various (language,\nembodied, and social) scenarios, due to inherent limitations in their\ninference, learning, and modeling capabilities. In this position paper, we\npresent a new perspective of machine reasoning, LAW, that connects the concepts\nof Language models, Agent models, and World models, for more robust and\nversatile reasoning capabilities. In particular, we propose that world and\nagent models are a better abstraction of reasoning, that introduces the crucial\nelements of deliberate human-like reasoning, including beliefs about the world\nand other agents, anticipation of consequences, goals/rewards, and strategic\nplanning. Crucially, language models in LAW serve as a backend to implement the\nsystem or its elements and hence provide the computational power and\nadaptability. We review the recent studies that have made relevant progress and\ndiscuss future research directions towards operationalizing the LAW framework.",
        "date": "2023-12-08T18:25:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05230v1"
    },
    {
        "title": "Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration",
        "authors": [
            "Qi-Wei Wang",
            "Da-Wei Zhou",
            "Yi-Kai Zhang",
            "De-Chuan Zhan",
            "Han-Jia Ye"
        ],
        "abstract": "Real-world scenarios are usually accompanied by continuously appearing\nclasses with scare labeled samples, which require the machine learning model to\nincrementally learn new classes and maintain the knowledge of base classes. In\nthis Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methods\neither introduce extra learnable components or rely on a frozen feature\nextractor to mitigate catastrophic forgetting and overfitting problems.\nHowever, we find a tendency for existing methods to misclassify the samples of\nnew classes into base classes, which leads to the poor performance of new\nclasses. In other words, the strong discriminability of base classes distracts\nthe classification of new classes. To figure out this intriguing phenomenon, we\nobserve that although the feature extractor is only trained on base classes, it\ncan surprisingly represent the semantic similarity between the base and unseen\nnew classes. Building upon these analyses, we propose a simple yet effective\nTraining-frEE calibratioN (TEEN) strategy to enhance the discriminability of\nnew classes by fusing the new prototypes (i.e., mean features of a class) with\nweighted base prototypes. In addition to standard benchmarks in FSCIL, TEEN\ndemonstrates remarkable performance and consistent improvements over baseline\nmethods in the few-shot learning scenario. Code is available at:\nhttps://github.com/wangkiw/TEEN",
        "date": "2023-12-08T18:24:08+00:00",
        "link": "http://arxiv.org/pdf/2312.05229v1"
    },
    {
        "title": "Neural Spectral Methods: Self-supervised learning in the spectral domain",
        "authors": [
            "Yiheng Du",
            "Nithin Chalapathi",
            "Aditi Krishnapriyan"
        ],
        "abstract": "We present Neural Spectral Methods, a technique to solve parametric Partial\nDifferential Equations (PDEs), grounded in classical spectral methods. Our\nmethod uses orthogonal bases to learn PDE solutions as mappings between\nspectral coefficients. In contrast to current machine learning approaches which\nenforce PDE constraints by minimizing the numerical quadrature of the residuals\nin the spatiotemporal domain, we leverage Parseval's identity and introduce a\nnew training strategy through a \\textit{spectral loss}. Our spectral loss\nenables more efficient differentiation through the neural network, and\nsubstantially reduces training complexity. At inference time, the computational\ncost of our method remains constant, regardless of the spatiotemporal\nresolution of the domain. Our experimental results demonstrate that our method\nsignificantly outperforms previous machine learning approaches in terms of\nspeed and accuracy by one to two orders of magnitude on multiple different\nproblems. When compared to numerical solvers of the same accuracy, our method\ndemonstrates a $10\\times$ increase in performance speed.",
        "date": "2023-12-08T18:20:43+00:00",
        "link": "http://arxiv.org/pdf/2312.05225v1"
    },
    {
        "title": "DeltaZip: Multi-Tenant Language Model Serving via Delta Compression",
        "authors": [
            "Xiaozhe Yao",
            "Ana Klimovic"
        ],
        "abstract": "Fine-tuning large language models (LLMs) for downstream tasks can greatly\nimprove model quality, however serving many different fine-tuned LLMs\nconcurrently for users in multi-tenant environments is challenging. Dedicating\nGPU memory for each model is prohibitively expensive and naively swapping large\nmodel weights in and out of GPU memory is slow. Our key insight is that\nfine-tuned models can be quickly swapped in and out of GPU memory by extracting\nand compressing the delta between each model and its pre-trained base model. We\npropose DeltaZip, an LLM serving system that efficiently serves multiple\nfull-parameter fine-tuned models concurrently by aggressively compressing model\ndeltas by a factor of $6\\times$ to $8\\times$ while maintaining high model\nquality. DeltaZip increases serving throughput by $1.5\\times$ to $3\\times$ and\nimproves SLO attainment compared to a vanilla HuggingFace serving system.",
        "date": "2023-12-08T18:07:05+00:00",
        "link": "http://arxiv.org/pdf/2312.05215v1"
    },
    {
        "title": "Conformal Prediction in Multi-User Settings: An Evaluation",
        "authors": [
            "Enrique Garcia-Ceja",
            "Luciano Garcia-Banuelos",
            "Nicolas Jourdan"
        ],
        "abstract": "Typically, machine learning models are trained and evaluated without making\nany distinction between users (e.g, using traditional hold-out and\ncross-validation). However, this produces inaccurate performance metrics\nestimates in multi-user settings. That is, situations where the data were\ncollected by multiple users with different characteristics (e.g., age, gender,\nheight, etc.) which is very common in user computer interaction and medical\napplications. For these types of scenarios model evaluation strategies that\nprovide better performance estimates have been proposed such as mixed,\nuser-independent, user-dependent, and user-adaptive models. Although those\nstrategies are better suited for multi-user systems, they are typically\nassessed with respect to performance metrics that capture the overall behavior\nof the models and do not provide any performance guarantees for individual\npredictions nor they provide any feedback about the predictions' uncertainty.\nIn order to overcome those limitations, in this work we evaluated the conformal\nprediction framework in several multi-user settings. Conformal prediction is a\nmodel agnostic method that provides confidence guarantees on the predictions,\nthus, increasing the trustworthiness and robustness of the models. We conducted\nextensive experiments using different evaluation strategies and found\nsignificant differences in terms of conformal performance measures. We also\nproposed several visualizations based on matrices, graphs, and charts that\ncapture different aspects of the resulting prediction sets.",
        "date": "2023-12-08T17:33:23+00:00",
        "link": "http://arxiv.org/pdf/2312.05195v1"
    },
    {
        "title": "AI Competitions and Benchmarks: Competition platforms",
        "authors": [
            "Andrey Ustyuzhanin",
            "Harald Carlens"
        ],
        "abstract": "The ecosystem of artificial intelligence competitions is a diverse and\nmultifaceted landscape, encompassing a variety of platforms that each host\nnumerous competitions annually, alongside a plethora of specialized websites\ndedicated to singular contests. These platforms adeptly manage the overarching\nadministrative responsibilities inherent in orchestrating competitions, thus\naffording organizers the liberty to allocate greater attention to other facets\nof their contests. Notably, these platforms exhibit considerable diversity in\ntheir operational functionalities, economic models, and community dynamics.\nThis chapter conducts an extensive review of the foremost services in this\nrealm and elucidates several alternative methodologies that facilitate the\nindependent hosting of such challenges. Keywords: competition platform,\nchallenge hosting services, comparison.",
        "date": "2023-12-08T17:16:50+00:00",
        "link": "http://arxiv.org/pdf/2312.05185v1"
    },
    {
        "title": "TENPLEX: Changing Resources of Deep Learning Jobs using Parallelizable Tensor Collections",
        "authors": [
            "Marcel Wagenländer",
            "Guo Li",
            "Bo Zhao",
            "Luo Mai",
            "Peter Pietzuch"
        ],
        "abstract": "Deep learning (DL) jobs use multi-dimensional parallelism, i.e they combine\ndata, model, and pipeline parallelism, to use large GPU clusters efficiently.\nThis couples jobs tightly to a set of GPU devices, but jobs may experience\nchanges to the device allocation: (i) resource elasticity during training adds\nor removes devices; (ii) hardware maintenance may require redeployment on\ndifferent devices; and (iii) device failures force jobs to run with fewer\ndevices. Current DL frameworks lack support for these scenarios, as they cannot\nchange the multi-dimensional parallelism of an already-running job in an\nefficient and model-independent way.\n  We describe Tenplex, a state management library for DL frameworks that\nenables jobs to change the GPU allocation and job parallelism at runtime.\nTenplex achieves this by externalizing the DL job state during training as a\nparallelizable tensor collection (PTC). When the GPU allocation for the DL job\nchanges, Tenplex uses the PTC to transform the DL job state: for the dataset\nstate, Tenplex repartitions it under data parallelism and exposes it to workers\nthrough a virtual file system; for the model state, Tenplex obtains it as\npartitioned checkpoints and transforms them to reflect the new parallelization\nconfiguration. For efficiency, these PTC transformations are executed in\nparallel with a minimum amount of data movement between devices and workers.\nOur experiments show that Tenplex enables DL jobs to support dynamic\nparallelization with low overhead.",
        "date": "2023-12-08T17:08:03+00:00",
        "link": "http://arxiv.org/pdf/2312.05181v1"
    },
    {
        "title": "MRI Scan Synthesis Methods based on Clustering and Pix2Pix",
        "authors": [
            "Giulia Baldini",
            "Melanie Schmidt",
            "Charlotte Zäske",
            "Liliana L. Caldeira"
        ],
        "abstract": "We consider a missing data problem in the context of automatic segmentation\nmethods for Magnetic Resonance Imaging (MRI) brain scans. Usually, automated\nMRI scan segmentation is based on multiple scans (e.g., T1-weighted,\nT2-weighted, T1CE, FLAIR). However, quite often a scan is blurry, missing or\notherwise unusable. We investigate the question whether a missing scan can be\nsynthesized. We exemplify that this is in principle possible by synthesizing a\nT2-weighted scan from a given T1-weighted scan. Our first aim is to compute a\npicture that resembles the missing scan closely, measured by average mean\nsquared error (MSE). We develop/use several methods for this, including a\nrandom baseline approach, a clustering-based method and pixel-to-pixel\ntranslation method by (Pix2Pix) which is based on conditional GANs. The lowest\nMSE is achieved by our clustering-based method. Our second aim is to compare\nthe methods with respect to the affect that using the synthesized scan has on\nthe segmentation process. For this, we use a DeepMedic model trained with the\nfour input scan modalities named above. We replace the T2-weighted scan by the\nsynthesized picture and evaluate the segmentations with respect to the tumor\nidentification, using Dice scores as numerical evaluation. The evaluation shows\nthat the segmentation works well with synthesized scans (in particular, with\nPix2Pix methods) in many cases.",
        "date": "2023-12-08T16:59:17+00:00",
        "link": "http://arxiv.org/pdf/2312.05176v1"
    },
    {
        "title": "A Review of Cooperation in Multi-agent Learning",
        "authors": [
            "Yali Du",
            "Joel Z. Leibo",
            "Usman Islam",
            "Richard Willis",
            "Peter Sunehag"
        ],
        "abstract": "Cooperation in multi-agent learning (MAL) is a topic at the intersection of\nnumerous disciplines, including game theory, economics, social sciences, and\nevolutionary biology. Research in this area aims to understand both how agents\ncan coordinate effectively when goals are aligned and how they may cooperate in\nsettings where gains from working together are possible but possibilities for\nconflict abound. In this paper we provide an overview of the fundamental\nconcepts, problem settings and algorithms of multi-agent learning. This\nencompasses reinforcement learning, multi-agent sequential decision-making,\nchallenges associated with multi-agent cooperation, and a comprehensive review\nof recent progress, along with an evaluation of relevant metrics. Finally we\ndiscuss open challenges in the field with the aim of inspiring new avenues for\nresearch.",
        "date": "2023-12-08T16:42:15+00:00",
        "link": "http://arxiv.org/pdf/2312.05162v1"
    },
    {
        "title": "Detecting Atomic Scale Surface Defects in STM of TMDs with Ensemble Deep Learning",
        "authors": [
            "Darian Smalley",
            "Stephanie D. Lough",
            "Luke Holtzman",
            "Kaikui Xu",
            "Madisen Holbrook",
            "Matthew R. Rosenberger",
            "J. C. Hone",
            "Katayun Barmak",
            "Masahiro Ishigami"
        ],
        "abstract": "Atomic-scale defect detection is shown in scanning tunneling microscopy\nimages of single crystal WSe2 using an ensemble of U-Net-like convolutional\nneural networks. Standard deep learning test metrics indicated good detection\nperformance with an average F1 score of 0.66 and demonstrated ensemble\ngeneralization to C-AFM images of WSe2 and STM images of MoSe2. Defect\ncoordinates were automatically extracted from defect detections maps showing\nthat STM image analysis enhanced by machine learning can be used to\ndramatically increase sample characterization throughput.",
        "date": "2023-12-08T16:38:51+00:00",
        "link": "http://arxiv.org/pdf/2312.05160v1"
    },
    {
        "title": "Deep Learning-Based Pilotless Spatial Multiplexing",
        "authors": [
            "Dani Korpi",
            "Mikko Honkala",
            "Janne M. J. Huttunen"
        ],
        "abstract": "This paper investigates the feasibility of machine learning (ML)-based\npilotless spatial multiplexing in multiple-input and multiple-output (MIMO)\ncommunication systems. Especially, it is shown that by training the transmitter\nand receiver jointly, the transmitter can learn such constellation shapes for\nthe spatial streams which facilitate completely blind separation and detection\nby the simultaneously learned receiver. To the best of our knowledge, this is\nthe first time ML-based spatial multiplexing without channel estimation pilots\nis demonstrated. The results show that the learned pilotless scheme can\noutperform a conventional pilot-based system by as much as 15-20% in terms of\nspectral efficiency, depending on the modulation order and signal-to-noise\nratio.",
        "date": "2023-12-08T16:38:02+00:00",
        "link": "http://arxiv.org/pdf/2312.05158v1"
    },
    {
        "title": "Uncertainty Quantification and Propagation in Surrogate-based Bayesian Inference",
        "authors": [
            "Philipp Reiser",
            "Javier Enrique Aguilar",
            "Anneli Guthke",
            "Paul-Christian Bürkner"
        ],
        "abstract": "Surrogate models are statistical or conceptual approximations for more\ncomplex simulation models. In this context, it is crucial to propagate the\nuncertainty induced by limited simulation budget and surrogate approximation\nerror to predictions, inference, and subsequent decision-relevant quantities.\nHowever, quantifying and then propagating the uncertainty of surrogates is\nusually limited to special analytic cases or is otherwise computationally very\nexpensive. In this paper, we propose a framework enabling a scalable, Bayesian\napproach to surrogate modeling with thorough uncertainty quantification,\npropagation, and validation. Specifically, we present three methods for\nBayesian inference with surrogate models given measurement data. This is a task\nwhere the propagation of surrogate uncertainty is especially relevant, because\nfailing to account for it may lead to biased and/or overconfident estimates of\nthe parameters of interest. We showcase our approach in two detailed case\nstudies for both linear and nonlinear modeling scenarios. Uncertainty\npropagation in surrogate models enables more reliable and safe approximation of\nexpensive simulators and will therefore be useful in various fields of\napplications.",
        "date": "2023-12-08T16:31:52+00:00",
        "link": "http://arxiv.org/pdf/2312.05153v1"
    },
    {
        "title": "Kraken: enabling joint trajectory prediction by utilizing Mode Transformer and Greedy Mode Processing",
        "authors": [
            "Daniil S. Antonenko",
            "Stepan Konev",
            "Yuriy Biktairov",
            "Boris Yangel"
        ],
        "abstract": "Accurate and reliable motion prediction is essential for safe urban autonomy.\nThe most prominent motion prediction approaches are based on modeling the\ndistribution of possible future trajectories of each actor in autonomous\nsystem's vicinity. These \"independent\" marginal predictions might be accurate\nenough to properly describe casual driving situations where the prediction\ntarget is not likely to interact with other actors. They are, however,\ninadequate for modeling interactive situations where the actors' future\ntrajectories are likely to intersect. To mitigate this issue we propose Kraken\n-- a real-time trajectory prediction model capable of approximating pairwise\ninteractions between the actors as well as producing accurate marginal\npredictions. Kraken relies on a simple Greedy Mode Processing technique\nallowing it to convert a factorized prediction for a pair of agents into a\nphysically-plausible joint prediction. It also utilizes the Mode Transformer\nmodule to increase the diversity of predicted trajectories and make the joint\nprediction more informative. We evaluate Kraken on Waymo Motion Prediction\nchallenge where it held the first place in the Interaction leaderboard and the\nsecond place in the Motion leaderboard in October 2021.",
        "date": "2023-12-08T16:24:05+00:00",
        "link": "http://arxiv.org/pdf/2312.05144v1"
    },
    {
        "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
        "authors": [
            "Shuai Tang",
            "Zhiwei Steven Wu",
            "Sergul Aydore",
            "Michael Kearns",
            "Aaron Roth"
        ],
        "abstract": "Recently, diffusion models have become popular tools for image synthesis\nbecause of their high-quality outputs. However, like other large-scale models,\nthey may leak private information about their training data. Here, we\ndemonstrate a privacy vulnerability of diffusion models through a\n\\emph{membership inference (MI) attack}, which aims to identify whether a\ntarget example belongs to the training set when given the trained diffusion\nmodel. Our proposed MI attack learns quantile regression models that predict (a\nquantile of) the distribution of reconstruction loss on examples not used in\ntraining. This allows us to define a granular hypothesis test for determining\nthe membership of a point in the training set, based on thresholding the\nreconstruction loss of that point using a custom threshold tailored to the\nexample. We also provide a simple bootstrap technique that takes a majority\nmembership prediction over ``a bag of weak attackers'' which improves the\naccuracy over individual quantile regression models. We show that our attack\noutperforms the prior state-of-the-art attack while being substantially less\ncomputationally expensive -- prior attacks required training multiple ``shadow\nmodels'' with the same architecture as the model under attack, whereas our\nattack requires training only much smaller models.",
        "date": "2023-12-08T16:21:24+00:00",
        "link": "http://arxiv.org/pdf/2312.05140v1"
    },
    {
        "title": "Optimal Multi-Distribution Learning",
        "authors": [
            "Zihan Zhang",
            "Wenhao Zhan",
            "Yuxin Chen",
            "Simon S. Du",
            "Jason D. Lee"
        ],
        "abstract": "Multi-distribution learning (MDL), which seeks to learn a shared model that\nminimizes the worst-case risk across $k$ distinct data distributions, has\nemerged as a unified framework in response to the evolving demand for\nrobustness, fairness, multi-group collaboration, etc. Achieving data-efficient\nMDL necessitates adaptive sampling, also called on-demand sampling, throughout\nthe learning process. However, there exist substantial gaps between the\nstate-of-the-art upper and lower bounds on the optimal sample complexity.\nFocusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we\npropose a novel algorithm that yields an $varepsilon$-optimal randomized\nhypothesis with a sample complexity on the order of $(d+k)/\\varepsilon^2$\n(modulo some logarithmic factor), matching the best-known lower bound. Our\nalgorithmic ideas and theory have been further extended to accommodate\nRademacher classes. The proposed algorithms are oracle-efficient, which access\nthe hypothesis class solely through an empirical risk minimization oracle.\nAdditionally, we establish the necessity of randomization, unveiling a large\nsample size barrier when only deterministic hypotheses are permitted. These\nfindings successfully resolve three open problems presented in COLT 2023 (i.e.,\nAwasthi et al., (2023, Problem 1, 3 and 4)).",
        "date": "2023-12-08T16:06:29+00:00",
        "link": "http://arxiv.org/pdf/2312.05134v1"
    },
    {
        "title": "On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against \"Truly Anonymous Synthetic Data''",
        "authors": [
            "Georgi Ganev",
            "Emiliano De Cristofaro"
        ],
        "abstract": "Training generative models to produce synthetic data is meant to provide a\nprivacy-friendly approach to data release. However, we get robust guarantees\nonly when models are trained to satisfy Differential Privacy (DP). Alas, this\nis not the standard in industry as many companies use ad-hoc strategies to\nempirically evaluate privacy based on the statistical similarity between\nsynthetic and real data. In this paper, we review the privacy metrics offered\nby leading companies in this space and shed light on a few critical flaws in\nreasoning about privacy entirely via empirical evaluations. We analyze the\nundesirable properties of the most popular metrics and filters and demonstrate\ntheir unreliability and inconsistency through counter-examples. We then present\na reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks all\nattributes of) at least 78% of the low-density train records (or outliers) with\nonly black-box access to a single fitted generative model and the privacy\nmetrics. Finally, we show that applying DP only to the model or using\nlow-utility generators does not mitigate ReconSyn as the privacy leakage\npredominantly comes from the metrics. Overall, our work serves as a warning to\npractitioners not to deviate from established privacy-preserving mechanisms.",
        "date": "2023-12-08T15:42:28+00:00",
        "link": "http://arxiv.org/pdf/2312.05114v1"
    },
    {
        "title": "TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce",
        "authors": [
            "Tongxin Hu",
            "Zhuang Li",
            "Xin Jin",
            "Lizhen Qu",
            "Xin Zhang"
        ],
        "abstract": "Annually, e-commerce platforms incur substantial financial losses due to\ntrademark infringements, making it crucial to identify and mitigate potential\nlegal risks tied to merchant information registered to the platforms. However,\nthe absence of high-quality datasets hampers research in this area. To address\nthis gap, our study introduces TMID, a novel dataset to detect trademark\ninfringement in merchant registrations. This is a real-world dataset sourced\ndirectly from Alipay, one of the world's largest e-commerce and digital payment\nplatforms. As infringement detection is a legal reasoning task requiring an\nunderstanding of the contexts and legal rules, we offer a thorough collection\nof legal rules and merchant and trademark-related contextual information with\nannotations from legal experts. We ensure the data quality by performing an\nextensive statistical analysis. Furthermore, we conduct an empirical study on\nthis dataset to highlight its value and the key challenges. Through this study,\nwe aim to contribute valuable resources to advance research into legal\ncompliance related to trademark infringement within the e-commerce sphere. The\ndataset is available at https://github.com/emnlpTMID/emnlpTMID.github.io .",
        "date": "2023-12-08T15:31:39+00:00",
        "link": "http://arxiv.org/pdf/2312.05103v1"
    },
    {
        "title": "Continual learning for surface defect segmentation by subnetwork creation and selection",
        "authors": [
            "Aleksandr Dekhovich",
            "Miguel A. Bessa"
        ],
        "abstract": "We introduce a new continual (or lifelong) learning algorithm called LDA-CP&S\nthat performs segmentation tasks without undergoing catastrophic forgetting.\nThe method is applied to two different surface defect segmentation problems\nthat are learned incrementally, i.e. providing data about one type of defect at\na time, while still being capable of predicting every defect that was seen\npreviously. Our method creates a defect-related subnetwork for each defect type\nvia iterative pruning and trains a classifier based on linear discriminant\nanalysis (LDA). At the inference stage, we first predict the defect type with\nLDA and then predict the surface defects using the selected subnetwork. We\ncompare our method with other continual learning methods showing a significant\nimprovement -- mean Intersection over Union better by a factor of two when\ncompared to existing methods on both datasets. Importantly, our approach shows\ncomparable results with joint training when all the training data (all defects)\nare seen simultaneously",
        "date": "2023-12-08T15:28:50+00:00",
        "link": "http://arxiv.org/pdf/2312.05100v1"
    },
    {
        "title": "INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers",
        "authors": [
            "Anjan Karmakar",
            "Romain Robbes"
        ],
        "abstract": "Pre-trained models of source code have recently been successfully applied to\na wide variety of Software Engineering tasks; they have also seen some\npractical adoption in practice, e.g. for code completion. Yet, we still know\nvery little about what these pre-trained models learn about source code. In\nthis article, we use probing--simple diagnostic tasks that do not further train\nthe models--to discover to what extent pre-trained models learn about specific\naspects of source code. We use an extensible framework to define 15 probing\ntasks that exercise surface, syntactic, structural and semantic characteristics\nof source code. We probe 8 pre-trained source code models, as well as a natural\nlanguage model (BERT) as our baseline. We find that models that incorporate\nsome structural information (such as GraphCodeBERT) have a better\nrepresentation of source code characteristics. Surprisingly, we find that for\nsome probing tasks, BERT is competitive with the source code models, indicating\nthat there are ample opportunities to improve source-code specific pre-training\non the respective code characteristics. We encourage other researchers to\nevaluate their models with our probing task suite, so that they may peer into\nthe hidden layers of the models and identify what intrinsic code\ncharacteristics are encoded.",
        "date": "2023-12-08T15:21:54+00:00",
        "link": "http://arxiv.org/pdf/2312.05092v1"
    },
    {
        "title": "UniTSA: A Universal Reinforcement Learning Framework for V2X Traffic Signal Control",
        "authors": [
            "Maonan Wang",
            "Xi Xiong",
            "Yuheng Kan",
            "Chengcheng Xu",
            "Man-On Pun"
        ],
        "abstract": "Traffic congestion is a persistent problem in urban areas, which calls for\nthe development of effective traffic signal control (TSC) systems. While\nexisting Reinforcement Learning (RL)-based methods have shown promising\nperformance in optimizing TSC, it is challenging to generalize these methods\nacross intersections of different structures. In this work, a universal\nRL-based TSC framework is proposed for Vehicle-to-Everything (V2X)\nenvironments. The proposed framework introduces a novel agent design that\nincorporates a junction matrix to characterize intersection states, making the\nproposed model applicable to diverse intersections. To equip the proposed\nRL-based framework with enhanced capability of handling various intersection\nstructures, novel traffic state augmentation methods are tailor-made for signal\nlight control systems. Finally, extensive experimental results derived from\nmultiple intersection configurations confirm the effectiveness of the proposed\nframework. The source code in this work is available at\nhttps://github.com/wmn7/Universal_Light",
        "date": "2023-12-08T15:18:40+00:00",
        "link": "http://arxiv.org/pdf/2312.05090v1"
    },
    {
        "title": "A Distributed ADMM-based Deep Learning Approach for Thermal Control in Multi-Zone Buildings",
        "authors": [
            "Vincent Taboga",
            "Hanane Dagdougui"
        ],
        "abstract": "The surge in electricity use, coupled with the dependency on intermittent\nrenewable energy sources, poses significant hurdles to effectively managing\npower grids, particularly during times of peak demand. Demand Response programs\nand energy conservation measures are essential to operate energy grids while\nensuring a responsible use of our resources This research combines distributed\noptimization using ADMM with Deep Learning models to plan indoor temperature\nsetpoints effectively. A two-layer hierarchical structure is used, with a\ncentral building coordinator at the upper layer and local controllers at the\nthermal zone layer. The coordinator must limit the building's maximum power by\ntranslating the building's total power to local power targets for each zone.\nLocal controllers can modify the temperature setpoints to meet the local power\ntargets. The resulting control algorithm, called Distributed Planning Networks,\nis designed to be both adaptable and scalable to many types of buildings,\ntackling two of the main challenges in the development of such systems. The\nproposed approach is tested on an 18-zone building modeled in EnergyPlus. The\nalgorithm successfully manages Demand Response peak events.",
        "date": "2023-12-08T14:46:50+00:00",
        "link": "http://arxiv.org/pdf/2312.05073v1"
    },
    {
        "title": "Soft Frequency Capping for Improved Ad Click Prediction in Yahoo Gemini Native",
        "authors": [
            "Michal Aharon",
            "Yohay Kaplan",
            "Rina Levy",
            "Oren Somekh",
            "Ayelet Blanc",
            "Neetai Eshel",
            "Avi Shahar",
            "Assaf Singer",
            "Alex Zlotnik"
        ],
        "abstract": "Yahoo's native advertising (also known as Gemini native) serves billions of\nad impressions daily, reaching a yearly run-rate of many hundred of millions\nUSD. Driving the Gemini native models that are used to predict both click\nprobability (pCTR) and conversion probability (pCONV) is OFFSET - a feature\nenhanced collaborative-filtering (CF) based event prediction algorithm. \\offset\nis a one-pass algorithm that updates its model for every new batch of logged\ndata using a stochastic gradient descent (SGD) based approach. Since OFFSET\nrepresents its users by their features (i.e., user-less model) due to sparsity\nissues, rule based hard frequency capping (HFC) is used to control the number\nof times a certain user views a certain ad. Moreover, related statistics reveal\nthat user ad fatigue results in a dramatic drop in click through rate (CTR).\nTherefore, to improve click prediction accuracy, we propose a soft frequency\ncapping (SFC) approach, where the frequency feature is incorporated into the\nOFFSET model as a user-ad feature and its weight vector is learned via logistic\nregression as part of OFFSET training. Online evaluation of the soft frequency\ncapping algorithm via bucket testing showed a significant 7.3% revenue lift.\nSince then, the frequency feature enhanced model has been pushed to production\nserving all traffic, and is generating a hefty revenue lift for Yahoo Gemini\nnative. We also report related statistics that reveal, among other things, that\nwhile users' gender does not affect ad fatigue, the latter seems to increase\nwith users' age.",
        "date": "2023-12-08T14:12:49+00:00",
        "link": "http://arxiv.org/pdf/2312.05052v1"
    },
    {
        "title": "Backward Learning for Goal-Conditioned Policies",
        "authors": [
            "Marc Höftmann",
            "Jan Robine",
            "Stefan Harmeling"
        ],
        "abstract": "Can we learn policies in reinforcement learning without rewards? Can we learn\na policy just by trying to reach a goal state? We answer these questions\npositively by proposing a multi-step procedure that first learns a world model\nthat goes backward in time, secondly generates goal-reaching backward\ntrajectories, thirdly improves those sequences using shortest path finding\nalgorithms, and finally trains a neural network policy by imitation learning.\nWe evaluate our method on a deterministic maze environment where the\nobservations are $64\\times 64$ pixel bird's eye images and can show that it\nconsistently reaches several goals.",
        "date": "2023-12-08T13:52:16+00:00",
        "link": "http://arxiv.org/pdf/2312.05044v1"
    },
    {
        "title": "SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control",
        "authors": [
            "Jaskirat Singh",
            "Jianming Zhang",
            "Qing Liu",
            "Cameron Smith",
            "Zhe Lin",
            "Liang Zheng"
        ],
        "abstract": "The field of generative image inpainting and object insertion has made\nsignificant progress with the recent advent of latent diffusion models.\nUtilizing a precise object mask can greatly enhance these applications.\nHowever, due to the challenges users encounter in creating high-fidelity masks,\nthere is a tendency for these methods to rely on more coarse masks (e.g.,\nbounding box) for these applications. This results in limited control and\ncompromised background content preservation. To overcome these limitations, we\nintroduce SmartMask, which allows any novice user to create detailed masks for\nprecise object insertion. Combined with a ControlNet-Inpaint model, our\nexperiments demonstrate that SmartMask achieves superior object insertion\nquality, preserving the background content more effectively than previous\nmethods. Notably, unlike prior works the proposed approach can also be used\neven without user-mask guidance, which allows it to perform mask-free object\ninsertion at diverse positions and scales. Furthermore, we find that when used\niteratively with a novel instruction-tuning based planning model, SmartMask can\nbe used to design detailed layouts from scratch. As compared with user-scribble\nbased layout design, we observe that SmartMask allows for better quality\noutputs with layout-to-image generation methods. Project page is available at\nhttps://smartmask-gen.github.io",
        "date": "2023-12-08T13:38:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05039v1"
    },
    {
        "title": "Grasp Force Optimization as a Bilinear Matrix Inequality Problem: A Deep Learning Approach",
        "authors": [
            "Hirakjyoti Basumatary",
            "Daksh Adhar",
            "Riddhiman Shaw",
            "Shyamanta M. Hazarika"
        ],
        "abstract": "Grasp force synthesis is a non-convex optimization problem involving\nconstraints that are bilinear. Traditional approaches to this problem involve\ngeneral-purpose gradient-based nonlinear optimization and semi-definite\nprogramming. With a view towards dealing with postural synergies and non-smooth\nbut convex positive semidefinite constraints, we look beyond gradient-based\noptimization. The focus of this paper is to undertake a grasp analysis of\nbiomimetic grasping in multi-fingered robotic hands as a bilinear matrix\ninequality (BMI) problem. Our analysis is to solve it using a deep learning\napproach to make the algorithm efficiently generate force closure grasps with\noptimal grasp quality on untrained/unseen objects.",
        "date": "2023-12-08T13:28:21+00:00",
        "link": "http://arxiv.org/pdf/2312.05034v1"
    },
    {
        "title": "Synthesizing Traffic Datasets using Graph Neural Networks",
        "authors": [
            "Daniel Rodriguez-Criado",
            "Maria Chli",
            "Luis J. Manso",
            "George Vogiatzis"
        ],
        "abstract": "Traffic congestion in urban areas presents significant challenges, and\nIntelligent Transportation Systems (ITS) have sought to address these via\nautomated and adaptive controls. However, these systems often struggle to\ntransfer simulated experiences to real-world scenarios. This paper introduces a\nnovel methodology for bridging this `sim-real' gap by creating photorealistic\nimages from 2D traffic simulations and recorded junction footage. We propose a\nnovel image generation approach, integrating a Conditional Generative\nAdversarial Network with a Graph Neural Network (GNN) to facilitate the\ncreation of realistic urban traffic images. We harness GNNs' ability to process\ninformation at different levels of abstraction alongside segmented images for\npreserving locality data. The presented architecture leverages the power of\nSPADE and Graph ATtention (GAT) network models to create images based on\nsimulated traffic scenarios. These images are conditioned by factors such as\nentity positions, colors, and time of day. The uniqueness of our approach lies\nin its ability to effectively translate structured and human-readable\nconditions, encoded as graphs, into realistic images. This advancement\ncontributes to applications requiring rich traffic image datasets, from data\naugmentation to urban traffic solutions. We further provide an application to\ntest the model's capabilities, including generating images with manually\ndefined positions for various entities.",
        "date": "2023-12-08T13:24:19+00:00",
        "link": "http://arxiv.org/pdf/2312.05031v1"
    },
    {
        "title": "Reinforcement Learning-Based Bionic Reflex Control for Anthropomorphic Robotic Grasping exploiting Domain Randomization",
        "authors": [
            "Hirakjyoti Basumatary",
            "Daksh Adhar",
            "Atharva Shrawge",
            "Prathamesh Kanbaskar",
            "Shyamanta M. Hazarika"
        ],
        "abstract": "Achieving human-level dexterity in robotic grasping remains a challenging\nendeavor. Robotic hands frequently encounter slippage and deformation during\nobject manipulation, issues rarely encountered by humans due to their sensory\nreceptors, experiential learning, and motor memory. The emulation of the human\ngrasping reflex within robotic hands is referred to as the ``bionic reflex\".\nPast endeavors in the realm of bionic reflex control predominantly relied on\nmodel-based and supervised learning approaches, necessitating human\nintervention during thresholding and labeling tasks. In this study, we\nintroduce an innovative bionic reflex control pipeline, leveraging\nreinforcement learning (RL); thereby eliminating the need for human\nintervention during control design. Our proposed bionic reflex controller has\nbeen designed and tested on an anthropomorphic hand, manipulating deformable\nobjects in the PyBullet physics simulator, incorporating domain randomization\n(DR) for enhanced Sim2Real transferability. Our findings underscore the promise\nof RL as a potent tool for advancing bionic reflex control within\nanthropomorphic robotic hands. We anticipate that this autonomous, RL-based\nbionic reflex controller will catalyze the development of dependable and highly\nefficient robotic and prosthetic hands, revolutionizing human-robot interaction\nand assistive technologies.",
        "date": "2023-12-08T13:04:41+00:00",
        "link": "http://arxiv.org/pdf/2312.05023v1"
    },
    {
        "title": "A Negative Result on Gradient Matching for Selective Backprop",
        "authors": [
            "Lukas Balles",
            "Cedric Archambeau",
            "Giovanni Zappella"
        ],
        "abstract": "With increasing scale in model and dataset size, the training of deep neural\nnetworks becomes a massive computational burden. One approach to speed up the\ntraining process is Selective Backprop. For this approach, we perform a forward\npass to obtain a loss value for each data point in a minibatch. The backward\npass is then restricted to a subset of that minibatch, prioritizing high-loss\nexamples. We build on this approach, but seek to improve the subset selection\nmechanism by choosing the (weighted) subset which best matches the mean\ngradient over the entire minibatch. We use the gradients w.r.t. the model's\nlast layer as a cheap proxy, resulting in virtually no overhead in addition to\nthe forward pass. At the same time, for our experiments we add a simple random\nselection baseline which has been absent from prior work. Surprisingly, we find\nthat both the loss-based as well as the gradient-matching strategy fail to\nconsistently outperform the random baseline.",
        "date": "2023-12-08T13:03:10+00:00",
        "link": "http://arxiv.org/pdf/2312.05021v1"
    },
    {
        "title": "Unbiased Filtering Of Accidental Clicks in Verizon Media Native Advertising",
        "authors": [
            "Yohay Kaplan",
            "Naama Krasne",
            "Alex Shtoff",
            "Oren Somekh"
        ],
        "abstract": "Verizon Media (VZM) native advertising is one of VZM largest and fastest\ngrowing businesses, reaching a run-rate of several hundred million USDs in the\npast year. Driving the VZM native models that are used to predict event\nprobabilities, such as click and conversion probabilities, is OFFSET - a\nfeature enhanced collaborative-filtering based event-prediction algorithm. In\nthis work we focus on the challenge of predicting click-through rates (CTR)\nwhen we are aware that some of the clicks have short dwell-time and are defined\nas accidental clicks. An accidental click implies little affinity between the\nuser and the ad, so predicting that similar users will click on the ad is\ninaccurate. Therefore, it may be beneficial to remove clicks with dwell-time\nlower than a predefined threshold from the training set. However, we cannot\nignore these positive events, as filtering these will cause the model to under\npredict. Previous approaches have tried to apply filtering and then adding\ncorrective biases to the CTR predictions, but did not yield revenue lifts and\ntherefore were not adopted. In this work, we present a new approach where the\npositive weight of the accidental clicks is distributed among all of the\nnegative events (skips), based on their likelihood of causing accidental\nclicks, as predicted by an auxiliary model. These likelihoods are taken as the\ncorrect labels of the negative events, shifting our training from using only\nbinary labels and adopting a binary cross-entropy loss function in our training\nprocess. After showing offline performance improvements, the modified model was\ntested online serving VZM native users, and provided 1.18% revenue lift over\nthe production model which is agnostic to accidental clicks.",
        "date": "2023-12-08T12:54:30+00:00",
        "link": "http://arxiv.org/pdf/2312.05017v1"
    },
    {
        "title": "Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs",
        "authors": [
            "Nicolas Hubert",
            "Pierre Monnin",
            "Heiko Paulheim"
        ],
        "abstract": "Knowledge graphs (KGs) comprise entities interconnected by relations of\ndifferent semantic meanings. KGs are being used in a wide range of\napplications. However, they inherently suffer from incompleteness, i.e.\nentities or facts about entities are missing. Consequently, a larger body of\nworks focuses on the completion of missing information in KGs, which is\ncommonly referred to as link prediction (LP). This task has traditionally and\nextensively been studied in the transductive setting, where all entities and\nrelations in the testing set are observed during training. Recently, several\nworks have tackled the LP task under more challenging settings, where entities\nand relations in the test set may be unobserved during training, or appear in\nonly a few facts. These works are known as inductive, few-shot, and zero-shot\nlink prediction. In this work, we conduct a systematic review of existing works\nin this area. A thorough analysis leads us to point out the undesirable\nexistence of diverging terminologies and task definitions for the\naforementioned settings, which further limits the possibility of comparison\nbetween recent works. We consequently aim at dissecting each setting\nthoroughly, attempting to reveal its intrinsic characteristics. A unifying\nnomenclature is ultimately proposed to refer to each of them in a simple and\nconsistent manner.",
        "date": "2023-12-08T12:13:40+00:00",
        "link": "http://arxiv.org/pdf/2312.04997v1"
    },
    {
        "title": "PFLlib: Personalized Federated Learning Algorithm Library",
        "authors": [
            "Jianqing Zhang",
            "Yang Liu",
            "Yang Hua",
            "Hao Wang",
            "Tao Song",
            "Zhengui Xue",
            "Ruhui Ma",
            "Jian Cao"
        ],
        "abstract": "Amid the ongoing advancements in Federated Learning (FL), a machine learning\nparadigm that allows collaborative learning with data privacy protection,\npersonalized FL (pFL) has gained significant prominence as a research direction\nwithin the FL domain. Whereas traditional FL (tFL) focuses on jointly learning\na global model, pFL aims to achieve a balance between the global and\npersonalized objectives of each client in FL settings. To foster the pFL\nresearch community, we propose PFLlib, a comprehensive pFL algorithm library\nwith an integrated evaluation platform. In PFLlib, We implement 34\nstate-of-the-art FL algorithms (including 7 classic tFL algorithms and 27 pFL\nalgorithms) and provide various evaluation environments with three\nstatistically heterogeneous scenarios and 14 datasets. At present, PFLlib has\nalready gained 850 stars and 199 forks on GitHub.",
        "date": "2023-12-08T12:03:08+00:00",
        "link": "http://arxiv.org/pdf/2312.04992v1"
    },
    {
        "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
        "authors": [
            "Luka Ribar",
            "Ivan Chelombiev",
            "Luke Hudlass-Galley",
            "Charlie Blake",
            "Carlo Luschi",
            "Douglas Orr"
        ],
        "abstract": "Generative large language models (LLMs) have opened up numerous novel\npossibilities, but due to their significant computational requirements their\nubiquitous use remains challenging. Some of the most useful applications\nrequire processing large numbers of samples at a time and using long contexts,\nboth significantly increasing the memory communication load of the models. We\nintroduce SparQ Attention, a technique for increasing the inference throughput\nof LLMs by reducing the memory bandwidth requirements within the attention\nblocks through selective fetching of the cached history. Our proposed technique\ncan be applied directly to off-the-shelf LLMs during inference, without\nrequiring any modification to the pre-training setup or additional fine-tuning.\nWe show how SparQ Attention can decrease the attention memory bandwidth\nrequirements up to eight times without any loss in accuracy by evaluating Llama\n2 and Pythia models on a wide range of downstream tasks.",
        "date": "2023-12-08T11:47:35+00:00",
        "link": "http://arxiv.org/pdf/2312.04985v1"
    },
    {
        "title": "Sequential inductive prediction intervals",
        "authors": [
            "Benny Avelin"
        ],
        "abstract": "In this paper we explore the concept of sequential inductive prediction\nintervals using theory from sequential testing. We furthermore introduce a\n3-parameter PAC definition of prediction intervals that allows us via\nsimulation to achieve almost sharp bounds with high probability.",
        "date": "2023-12-08T10:28:55+00:00",
        "link": "http://arxiv.org/pdf/2312.04950v1"
    },
    {
        "title": "Scientific Preparation for CSST: Classification of Galaxy and Nebula/Star Cluster Based on Deep Learning",
        "authors": [
            "Yuquan Zhang",
            "Zhong Cao",
            "Feng Wang",
            "Lam",
            "Man I",
            "Hui Deng",
            "Ying Mei",
            "Lei Tan"
        ],
        "abstract": "The Chinese Space Station Telescope (abbreviated as CSST) is a future\nadvanced space telescope. Real-time identification of galaxy and nebula/star\ncluster (abbreviated as NSC) images is of great value during CSST survey. While\nrecent research on celestial object recognition has progressed, the rapid and\nefficient identification of high-resolution local celestial images remains\nchallenging. In this study, we conducted galaxy and NSC image classification\nresearch using deep learning methods based on data from the Hubble Space\nTelescope. We built a Local Celestial Image Dataset and designed a deep\nlearning model named HR-CelestialNet for classifying images of the galaxy and\nNSC. HR-CelestialNet achieved an accuracy of 89.09% on the testing set,\noutperforming models such as AlexNet, VGGNet and ResNet, while demonstrating\nfaster recognition speeds. Furthermore, we investigated the factors influencing\nCSST image quality and evaluated the generalization ability of HR-CelestialNet\non the blurry image dataset, demonstrating its robustness to low image quality.\nThe proposed method can enable real-time identification of celestial images\nduring CSST survey mission.",
        "date": "2023-12-08T10:27:40+00:00",
        "link": "http://arxiv.org/pdf/2312.04948v1"
    },
    {
        "title": "Benchmarking and Analysis of Unsupervised Object Segmentation from Real-world Single Images",
        "authors": [
            "Yafei Yang",
            "Bo Yang"
        ],
        "abstract": "In this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically\ninvestigate the effectiveness of existing unsupervised models on challenging\nreal-world images. We first introduce seven complexity factors to\nquantitatively measure the distributions of background and foreground object\nbiases in appearance and geometry for datasets with human annotations. With the\naid of these factors, we empirically find that, not surprisingly, existing\nunsupervised models fail to segment generic objects in real-world images,\nalthough they can easily achieve excellent performance on numerous simple\nsynthetic datasets, due to the vast gap in objectness biases between synthetic\nand real images. By conducting extensive experiments on multiple groups of\nablated real-world datasets, we ultimately find that the key factors underlying\nthe failure of existing unsupervised models on real-world images are the\nchallenging distributions of background and foreground object biases in\nappearance and geometry. Because of this, the inductive biases introduced in\nexisting unsupervised models can hardly capture the diverse object\ndistributions. Our research results suggest that future work should exploit\nmore explicit objectness biases in the network design.",
        "date": "2023-12-08T10:25:59+00:00",
        "link": "http://arxiv.org/pdf/2312.04947v1"
    },
    {
        "title": "The ICL Consistency Test",
        "authors": [
            "Lucas Weber",
            "Elia Bruni",
            "Dieuwke Hupkes"
        ],
        "abstract": "Just like the previous generation of task-tuned models, large language models\n(LLMs) that are adapted to tasks via prompt-based methods like\nin-context-learning (ICL) perform well in some setups but not in others. This\nlack of consistency in prompt-based learning hints at a lack of robust\ngeneralisation. We here introduce the ICL consistency test -- a contribution to\nthe GenBench collaborative benchmark task (CBT) -- which evaluates how\nconsistent a model makes predictions across many different setups while using\nthe same data. The test is based on different established natural language\ninference tasks. We provide preprocessed data constituting 96 different\n'setups' and a metric that estimates model consistency across these setups. The\nmetric is provided on a fine-grained level to understand what properties of a\nsetup render predictions unstable and on an aggregated level to compare overall\nmodel consistency. We conduct an empirical analysis of eight state-of-the-art\nmodels, and our consistency metric reveals how all tested LLMs lack robust\ngeneralisation.",
        "date": "2023-12-08T10:22:43+00:00",
        "link": "http://arxiv.org/pdf/2312.04945v1"
    },
    {
        "title": "Canaries and Whistles: Resilient Drone Communication Networks with (or without) Deep Reinforcement Learning",
        "authors": [
            "Chris Hicks",
            "Vasilios Mavroudis",
            "Myles Foley",
            "Thomas Davies",
            "Kate Highnam",
            "Tim Watson"
        ],
        "abstract": "Communication networks able to withstand hostile environments are critically\nimportant for disaster relief operations. In this paper, we consider a\nchallenging scenario where drones have been compromised in the supply chain,\nduring their manufacture, and harbour malicious software capable of\nwide-ranging and infectious disruption. We investigate multi-agent deep\nreinforcement learning as a tool for learning defensive strategies that\nmaximise communications bandwidth despite continual adversarial interference.\nUsing a public challenge for learning network resilience strategies, we propose\na state-of-the-art expert technique and study its superiority over deep\nreinforcement learning agents. Correspondingly, we identify three specific\nmethods for improving the performance of our learning-based agents: (1)\nensuring each observation contains the necessary information, (2) using expert\nagents to provide a curriculum for learning, and (3) paying close attention to\nreward. We apply our methods and present a new mixed strategy enabling expert\nand learning-based agents to work together and improve on all prior results.",
        "date": "2023-12-08T10:13:44+00:00",
        "link": "http://arxiv.org/pdf/2312.04940v1"
    },
    {
        "title": "Zoology: Measuring and Improving Recall in Efficient Language Models",
        "authors": [
            "Simran Arora",
            "Sabri Eyuboglu",
            "Aman Timalsina",
            "Isys Johnson",
            "Michael Poli",
            "James Zou",
            "Atri Rudra",
            "Christopher Ré"
        ],
        "abstract": "Attention-free language models that combine gating and convolutions are\ngrowing in popularity due to their efficiency and increasingly competitive\nperformance. To better understand these architectures, we pretrain a suite of\n17 attention and \"gated-convolution\" language models, finding that SoTA\ngated-convolution architectures still underperform attention by up to 2.1\nperplexity points on the Pile. In fine-grained analysis, we find 82% of the gap\nis explained by each model's ability to recall information that is previously\nmentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it\nmeans no\" $\\rightarrow$ \"??\". On this task, termed \"associative recall\", we\nfind that attention outperforms gated-convolutions by a large margin: a 70M\nparameter attention model outperforms a 1.4 billion parameter gated-convolution\nmodel on associative recall. This is surprising because prior work shows gated\nconvolutions can perfectly solve synthetic tests for AR capability. To close\nthe gap between synthetics and real language, we develop a new formalization of\nthe task called multi-query associative recall (MQAR) that better reflects\nactual language. We perform an empirical and theoretical study of MQAR that\nelucidates differences in the parameter-efficiency of attention and\ngated-convolution recall. Informed by our analysis, we evaluate simple\nconvolution-attention hybrids and show that hybrids with input-dependent sparse\nattention patterns can close 97.4% of the gap to attention, while maintaining\nsub-quadratic scaling. Our code is accessible at:\nhttps://github.com/HazyResearch/zoology.",
        "date": "2023-12-08T09:44:25+00:00",
        "link": "http://arxiv.org/pdf/2312.04927v1"
    },
    {
        "title": "Accelerating Convolutional Neural Network Pruning via Spatial Aura Entropy",
        "authors": [
            "Bogdan Musat",
            "Razvan Andonie"
        ],
        "abstract": "In recent years, pruning has emerged as a popular technique to reduce the\ncomputational complexity and memory footprint of Convolutional Neural Network\n(CNN) models. Mutual Information (MI) has been widely used as a criterion for\nidentifying unimportant filters to prune. However, existing methods for MI\ncomputation suffer from high computational cost and sensitivity to noise,\nleading to suboptimal pruning performance. We propose a novel method to improve\nMI computation for CNN pruning, using the spatial aura entropy. The spatial\naura entropy is useful for evaluating the heterogeneity in the distribution of\nthe neural activations over a neighborhood, providing information about local\nfeatures. Our method effectively improves the MI computation for CNN pruning,\nleading to more robust and efficient pruning. Experimental results on the\nCIFAR-10 benchmark dataset demonstrate the superiority of our approach in terms\nof pruning performance and computational efficiency.",
        "date": "2023-12-08T09:43:49+00:00",
        "link": "http://arxiv.org/pdf/2312.04926v1"
    },
    {
        "title": "Pruning Convolutional Filters via Reinforcement Learning with Entropy Minimization",
        "authors": [
            "Bogdan Musat",
            "Razvan Andonie"
        ],
        "abstract": "Structural pruning has become an integral part of neural network\noptimization, used to achieve architectural configurations which can be\ndeployed and run more efficiently on embedded devices. Previous results showed\nthat pruning is possible with minimum performance loss by utilizing a\nreinforcement learning agent which makes decisions about the sparsity level of\neach neural layer by maximizing as a reward the accuracy of the network. We\nintroduce a novel information-theoretic reward function which minimizes the\nspatial entropy of convolutional activations. This minimization ultimately acts\nas a proxy for maintaining accuracy, although these two criteria are not\nrelated in any way. Our method shows that there is another possibility to\npreserve accuracy without the need to directly optimize it in the agent's\nreward function. In our experiments, we were able to reduce the total number of\nFLOPS of multiple popular neural network architectures by 5-10x, incurring\nminimal or no performance drop and being on par with the solution found by\nmaximizing the accuracy.",
        "date": "2023-12-08T09:34:57+00:00",
        "link": "http://arxiv.org/pdf/2312.04918v1"
    },
    {
        "title": "Operationalizing Assurance Cases for Data Scientists: A Showcase of Concepts and Tooling in the Context of Test Data Quality for Machine Learning",
        "authors": [
            "Lisa Jöckel",
            "Michael Kläs",
            "Janek Groß",
            "Pascal Gerber",
            "Markus Scholz",
            "Jonathan Eberle",
            "Marc Teschner",
            "Daniel Seifert",
            "Richard Hawkins",
            "John Molloy",
            "Jens Ottnad"
        ],
        "abstract": "Assurance Cases (ACs) are an established approach in safety engineering to\nargue quality claims in a structured way. In the context of quality assurance\nfor Machine Learning (ML)-based software components, ACs are also being\ndiscussed and appear promising. Tools for operationalizing ACs do exist, yet\nmainly focus on supporting safety engineers on the system level. However,\nassuring the quality of an ML component within the system is commonly the\nresponsibility of data scientists, who are usually less familiar with these\ntools. To address this gap, we propose a framework to support the\noperationalization of ACs for ML components based on technologies that data\nscientists use on a daily basis: Python and Jupyter Notebook. Our aim is to\nmake the process of creating ML-related evidence in ACs more effective. Results\nfrom the application of the framework, documented through notebooks, can be\nintegrated into existing AC tools. We illustrate the application of the\nframework on an example excerpt concerned with the quality of the test data.",
        "date": "2023-12-08T09:34:46+00:00",
        "link": "http://arxiv.org/pdf/2312.04917v1"
    },
    {
        "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
        "authors": [
            "Yanxi Chen",
            "Xuchen Pan",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "abstract": "We present EE-LLM, a framework for large-scale training and inference of\nearly-exit large language models (LLMs). While recent works have shown\npreliminary evidence for the efficacy of early exiting in accelerating LLM\ninference, EE-LLM makes a foundational step towards scaling up early-exit LLMs\nby supporting their training and inference with massive 3D parallelism. Built\nupon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and\nperformance optimizations tailored to early exiting, including a lightweight\nmethod that facilitates backpropagation for the early-exit training objective\nwith pipeline parallelism, techniques of leveraging idle resources in the\noriginal pipeline schedule for computation related to early-exit layers, and\ntwo approaches of early-exit inference that are compatible with KV caching for\nautoregressive generation. Our analytical and empirical study shows that EE-LLM\nachieves great training efficiency with negligible computational overhead\ncompared to standard LLM training, as well as outstanding inference speedup\nwithout compromising output quality. To facilitate further research and\nadoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.",
        "date": "2023-12-08T09:31:50+00:00",
        "link": "http://arxiv.org/pdf/2312.04916v1"
    },
    {
        "title": "SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation",
        "authors": [
            "Bangyan He",
            "Xiaojun Jia",
            "Siyuan Liang",
            "Tianrui Lou",
            "Yang Liu",
            "Xiaochun Cao"
        ],
        "abstract": "Current Visual-Language Pre-training (VLP) models are vulnerable to\nadversarial examples. These adversarial examples present substantial security\nrisks to VLP models, as they can leverage inherent weaknesses in the models,\nresulting in incorrect predictions. In contrast to white-box adversarial\nattacks, transfer attacks (where the adversary crafts adversarial examples on a\nwhite-box model to fool another black-box model) are more reflective of\nreal-world scenarios, thus making them more meaningful for research. By\nsummarizing and analyzing existing research, we identified two factors that can\ninfluence the efficacy of transfer attacks on VLP models: inter-modal\ninteraction and data diversity. Based on these insights, we propose a\nself-augment-based transfer attack method, termed SA-Attack. Specifically,\nduring the generation of adversarial images and adversarial texts, we apply\ndifferent data augmentation methods to the image modality and text modality,\nrespectively, with the aim of improving the adversarial transferability of the\ngenerated adversarial images and texts. Experiments conducted on the FLickr30K\nand COCO datasets have validated the effectiveness of our method. Our code will\nbe available after this paper is accepted.",
        "date": "2023-12-08T09:08:50+00:00",
        "link": "http://arxiv.org/pdf/2312.04913v1"
    },
    {
        "title": "Collinear datasets augmentation using Procrustes validation sets",
        "authors": [
            "Sergey Kucheryavskiy",
            "Sergei Zhilin"
        ],
        "abstract": "In this paper, we propose a new method for the augmentation of numeric and\nmixed datasets. The method generates additional data points by utilizing\ncross-validation resampling and latent variable modeling. It is particularly\nefficient for datasets with moderate to high degrees of collinearity, as it\ndirectly utilizes this property for generation. The method is simple, fast, and\nhas very few parameters, which, as shown in the paper, do not require specific\ntuning. It has been tested on several real datasets; here, we report detailed\nresults for two cases, prediction of protein in minced meat based on near\ninfrared spectra (fully numeric data with high degree of collinearity) and\ndiscrimination of patients referred for coronary angiography (mixed data, with\nboth numeric and categorical variables, and moderate collinearity). In both\ncases, artificial neural networks were employed for developing the regression\nand the discrimination models. The results show a clear improvement in the\nperformance of the models; thus for the prediction of meat protein, fitting the\nmodel to the augmented data resulted in a reduction in the root mean squared\nerror computed for the independent test set by 1.5 to 3 times.",
        "date": "2023-12-08T09:07:11+00:00",
        "link": "http://arxiv.org/pdf/2312.04911v1"
    },
    {
        "title": "Two-Timescale Q-Learning with Function Approximation in Zero-Sum Stochastic Games",
        "authors": [
            "Zaiwei Chen",
            "Kaiqing Zhang",
            "Eric Mazumdar",
            "Asuman Ozdaglar",
            "Adam Wierman"
        ],
        "abstract": "We consider two-player zero-sum stochastic games and propose a two-timescale\n$Q$-learning algorithm with function approximation that is payoff-based,\nconvergent, rational, and symmetric between the two players. In two-timescale\n$Q$-learning, the fast-timescale iterates are updated in spirit to the\nstochastic gradient descent and the slow-timescale iterates (which we use to\ncompute the policies) are updated by taking a convex combination between its\nprevious iterate and the latest fast-timescale iterate. Introducing the slow\ntimescale as well as its update equation marks as our main algorithmic novelty.\nIn the special case of linear function approximation, we establish, to the best\nof our knowledge, the first last-iterate finite-sample bound for payoff-based\nindependent learning dynamics of these types. The result implies a polynomial\nsample complexity to find a Nash equilibrium in such stochastic games.\n  To establish the results, we model our proposed algorithm as a two-timescale\nstochastic approximation and derive the finite-sample bound through a\nLyapunov-based approach. The key novelty lies in constructing a valid Lyapunov\nfunction to capture the evolution of the slow-timescale iterates. Specifically,\nthrough a change of variable, we show that the update equation of the\nslow-timescale iterates resembles the classical smoothed best-response\ndynamics, where the regularized Nash gap serves as a valid Lyapunov function.\nThis insight enables us to construct a valid Lyapunov function via a\ngeneralized variant of the Moreau envelope of the regularized Nash gap. The\nconstruction of our Lyapunov function might be of broad independent interest in\nstudying the behavior of stochastic approximation algorithms.",
        "date": "2023-12-08T08:39:36+00:00",
        "link": "http://arxiv.org/pdf/2312.04905v1"
    },
    {
        "title": "KwaiAgents: Generalized Information-seeking Agent System with Large Language Models",
        "authors": [
            "Haojie Pan",
            "Zepeng Zhai",
            "Hao Yuan",
            "Yaojia Lv",
            "Ruiji Fu",
            "Ming Liu",
            "Zhongyuan Wang",
            "Bing Qin"
        ],
        "abstract": "Driven by curiosity, humans have continually sought to explore and understand\nthe world around them, leading to the invention of various tools to satiate\nthis inquisitiveness. Despite not having the capacity to process and memorize\nvast amounts of information in their brains, humans excel in critical thinking,\nplanning, reflection, and harnessing available tools to interact with and\ninterpret the world, enabling them to find answers efficiently. The recent\nadvancements in large language models (LLMs) suggest that machines might also\npossess the aforementioned human-like capabilities, allowing them to exhibit\npowerful abilities even with a constrained parameter count. In this paper, we\nintroduce KwaiAgents, a generalized information-seeking agent system based on\nLLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its\ncognitive core, which is capable of understanding a user's query, behavior\nguidelines, and referencing external documents. The agent can also update and\nretrieve information from its internal memory, plan and execute actions using a\ntime-aware search-browse toolkit, and ultimately provide a comprehensive\nresponse. We further investigate the system's performance when powered by LLMs\nless advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,\ndesigned to ensure even an open-sourced 7B or 13B model performs well among\nmany agent systems. We exploit both benchmark and human evaluations to\nsystematically validate these capabilities. Extensive experiments show the\nsuperiority of our agent system compared to other autonomous agents and\nhighlight the enhanced generalized agent-abilities of our fine-tuned LLMs.",
        "date": "2023-12-08T08:11:11+00:00",
        "link": "http://arxiv.org/pdf/2312.04889v1"
    },
    {
        "title": "Understanding Community Bias Amplification in Graph Representation Learning",
        "authors": [
            "Shengzhong Zhang",
            "Wenjie Yang",
            "Yimin Zhang",
            "Hongwei Zhang",
            "Divin Yan",
            "Zengfeng Huang"
        ],
        "abstract": "In this work, we discover a phenomenon of community bias amplification in\ngraph representation learning, which refers to the exacerbation of performance\nbias between different classes by graph representation learning. We conduct an\nin-depth theoretical study of this phenomenon from a novel spectral\nperspective. Our analysis suggests that structural bias between communities\nresults in varying local convergence speeds for node embeddings. This\nphenomenon leads to bias amplification in the classification results of\ndownstream tasks. Based on the theoretical insights, we propose random graph\ncoarsening, which is proved to be effective in dealing with the above issue.\nFinally, we propose a novel graph contrastive learning model called Random\nGraph Coarsening Contrastive Learning (RGCCL), which utilizes random coarsening\nas data augmentation and mitigates community bias by contrasting the coarsened\ngraph with the original graph. Extensive experiments on various datasets\ndemonstrate the advantage of our method when dealing with community bias\namplification.",
        "date": "2023-12-08T07:43:05+00:00",
        "link": "http://arxiv.org/pdf/2312.04883v1"
    },
    {
        "title": "HC-Ref: Hierarchical Constrained Refinement for Robust Adversarial Training of GNNs",
        "authors": [
            "Xiaobing Pei",
            "Haoran Yang",
            "Gang Shen"
        ],
        "abstract": "Recent studies have shown that attackers can catastrophically reduce the\nperformance of GNNs by maliciously modifying the graph structure or node\nfeatures on the graph. Adversarial training, which has been shown to be one of\nthe most effective defense mechanisms against adversarial attacks in computer\nvision, holds great promise for enhancing the robustness of GNNs. There is\nlimited research on defending against attacks by performing adversarial\ntraining on graphs, and it is crucial to delve deeper into this approach to\noptimize its effectiveness. Therefore, based on robust adversarial training on\ngraphs, we propose a hierarchical constraint refinement framework (HC-Ref) that\nenhances the anti-perturbation capabilities of GNNs and downstream classifiers\nseparately, ultimately leading to improved robustness. We propose corresponding\nadversarial regularization terms that are conducive to adaptively narrowing the\ndomain gap between the normal part and the perturbation part according to the\ncharacteristics of different layers, promoting the smoothness of the predicted\ndistribution of both parts. Moreover, existing research on graph robust\nadversarial training primarily concentrates on training from the standpoint of\nnode feature perturbations and seldom takes into account alterations in the\ngraph structure. This limitation makes it challenging to prevent attacks based\non topological changes in the graph. This paper generates adversarial examples\nby utilizing graph structure perturbations, offering an effective approach to\ndefend against attack methods that are based on topological changes. Extensive\nexperiments on two real-world graph benchmarks show that HC-Ref successfully\nresists various attacks and has better node classification performance compared\nto several baseline methods.",
        "date": "2023-12-08T07:32:56+00:00",
        "link": "http://arxiv.org/pdf/2312.04879v1"
    },
    {
        "title": "StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning",
        "authors": [
            "Shengzhong Zhang",
            "Wenjie Yang",
            "Xinyuan Cao",
            "Hongwei Zhang",
            "Zengfeng Huang"
        ],
        "abstract": "Graph contrastive learning (GCL) has become a powerful tool for learning\ngraph data, but its scalability remains a significant challenge. In this work,\nwe propose a simple yet effective training framework called Structural\nCompression (StructComp) to address this issue. Inspired by a sparse low-rank\napproximation on the diffusion matrix, StructComp trains the encoder with the\ncompressed nodes. This allows the encoder not to perform any message passing\nduring the training stage, and significantly reduces the number of sample pairs\nin the contrastive loss. We theoretically prove that the original GCL loss can\nbe approximated with the contrastive loss computed by StructComp. Moreover,\nStructComp can be regarded as an additional regularization term for GCL models,\nresulting in a more robust encoder. Empirical studies on seven benchmark\ndatasets show that StructComp greatly reduces the time and memory consumption\nwhile improving model performance compared to the vanilla GCL models and\nscalable training methods.",
        "date": "2023-12-08T06:46:18+00:00",
        "link": "http://arxiv.org/pdf/2312.04865v1"
    },
    {
        "title": "Damage GAN: A Generative Model for Imbalanced Data",
        "authors": [
            "Ali Anaissi",
            "Yuanzhe Jia",
            "Ali Braytee",
            "Mohamad Naji",
            "Widad Alyassine"
        ],
        "abstract": "This study delves into the application of Generative Adversarial Networks\n(GANs) within the context of imbalanced datasets. Our primary aim is to enhance\nthe performance and stability of GANs in such datasets. In pursuit of this\nobjective, we introduce a novel network architecture known as Damage GAN,\nbuilding upon the ContraD GAN framework which seamlessly integrates GANs and\ncontrastive learning. Through the utilization of contrastive learning, the\ndiscriminator is trained to develop an unsupervised representation capable of\ndistinguishing all provided samples. Our approach draws inspiration from the\nstraightforward framework for contrastive learning of visual representations\n(SimCLR), leading to the formulation of a distinctive loss function. We also\nexplore the implementation of self-damaging contrastive learning (SDCLR) to\nfurther enhance the optimization of the ContraD GAN model. Comparative\nevaluations against baseline models including the deep convolutional GAN\n(DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed\nmodel, Damage GAN, in terms of generated image distribution, model stability,\nand image quality when applied to imbalanced datasets.",
        "date": "2023-12-08T06:36:33+00:00",
        "link": "http://arxiv.org/pdf/2312.04862v1"
    },
    {
        "title": "Analysis on Effects of Fault Elements in Memristive Neuromorphic Systems",
        "authors": [
            "Hyun-Jong Lee",
            "Jae-Han Lim"
        ],
        "abstract": "Nowadays, neuromorphic systems based on Spiking Neural Networks (SNNs)\nattract attentions of many researchers. There are many studies to improve\nperformances of neuromorphic systems. These studies have been showing\nsatisfactory results. To magnify performances of neuromorphic systems,\ndeveloping actual neuromorphic systems is essential. For developing them,\nmemristors play key role due to their useful characteristics. Although\nmemristors are essential for actual neuromorphic systems, they are vulnerable\nto faults. However, there are few studies analyzing effects of fault elements\nin neuromorphic systems using memristors. To solve this problem, we analyze\nperformance of a memristive neuromorphic system with fault elements changing\nfault ratios, types, and positions. We choose neurons and synapses to inject\nfaults. We inject two types of faults to synapses: SA0 and SA1 faults. The\nfault synapses appear in random and important positions. Through our analysis,\nwe discover the following four interesting points. First, memristive\ncharacteristics increase vulnerability of neuromorphic systems to fault\nelements. Second, fault neuron ratios reducing performance sharply exist.\nThird, performance degradation by fault synapses depends on fault types.\nFinally, SA1 fault synapses improve performance when they appear in important\npositions.",
        "date": "2023-12-08T05:37:14+00:00",
        "link": "http://arxiv.org/pdf/2312.04840v1"
    },
    {
        "title": "Not All Negatives AreWorth Attending to: Meta-Bootstrapping Negative Sampling Framework for Link Prediction",
        "authors": [
            "Yakun Wang",
            "Binbin Hu",
            "Shuo Yang",
            "Meiqi Zhu",
            "Zhiqiang Zhang",
            "Qiyang Zhang",
            "Jun Zhou",
            "Guo Ye",
            "Huimei He"
        ],
        "abstract": "The rapid development of graph neural networks (GNNs) encourages the rising\nof link prediction, achieving promising performance with various applications.\nUnfortunately, through a comprehensive analysis, we surprisingly find that\ncurrent link predictors with dynamic negative samplers (DNSs) suffer from the\nmigration phenomenon between \"easy\" and \"hard\" samples, which goes against the\npreference of DNS of choosing \"hard\" negatives, thus severely hindering\ncapability. Towards this end, we propose the MeBNS framework, serving as a\ngeneral plugin that can potentially improve current negative sampling based\nlink predictors. In particular, we elaborately devise a Meta-learning Supported\nTeacher-student GNN (MST-GNN) that is not only built upon teacher-student\narchitecture for alleviating the migration between \"easy\" and \"hard\" samples\nbut also equipped with a meta learning based sample re-weighting module for\nhelping the student GNN distinguish \"hard\" samples in a fine-grained manner. To\neffectively guide the learning of MST-GNN, we prepare a Structure enhanced\nTraining Data Generator (STD-Generator) and an Uncertainty based Meta Data\nCollector (UMD-Collector) for supporting the teacher and student GNN,\nrespectively. Extensive experiments show that the MeBNS achieves remarkable\nperformance across six link prediction benchmark datasets.",
        "date": "2023-12-08T03:05:42+00:00",
        "link": "http://arxiv.org/pdf/2312.04815v1"
    },
    {
        "title": "Joint User Association, Interference Cancellation and Power Control for Multi-IRS Assisted UAV Communications",
        "authors": [
            "Zhaolong Ning",
            "Hao Hu",
            "Xiaojie Wang",
            "Qingqing Wu",
            "Chau Yuen",
            "F. Richard Yu",
            "Yan Zhang"
        ],
        "abstract": "Intelligent reflecting surface (IRS)-assisted unmanned aerial vehicle (UAV)\ncommunications are expected to alleviate the load of ground base stations in a\ncost-effective way. Existing studies mainly focus on the deployment and\nresource allocation of a single IRS instead of multiple IRSs, whereas it is\nextremely challenging for joint multi-IRS multi-user association in UAV\ncommunications with constrained reflecting resources and dynamic scenarios. To\naddress the aforementioned challenges, we propose a new optimization algorithm\nfor joint IRS-user association, trajectory optimization of UAVs, successive\ninterference cancellation (SIC) decoding order scheduling and power allocation\nto maximize system energy efficiency. We first propose an inverse soft-Q\nlearning-based algorithm to optimize multi-IRS multi-user association. Then,\nSCA and Dinkelbach-based algorithm are leveraged to optimize UAV trajectory\nfollowed by the optimization of SIC decoding order scheduling and power\nallocation. Finally, theoretical analysis and performance results show\nsignificant advantages of the designed algorithm in convergence rate and energy\nefficiency.",
        "date": "2023-12-08T01:57:10+00:00",
        "link": "http://arxiv.org/pdf/2312.04786v1"
    },
    {
        "title": "Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs",
        "authors": [
            "Zhuo Zhang",
            "Guangyu Shen",
            "Guanhong Tao",
            "Siyuan Cheng",
            "Xiangyu Zhang"
        ],
        "abstract": "Large Language Models (LLMs) are now widely used in various applications,\nmaking it crucial to align their ethical standards with human values. However,\nrecent jail-breaking methods demonstrate that this alignment can be undermined\nusing carefully constructed prompts. In our study, we reveal a new threat to\nLLM alignment when a bad actor has access to the model's output logits, a\ncommon feature in both open-source LLMs and many commercial LLM APIs (e.g.,\ncertain GPT models). It does not rely on crafting specific prompts. Instead, it\nexploits the fact that even when an LLM rejects a toxic request, a harmful\nresponse often hides deep in the output logits. By forcefully selecting\nlower-ranked output tokens during the auto-regressive generation process at a\nfew critical output positions, we can compel the model to reveal these hidden\nresponses. We term this process model interrogation. This approach differs from\nand outperforms jail-breaking methods, achieving 92% effectiveness compared to\n62%, and is 10 to 20 times faster. The harmful content uncovered through our\nmethod is more relevant, complete, and clear. Additionally, it can complement\njail-breaking strategies, with which results in further boosting attack\nperformance. Our findings indicate that interrogation can extract toxic\nknowledge even from models specifically designed for coding tasks.",
        "date": "2023-12-08T01:41:36+00:00",
        "link": "http://arxiv.org/pdf/2312.04782v1"
    },
    {
        "title": "Image Synthesis-based Late Stage Cancer Augmentation and Semi-Supervised Segmentation for MRI Rectal Cancer Staging",
        "authors": [
            "Saeko Sasuga",
            "Akira Kudo",
            "Yoshiro Kitamura",
            "Satoshi Iizuka",
            "Edgar Simo-Serra",
            "Atsushi Hamabe",
            "Masayuki Ishii",
            "Ichiro Takemasa"
        ],
        "abstract": "Rectal cancer is one of the most common diseases and a major cause of\nmortality. For deciding rectal cancer treatment plans, T-staging is important.\nHowever, evaluating the index from preoperative MRI images requires high\nradiologists' skill and experience. Therefore, the aim of this study is to\nsegment the mesorectum, rectum, and rectal cancer region so that the system can\npredict T-stage from segmentation results. Generally, shortage of large and\ndiverse dataset and high quality annotation are known to be the bottlenecks in\ncomputer aided diagnostics development. Regarding rectal cancer, advanced\ncancer images are very rare, and per-pixel annotation requires high\nradiologists' skill and time. Therefore, it is not feasible to collect\ncomprehensive disease patterns in a training dataset. To tackle this, we\npropose two kinds of approaches of image synthesis-based late stage cancer\naugmentation and semi-supervised learning which is designed for T-stage\nprediction. In the image synthesis data augmentation approach, we generated\nadvanced cancer images from labels. The real cancer labels were deformed to\nresemble advanced cancer labels by artificial cancer progress simulation. Next,\nwe introduce a T-staging loss which enables us to train segmentation models\nfrom per-image T-stage labels. The loss works to keep inclusion/invasion\nrelationships between rectum and cancer region consistent to the ground truth\nT-stage. The verification tests show that the proposed method obtains the best\nsensitivity (0.76) and specificity (0.80) in distinguishing between over T3\nstage and underT2. In the ablation studies, our semi-supervised learning\napproach with the T-staging loss improved specificity by 0.13. Adding the image\nsynthesis-based data augmentation improved the DICE score of invasion cancer\narea by 0.08 from baseline.",
        "date": "2023-12-08T01:36:24+00:00",
        "link": "http://arxiv.org/pdf/2312.04779v1"
    },
    {
        "title": "Remembering to Be Fair: On Non-Markovian Fairness in Sequential DecisionMaking (Preliminary Report)",
        "authors": [
            "Parand A. Alamdari",
            "Toryn Q. Klassen",
            "Elliot Creager",
            "Sheila A. McIlraith"
        ],
        "abstract": "Fair decision making has largely been studied with respect to a single\ndecision. In this paper we investigate the notion of fairness in the context of\nsequential decision making where multiple stakeholders can be affected by the\noutcomes of decisions, and where decision making may be informed by additional\nconstraints and criteria beyond the requirement of fairness. In this setting,\nwe observe that fairness often depends on the history of the sequential\ndecision-making process and not just on the current state. To advance our\nunderstanding of this class of fairness problems, we define the notion of\nnon-Markovian fairness in the context of sequential decision making. We\nidentify properties of non-Markovian fairness, including notions of long-term,\nanytime, periodic, and bounded fairness. We further explore the interplay\nbetween non-Markovian fairness and memory, and how this can support\nconstruction of fair policies in sequential decision-making settings.",
        "date": "2023-12-08T01:04:36+00:00",
        "link": "http://arxiv.org/pdf/2312.04772v1"
    },
    {
        "title": "The Graph Lottery Ticket Hypothesis: Finding Sparse, Informative Graph Structure",
        "authors": [
            "Anton Tsitsulin",
            "Bryan Perozzi"
        ],
        "abstract": "Graph learning methods help utilize implicit relationships among data items,\nthereby reducing training label requirements and improving task performance.\nHowever, determining the optimal graph structure for a particular learning task\nremains a challenging research problem.\n  In this work, we introduce the Graph Lottery Ticket (GLT) Hypothesis - that\nthere is an extremely sparse backbone for every graph, and that graph learning\nalgorithms attain comparable performance when trained on that subgraph as on\nthe full graph. We identify and systematically study 8 key metrics of interest\nthat directly influence the performance of graph learning algorithms.\nSubsequently, we define the notion of a \"winning ticket\" for graph structure -\nan extremely sparse subset of edges that can deliver a robust approximation of\nthe entire graph's performance. We propose a straightforward and efficient\nalgorithm for finding these GLTs in arbitrary graphs. Empirically, we observe\nthat performance of different graph learning algorithms can be matched or even\nexceeded on graphs with the average degree as low as 5.",
        "date": "2023-12-08T00:24:44+00:00",
        "link": "http://arxiv.org/pdf/2312.04762v1"
    },
    {
        "title": "Physics-Informed Convolutional Autoencoder for Cyber Anomaly Detection in Power Distribution Grids",
        "authors": [
            "Mehdi Jabbari Zideh",
            "Sarika Khushalani Solanki"
        ],
        "abstract": "The growing trend toward the modernization of power distribution systems has\nfacilitated the installation of advanced measurement units and promotion of the\ncyber communication systems. However, these infrastructures are still prone to\nstealth cyber attacks. The existing data-driven anomaly detection methods\nsuffer from a lack of knowledge about the system's physics, lack of\ninterpretability, and scalability issues hindering their practical applications\nin real-world scenarios. To address these concerns, physics-informed neural\nnetworks (PINNs) were introduced. This paper proposes a multivariate\nphysics-informed convolutional autoencoder (PIConvAE) to detect stealthy\ncyber-attacks in power distribution grids. The proposed model integrates the\nphysical principles into the loss function of the neural network by applying\nKirchhoff's law. Simulations are performed on the modified IEEE 13-bus and\n123-bus systems using OpenDSS software to validate the efficacy of the proposed\nmodel for stealth attacks. The numerical results prove the superior performance\nof the proposed PIConvAE in three aspects: a) it provides more accurate results\ncompared to the data-driven ConvAE model, b) it requires less training time to\nconverge c) the model excels in effectively detecting a wide range of attack\nmagnitudes making it powerful in detecting stealth attacks.",
        "date": "2023-12-08T00:05:13+00:00",
        "link": "http://arxiv.org/pdf/2312.04758v1"
    },
    {
        "title": "Induced Generative Adversarial Particle Transformers",
        "authors": [
            "Anni Li",
            "Venkat Krishnamohan",
            "Raghav Kansal",
            "Rounak Sen",
            "Steven Tsan",
            "Zhaoyu Zhang",
            "Javier Duarte"
        ],
        "abstract": "In high energy physics (HEP), machine learning methods have emerged as an\neffective way to accurately simulate particle collisions at the Large Hadron\nCollider (LHC). The message-passing generative adversarial network (MPGAN) was\nthe first model to simulate collisions as point, or ``particle'', clouds, with\nstate-of-the-art results, but suffered from quadratic time complexity.\nRecently, generative adversarial particle transformers (GAPTs) were introduced\nto address this drawback; however, results did not surpass MPGAN. We introduce\ninduced GAPT (iGAPT) which, by integrating ``induced particle-attention\nblocks'' and conditioning on global jet attributes, not only offers linear time\ncomplexity but is also able to capture intricate jet substructure, surpassing\nMPGAN in many metrics. Our experiments demonstrate the potential of iGAPT to\nsimulate complex HEP data accurately and efficiently.",
        "date": "2023-12-08T00:02:16+00:00",
        "link": "http://arxiv.org/pdf/2312.04757v1"
    },
    {
        "title": "SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control",
        "authors": [
            "Jaskirat Singh",
            "Jianming Zhang",
            "Qing Liu",
            "Cameron Smith",
            "Zhe Lin",
            "Liang Zheng"
        ],
        "abstract": "The field of generative image inpainting and object insertion has made\nsignificant progress with the recent advent of latent diffusion models.\nUtilizing a precise object mask can greatly enhance these applications.\nHowever, due to the challenges users encounter in creating high-fidelity masks,\nthere is a tendency for these methods to rely on more coarse masks (e.g.,\nbounding box) for these applications. This results in limited control and\ncompromised background content preservation. To overcome these limitations, we\nintroduce SmartMask, which allows any novice user to create detailed masks for\nprecise object insertion. Combined with a ControlNet-Inpaint model, our\nexperiments demonstrate that SmartMask achieves superior object insertion\nquality, preserving the background content more effectively than previous\nmethods. Notably, unlike prior works the proposed approach can also be used\neven without user-mask guidance, which allows it to perform mask-free object\ninsertion at diverse positions and scales. Furthermore, we find that when used\niteratively with a novel instruction-tuning based planning model, SmartMask can\nbe used to design detailed layouts from scratch. As compared with user-scribble\nbased layout design, we observe that SmartMask allows for better quality\noutputs with layout-to-image generation methods. Project page is available at\nhttps://smartmask-gen.github.io",
        "date": "2023-12-08T13:38:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05039v1"
    },
    {
        "title": "KBFormer: A Diffusion Model for Structured Entity Completion",
        "authors": [
            "Ouail Kitouni",
            "Niklas Nolte",
            "James Hensman",
            "Bhaskar Mitra"
        ],
        "abstract": "We develop a generative attention-based approach to modeling structured\nentities comprising different property types, such as numerical, categorical,\nstring, and composite. This approach handles such heterogeneous data through a\nmixed continuous-discrete diffusion process over the properties. Our flexible\nframework can model entities with arbitrary hierarchical properties, enabling\napplications to structured Knowledge Base (KB) entities and tabular data. Our\napproach obtains state-of-the-art performance on a majority of cases across 15\ndatasets. In addition, experiments with a device KB and a nuclear physics\ndataset demonstrate the model's ability to learn representations useful for\nentity completion in diverse settings. This has many downstream use cases,\nincluding modeling numerical properties with high accuracy - critical for\nscience applications, which also benefit from the model's inherent\nprobabilistic nature.",
        "date": "2023-12-08T18:59:14+00:00",
        "link": "http://arxiv.org/pdf/2312.05253v1"
    },
    {
        "title": "TaskMet: Task-Driven Metric Learning for Model Learning",
        "authors": [
            "Dishank Bansal",
            "Ricky T. Q. Chen",
            "Mustafa Mukadam",
            "Brandon Amos"
        ],
        "abstract": "Deep learning models are often deployed in downstream tasks that the training\nprocedure may not be aware of. For example, models solely trained to achieve\naccurate predictions may struggle to perform well on downstream tasks because\nseemingly small prediction errors may incur drastic task errors. The standard\nend-to-end learning approach is to make the task loss differentiable or to\nintroduce a differentiable surrogate that the model can be trained on. In these\nsettings, the task loss needs to be carefully balanced with the prediction loss\nbecause they may have conflicting objectives. We propose take the task loss\nsignal one level deeper than the parameters of the model and use it to learn\nthe parameters of the loss function the model is trained on, which can be done\nby learning a metric in the prediction space. This approach does not alter the\noptimal prediction model itself, but rather changes the model learning to\nemphasize the information important for the downstream task. This enables us to\nachieve the best of both worlds: a prediction model trained in the original\nprediction space while also being valuable for the desired downstream task. We\nvalidate our approach through experiments conducted in two main settings: 1)\ndecision-focused model learning scenarios involving portfolio optimization and\nbudget allocation, and 2) reinforcement learning in noisy environments with\ndistracting states. The source code to reproduce our experiments is available\nat https://github.com/facebookresearch/taskmet",
        "date": "2023-12-08T18:59:03+00:00",
        "link": "http://arxiv.org/pdf/2312.05250v1"
    },
    {
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
        "authors": [
            "Zhiting Hu",
            "Tianmin Shu"
        ],
        "abstract": "Despite their tremendous success in many applications, large language models\noften fall short of consistent reasoning and planning in various (language,\nembodied, and social) scenarios, due to inherent limitations in their\ninference, learning, and modeling capabilities. In this position paper, we\npresent a new perspective of machine reasoning, LAW, that connects the concepts\nof Language models, Agent models, and World models, for more robust and\nversatile reasoning capabilities. In particular, we propose that world and\nagent models are a better abstraction of reasoning, that introduces the crucial\nelements of deliberate human-like reasoning, including beliefs about the world\nand other agents, anticipation of consequences, goals/rewards, and strategic\nplanning. Crucially, language models in LAW serve as a backend to implement the\nsystem or its elements and hence provide the computational power and\nadaptability. We review the recent studies that have made relevant progress and\ndiscuss future research directions towards operationalizing the LAW framework.",
        "date": "2023-12-08T18:25:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05230v1"
    },
    {
        "title": "HALO: An Ontology for Representing Hallucinations in Generative Models",
        "authors": [
            "Navapat Nananukul",
            "Mayank Kejriwal"
        ],
        "abstract": "Recent progress in generative AI, including large language models (LLMs) like\nChatGPT, has opened up significant opportunities in fields ranging from natural\nlanguage processing to knowledge discovery and data mining. However, there is\nalso a growing awareness that the models can be prone to problems such as\nmaking information up or `hallucinations', and faulty reasoning on seemingly\nsimple problems. Because of the popularity of models like ChatGPT, both\nacademic scholars and citizen scientists have documented hallucinations of\nseveral different types and severity. Despite this body of work, a formal model\nfor describing and representing these hallucinations (with relevant meta-data)\nat a fine-grained level, is still lacking. In this paper, we address this gap\nby presenting the Hallucination Ontology or HALO, a formal, extensible ontology\nwritten in OWL that currently offers support for six different types of\nhallucinations known to arise in LLMs, along with support for provenance and\nexperimental metadata. We also collect and publish a dataset containing\nhallucinations that we inductively gathered across multiple independent Web\nsources, and show that HALO can be successfully used to model this dataset and\nanswer competency questions.",
        "date": "2023-12-08T17:57:20+00:00",
        "link": "http://arxiv.org/pdf/2312.05209v1"
    },
    {
        "title": "Conformal Prediction in Multi-User Settings: An Evaluation",
        "authors": [
            "Enrique Garcia-Ceja",
            "Luciano Garcia-Banuelos",
            "Nicolas Jourdan"
        ],
        "abstract": "Typically, machine learning models are trained and evaluated without making\nany distinction between users (e.g, using traditional hold-out and\ncross-validation). However, this produces inaccurate performance metrics\nestimates in multi-user settings. That is, situations where the data were\ncollected by multiple users with different characteristics (e.g., age, gender,\nheight, etc.) which is very common in user computer interaction and medical\napplications. For these types of scenarios model evaluation strategies that\nprovide better performance estimates have been proposed such as mixed,\nuser-independent, user-dependent, and user-adaptive models. Although those\nstrategies are better suited for multi-user systems, they are typically\nassessed with respect to performance metrics that capture the overall behavior\nof the models and do not provide any performance guarantees for individual\npredictions nor they provide any feedback about the predictions' uncertainty.\nIn order to overcome those limitations, in this work we evaluated the conformal\nprediction framework in several multi-user settings. Conformal prediction is a\nmodel agnostic method that provides confidence guarantees on the predictions,\nthus, increasing the trustworthiness and robustness of the models. We conducted\nextensive experiments using different evaluation strategies and found\nsignificant differences in terms of conformal performance measures. We also\nproposed several visualizations based on matrices, graphs, and charts that\ncapture different aspects of the resulting prediction sets.",
        "date": "2023-12-08T17:33:23+00:00",
        "link": "http://arxiv.org/pdf/2312.05195v1"
    },
    {
        "title": "AI Competitions and Benchmarks: Competition platforms",
        "authors": [
            "Andrey Ustyuzhanin",
            "Harald Carlens"
        ],
        "abstract": "The ecosystem of artificial intelligence competitions is a diverse and\nmultifaceted landscape, encompassing a variety of platforms that each host\nnumerous competitions annually, alongside a plethora of specialized websites\ndedicated to singular contests. These platforms adeptly manage the overarching\nadministrative responsibilities inherent in orchestrating competitions, thus\naffording organizers the liberty to allocate greater attention to other facets\nof their contests. Notably, these platforms exhibit considerable diversity in\ntheir operational functionalities, economic models, and community dynamics.\nThis chapter conducts an extensive review of the foremost services in this\nrealm and elucidates several alternative methodologies that facilitate the\nindependent hosting of such challenges. Keywords: competition platform,\nchallenge hosting services, comparison.",
        "date": "2023-12-08T17:16:50+00:00",
        "link": "http://arxiv.org/pdf/2312.05185v1"
    },
    {
        "title": "TENPLEX: Changing Resources of Deep Learning Jobs using Parallelizable Tensor Collections",
        "authors": [
            "Marcel Wagenländer",
            "Guo Li",
            "Bo Zhao",
            "Luo Mai",
            "Peter Pietzuch"
        ],
        "abstract": "Deep learning (DL) jobs use multi-dimensional parallelism, i.e they combine\ndata, model, and pipeline parallelism, to use large GPU clusters efficiently.\nThis couples jobs tightly to a set of GPU devices, but jobs may experience\nchanges to the device allocation: (i) resource elasticity during training adds\nor removes devices; (ii) hardware maintenance may require redeployment on\ndifferent devices; and (iii) device failures force jobs to run with fewer\ndevices. Current DL frameworks lack support for these scenarios, as they cannot\nchange the multi-dimensional parallelism of an already-running job in an\nefficient and model-independent way.\n  We describe Tenplex, a state management library for DL frameworks that\nenables jobs to change the GPU allocation and job parallelism at runtime.\nTenplex achieves this by externalizing the DL job state during training as a\nparallelizable tensor collection (PTC). When the GPU allocation for the DL job\nchanges, Tenplex uses the PTC to transform the DL job state: for the dataset\nstate, Tenplex repartitions it under data parallelism and exposes it to workers\nthrough a virtual file system; for the model state, Tenplex obtains it as\npartitioned checkpoints and transforms them to reflect the new parallelization\nconfiguration. For efficiency, these PTC transformations are executed in\nparallel with a minimum amount of data movement between devices and workers.\nOur experiments show that Tenplex enables DL jobs to support dynamic\nparallelization with low overhead.",
        "date": "2023-12-08T17:08:03+00:00",
        "link": "http://arxiv.org/pdf/2312.05181v1"
    },
    {
        "title": "DARLEI: Deep Accelerated Reinforcement Learning with Evolutionary Intelligence",
        "authors": [
            "Saeejith Nair",
            "Mohammad Javad Shafiee",
            "Alexander Wong"
        ],
        "abstract": "We present DARLEI, a framework that combines evolutionary algorithms with\nparallelized reinforcement learning for efficiently training and evolving\npopulations of UNIMAL agents. Our approach utilizes Proximal Policy\nOptimization (PPO) for individual agent learning and pairs it with a tournament\nselection-based generational learning mechanism to foster morphological\nevolution. By building on Nvidia's Isaac Gym, DARLEI leverages GPU accelerated\nsimulation to achieve over 20x speedup using just a single workstation,\ncompared to previous work which required large distributed CPU clusters. We\nsystematically characterize DARLEI's performance under various conditions,\nrevealing factors impacting diversity of evolved morphologies. For example, by\nenabling inter-agent collisions within the simulator, we find that we can\nsimulate some multi-agent interactions between the same morphology, and see how\nit influences individual agent capabilities and long-term evolutionary\nadaptation. While current results demonstrate limited diversity across\ngenerations, we hope to extend DARLEI in future work to include interactions\nbetween diverse morphologies in richer environments, and create a platform that\nallows for coevolving populations and investigating emergent behaviours in\nthem. Our source code is also made publicly at\nhttps://saeejithnair.github.io/darlei.",
        "date": "2023-12-08T16:51:10+00:00",
        "link": "http://arxiv.org/pdf/2312.05171v1"
    },
    {
        "title": "A Review of Cooperation in Multi-agent Learning",
        "authors": [
            "Yali Du",
            "Joel Z. Leibo",
            "Usman Islam",
            "Richard Willis",
            "Peter Sunehag"
        ],
        "abstract": "Cooperation in multi-agent learning (MAL) is a topic at the intersection of\nnumerous disciplines, including game theory, economics, social sciences, and\nevolutionary biology. Research in this area aims to understand both how agents\ncan coordinate effectively when goals are aligned and how they may cooperate in\nsettings where gains from working together are possible but possibilities for\nconflict abound. In this paper we provide an overview of the fundamental\nconcepts, problem settings and algorithms of multi-agent learning. This\nencompasses reinforcement learning, multi-agent sequential decision-making,\nchallenges associated with multi-agent cooperation, and a comprehensive review\nof recent progress, along with an evaluation of relevant metrics. Finally we\ndiscuss open challenges in the field with the aim of inspiring new avenues for\nresearch.",
        "date": "2023-12-08T16:42:15+00:00",
        "link": "http://arxiv.org/pdf/2312.05162v1"
    },
    {
        "title": "On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against \"Truly Anonymous Synthetic Data''",
        "authors": [
            "Georgi Ganev",
            "Emiliano De Cristofaro"
        ],
        "abstract": "Training generative models to produce synthetic data is meant to provide a\nprivacy-friendly approach to data release. However, we get robust guarantees\nonly when models are trained to satisfy Differential Privacy (DP). Alas, this\nis not the standard in industry as many companies use ad-hoc strategies to\nempirically evaluate privacy based on the statistical similarity between\nsynthetic and real data. In this paper, we review the privacy metrics offered\nby leading companies in this space and shed light on a few critical flaws in\nreasoning about privacy entirely via empirical evaluations. We analyze the\nundesirable properties of the most popular metrics and filters and demonstrate\ntheir unreliability and inconsistency through counter-examples. We then present\na reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks all\nattributes of) at least 78% of the low-density train records (or outliers) with\nonly black-box access to a single fitted generative model and the privacy\nmetrics. Finally, we show that applying DP only to the model or using\nlow-utility generators does not mitigate ReconSyn as the privacy leakage\npredominantly comes from the metrics. Overall, our work serves as a warning to\npractitioners not to deviate from established privacy-preserving mechanisms.",
        "date": "2023-12-08T15:42:28+00:00",
        "link": "http://arxiv.org/pdf/2312.05114v1"
    },
    {
        "title": "Backward Learning for Goal-Conditioned Policies",
        "authors": [
            "Marc Höftmann",
            "Jan Robine",
            "Stefan Harmeling"
        ],
        "abstract": "Can we learn policies in reinforcement learning without rewards? Can we learn\na policy just by trying to reach a goal state? We answer these questions\npositively by proposing a multi-step procedure that first learns a world model\nthat goes backward in time, secondly generates goal-reaching backward\ntrajectories, thirdly improves those sequences using shortest path finding\nalgorithms, and finally trains a neural network policy by imitation learning.\nWe evaluate our method on a deterministic maze environment where the\nobservations are $64\\times 64$ pixel bird's eye images and can show that it\nconsistently reaches several goals.",
        "date": "2023-12-08T13:52:16+00:00",
        "link": "http://arxiv.org/pdf/2312.05044v1"
    },
    {
        "title": "Physical-Layer Semantic-Aware Network for Zero-Shot Wireless Sensing",
        "authors": [
            "Huixiang Zhu",
            "Yong Xiao",
            "Yingyu Li",
            "Guangming Shi",
            "Walid Saad"
        ],
        "abstract": "Device-free wireless sensing has recently attracted significant interest due\nto its potential to support a wide range of immersive human-machine interactive\napplications. However, data heterogeneity in wireless signals and data privacy\nregulation of distributed sensing have been considered as the major challenges\nthat hinder the wide applications of wireless sensing in large area networking\nsystems. Motivated by the observation that signals recorded by wireless\nreceivers are closely related to a set of physical-layer semantic features, in\nthis paper we propose a novel zero-shot wireless sensing solution that allows\nmodels constructed in one or a limited number of locations to be directly\ntransferred to other locations without any labeled data. We develop a novel\nphysical-layer semantic-aware network (pSAN) framework to characterize the\ncorrelation between physical-layer semantic features and the sensing data\ndistributions across different receivers. We then propose a pSAN-based\nzero-shot learning solution in which each receiver can obtain a\nlocation-specific gesture recognition model by directly aggregating the already\nconstructed models of other receivers. We theoretically prove that models\nobtained by our proposed solution can approach the optimal model without\nrequiring any local model training. Experimental results once again verify that\nthe accuracy of models derived by our proposed solution matches that of the\nmodels trained by the real labeled data based on supervised learning approach.",
        "date": "2023-12-08T13:50:30+00:00",
        "link": "http://arxiv.org/pdf/2312.05043v1"
    },
    {
        "title": "SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control",
        "authors": [
            "Jaskirat Singh",
            "Jianming Zhang",
            "Qing Liu",
            "Cameron Smith",
            "Zhe Lin",
            "Liang Zheng"
        ],
        "abstract": "The field of generative image inpainting and object insertion has made\nsignificant progress with the recent advent of latent diffusion models.\nUtilizing a precise object mask can greatly enhance these applications.\nHowever, due to the challenges users encounter in creating high-fidelity masks,\nthere is a tendency for these methods to rely on more coarse masks (e.g.,\nbounding box) for these applications. This results in limited control and\ncompromised background content preservation. To overcome these limitations, we\nintroduce SmartMask, which allows any novice user to create detailed masks for\nprecise object insertion. Combined with a ControlNet-Inpaint model, our\nexperiments demonstrate that SmartMask achieves superior object insertion\nquality, preserving the background content more effectively than previous\nmethods. Notably, unlike prior works the proposed approach can also be used\neven without user-mask guidance, which allows it to perform mask-free object\ninsertion at diverse positions and scales. Furthermore, we find that when used\niteratively with a novel instruction-tuning based planning model, SmartMask can\nbe used to design detailed layouts from scratch. As compared with user-scribble\nbased layout design, we observe that SmartMask allows for better quality\noutputs with layout-to-image generation methods. Project page is available at\nhttps://smartmask-gen.github.io",
        "date": "2023-12-08T13:38:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05039v1"
    },
    {
        "title": "Grasp Force Optimization as a Bilinear Matrix Inequality Problem: A Deep Learning Approach",
        "authors": [
            "Hirakjyoti Basumatary",
            "Daksh Adhar",
            "Riddhiman Shaw",
            "Shyamanta M. Hazarika"
        ],
        "abstract": "Grasp force synthesis is a non-convex optimization problem involving\nconstraints that are bilinear. Traditional approaches to this problem involve\ngeneral-purpose gradient-based nonlinear optimization and semi-definite\nprogramming. With a view towards dealing with postural synergies and non-smooth\nbut convex positive semidefinite constraints, we look beyond gradient-based\noptimization. The focus of this paper is to undertake a grasp analysis of\nbiomimetic grasping in multi-fingered robotic hands as a bilinear matrix\ninequality (BMI) problem. Our analysis is to solve it using a deep learning\napproach to make the algorithm efficiently generate force closure grasps with\noptimal grasp quality on untrained/unseen objects.",
        "date": "2023-12-08T13:28:21+00:00",
        "link": "http://arxiv.org/pdf/2312.05034v1"
    },
    {
        "title": "Reinforcement Learning-Based Bionic Reflex Control for Anthropomorphic Robotic Grasping exploiting Domain Randomization",
        "authors": [
            "Hirakjyoti Basumatary",
            "Daksh Adhar",
            "Atharva Shrawge",
            "Prathamesh Kanbaskar",
            "Shyamanta M. Hazarika"
        ],
        "abstract": "Achieving human-level dexterity in robotic grasping remains a challenging\nendeavor. Robotic hands frequently encounter slippage and deformation during\nobject manipulation, issues rarely encountered by humans due to their sensory\nreceptors, experiential learning, and motor memory. The emulation of the human\ngrasping reflex within robotic hands is referred to as the ``bionic reflex\".\nPast endeavors in the realm of bionic reflex control predominantly relied on\nmodel-based and supervised learning approaches, necessitating human\nintervention during thresholding and labeling tasks. In this study, we\nintroduce an innovative bionic reflex control pipeline, leveraging\nreinforcement learning (RL); thereby eliminating the need for human\nintervention during control design. Our proposed bionic reflex controller has\nbeen designed and tested on an anthropomorphic hand, manipulating deformable\nobjects in the PyBullet physics simulator, incorporating domain randomization\n(DR) for enhanced Sim2Real transferability. Our findings underscore the promise\nof RL as a potent tool for advancing bionic reflex control within\nanthropomorphic robotic hands. We anticipate that this autonomous, RL-based\nbionic reflex controller will catalyze the development of dependable and highly\nefficient robotic and prosthetic hands, revolutionizing human-robot interaction\nand assistive technologies.",
        "date": "2023-12-08T13:04:41+00:00",
        "link": "http://arxiv.org/pdf/2312.05023v1"
    },
    {
        "title": "A Negative Result on Gradient Matching for Selective Backprop",
        "authors": [
            "Lukas Balles",
            "Cedric Archambeau",
            "Giovanni Zappella"
        ],
        "abstract": "With increasing scale in model and dataset size, the training of deep neural\nnetworks becomes a massive computational burden. One approach to speed up the\ntraining process is Selective Backprop. For this approach, we perform a forward\npass to obtain a loss value for each data point in a minibatch. The backward\npass is then restricted to a subset of that minibatch, prioritizing high-loss\nexamples. We build on this approach, but seek to improve the subset selection\nmechanism by choosing the (weighted) subset which best matches the mean\ngradient over the entire minibatch. We use the gradients w.r.t. the model's\nlast layer as a cheap proxy, resulting in virtually no overhead in addition to\nthe forward pass. At the same time, for our experiments we add a simple random\nselection baseline which has been absent from prior work. Surprisingly, we find\nthat both the loss-based as well as the gradient-matching strategy fail to\nconsistently outperform the random baseline.",
        "date": "2023-12-08T13:03:10+00:00",
        "link": "http://arxiv.org/pdf/2312.05021v1"
    },
    {
        "title": "Vision-based Learning for Drones: A Survey",
        "authors": [
            "Jiaping Xiao",
            "Rangya Zhang",
            "Yuhang Zhang",
            "Mir Feroskhan"
        ],
        "abstract": "Drones as advanced cyber-physical systems are undergoing a transformative\nshift with the advent of vision-based learning, a field that is rapidly gaining\nprominence due to its profound impact on drone autonomy and functionality.\nDifferent from existing task-specific surveys, this review offers a\ncomprehensive overview of vision-based learning in drones, emphasizing its\npivotal role in enhancing their operational capabilities. We start by\nelucidating the fundamental principles of vision-based learning, highlighting\nhow it significantly improves drones' visual perception and decision-making\nprocesses. We then categorize vision-based control methods into indirect,\nsemi-direct, and end-to-end approaches from the perception-control perspective.\nWe further explore various applications of vision-based drones with learning\ncapabilities, ranging from single-agent systems to more complex multi-agent and\nheterogeneous system scenarios, and underscore the challenges and innovations\ncharacterizing each area. Finally, we explore open questions and potential\nsolutions, paving the way for ongoing research and development in this dynamic\nand rapidly evolving field. With growing large language models (LLMs) and\nembodied intelligence, vision-based learning for drones provides a promising\nbut challenging road towards artificial general intelligence (AGI) in 3D\nphysical world.",
        "date": "2023-12-08T12:57:13+00:00",
        "link": "http://arxiv.org/pdf/2312.05019v1"
    },
    {
        "title": "Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs",
        "authors": [
            "Nicolas Hubert",
            "Pierre Monnin",
            "Heiko Paulheim"
        ],
        "abstract": "Knowledge graphs (KGs) comprise entities interconnected by relations of\ndifferent semantic meanings. KGs are being used in a wide range of\napplications. However, they inherently suffer from incompleteness, i.e.\nentities or facts about entities are missing. Consequently, a larger body of\nworks focuses on the completion of missing information in KGs, which is\ncommonly referred to as link prediction (LP). This task has traditionally and\nextensively been studied in the transductive setting, where all entities and\nrelations in the testing set are observed during training. Recently, several\nworks have tackled the LP task under more challenging settings, where entities\nand relations in the test set may be unobserved during training, or appear in\nonly a few facts. These works are known as inductive, few-shot, and zero-shot\nlink prediction. In this work, we conduct a systematic review of existing works\nin this area. A thorough analysis leads us to point out the undesirable\nexistence of diverging terminologies and task definitions for the\naforementioned settings, which further limits the possibility of comparison\nbetween recent works. We consequently aim at dissecting each setting\nthoroughly, attempting to reveal its intrinsic characteristics. A unifying\nnomenclature is ultimately proposed to refer to each of them in a simple and\nconsistent manner.",
        "date": "2023-12-08T12:13:40+00:00",
        "link": "http://arxiv.org/pdf/2312.04997v1"
    },
    {
        "title": "Out of Context: How important is Local Context in Neural Program Repair?",
        "authors": [
            "Julian Aron Prenner",
            "Romain Robbes"
        ],
        "abstract": "Deep learning source code models have been applied very successfully to the\nproblem of automated program repair. One of the standing issues is the small\ninput window of current models which often cannot fully fit the context code\nrequired for a bug fix (e.g., method or class declarations of a project).\nInstead, input is often restricted to the local context, that is, the lines\nbelow and above the bug location. In this work we study the importance of this\nlocal context on repair success: how much local context is needed?; is context\nbefore or after the bug location more important? how is local context tied to\nthe bug type? To answer these questions we train and evaluate Transformer\nmodels in many different local context configurations on three datasets and two\nprogramming languages. Our results indicate that overall repair success\nincreases with the size of the local context (albeit not for all bug types) and\nconfirm the common practice that roughly 50-60% of the input window should be\nused for context leading the bug. Our results are not only relevant for\nresearchers working on Transformer-based APR tools but also for benchmark and\ndataset creators who must decide what and how much context to include in their\ndatasets.",
        "date": "2023-12-08T11:49:02+00:00",
        "link": "http://arxiv.org/pdf/2312.04986v1"
    },
    {
        "title": "MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness",
        "authors": [
            "Xiaoyun Xu",
            "Shujian Yu",
            "Jingzheng Wu",
            "Stjepan Picek"
        ],
        "abstract": "Vision Transformers (ViTs) achieve superior performance on various tasks\ncompared to convolutional neural networks (CNNs), but ViTs are also vulnerable\nto adversarial attacks. Adversarial training is one of the most successful\nmethods to build robust CNN models. Thus, recent works explored new\nmethodologies for adversarial training of ViTs based on the differences between\nViTs and CNNs, such as better training strategies, preventing attention from\nfocusing on a single block, or discarding low-attention embeddings. However,\nthese methods still follow the design of traditional supervised adversarial\ntraining, limiting the potential of adversarial training on ViTs. This paper\nproposes a novel defense method, MIMIR, which aims to build a different\nadversarial training methodology by utilizing Masked Image Modeling at\npre-training. We create an autoencoder that accepts adversarial examples as\ninput but takes the clean examples as the modeling target. Then, we create a\nmutual information (MI) penalty following the idea of the Information\nBottleneck. Among the two information source inputs and corresponding\nadversarial perturbation, the perturbation information is eliminated due to the\nconstraint of the modeling target. Next, we provide a theoretical analysis of\nMIMIR using the bounds of the MI penalty. We also design two adaptive attacks\nwhen the adversary is aware of the MIMIR defense and show that MIMIR still\nperforms well. The experimental results show that MIMIR improves (natural and\nadversarial) accuracy on average by 4.19\\% on CIFAR-10 and 5.52\\% on\nImageNet-1K, compared to baselines. On Tiny-ImageNet, we obtained improved\nnatural accuracy of 2.99\\% on average and comparable adversarial accuracy. Our\ncode and trained models are publicly\navailable\\footnote{\\url{https://anonymous.4open.science/r/MIMIR-5444/README.md}}.",
        "date": "2023-12-08T10:50:02+00:00",
        "link": "http://arxiv.org/pdf/2312.04960v1"
    },
    {
        "title": "Benchmarking and Analysis of Unsupervised Object Segmentation from Real-world Single Images",
        "authors": [
            "Yafei Yang",
            "Bo Yang"
        ],
        "abstract": "In this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically\ninvestigate the effectiveness of existing unsupervised models on challenging\nreal-world images. We first introduce seven complexity factors to\nquantitatively measure the distributions of background and foreground object\nbiases in appearance and geometry for datasets with human annotations. With the\naid of these factors, we empirically find that, not surprisingly, existing\nunsupervised models fail to segment generic objects in real-world images,\nalthough they can easily achieve excellent performance on numerous simple\nsynthetic datasets, due to the vast gap in objectness biases between synthetic\nand real images. By conducting extensive experiments on multiple groups of\nablated real-world datasets, we ultimately find that the key factors underlying\nthe failure of existing unsupervised models on real-world images are the\nchallenging distributions of background and foreground object biases in\nappearance and geometry. Because of this, the inductive biases introduced in\nexisting unsupervised models can hardly capture the diverse object\ndistributions. Our research results suggest that future work should exploit\nmore explicit objectness biases in the network design.",
        "date": "2023-12-08T10:25:59+00:00",
        "link": "http://arxiv.org/pdf/2312.04947v1"
    },
    {
        "title": "The ICL Consistency Test",
        "authors": [
            "Lucas Weber",
            "Elia Bruni",
            "Dieuwke Hupkes"
        ],
        "abstract": "Just like the previous generation of task-tuned models, large language models\n(LLMs) that are adapted to tasks via prompt-based methods like\nin-context-learning (ICL) perform well in some setups but not in others. This\nlack of consistency in prompt-based learning hints at a lack of robust\ngeneralisation. We here introduce the ICL consistency test -- a contribution to\nthe GenBench collaborative benchmark task (CBT) -- which evaluates how\nconsistent a model makes predictions across many different setups while using\nthe same data. The test is based on different established natural language\ninference tasks. We provide preprocessed data constituting 96 different\n'setups' and a metric that estimates model consistency across these setups. The\nmetric is provided on a fine-grained level to understand what properties of a\nsetup render predictions unstable and on an aggregated level to compare overall\nmodel consistency. We conduct an empirical analysis of eight state-of-the-art\nmodels, and our consistency metric reveals how all tested LLMs lack robust\ngeneralisation.",
        "date": "2023-12-08T10:22:43+00:00",
        "link": "http://arxiv.org/pdf/2312.04945v1"
    },
    {
        "title": "Canaries and Whistles: Resilient Drone Communication Networks with (or without) Deep Reinforcement Learning",
        "authors": [
            "Chris Hicks",
            "Vasilios Mavroudis",
            "Myles Foley",
            "Thomas Davies",
            "Kate Highnam",
            "Tim Watson"
        ],
        "abstract": "Communication networks able to withstand hostile environments are critically\nimportant for disaster relief operations. In this paper, we consider a\nchallenging scenario where drones have been compromised in the supply chain,\nduring their manufacture, and harbour malicious software capable of\nwide-ranging and infectious disruption. We investigate multi-agent deep\nreinforcement learning as a tool for learning defensive strategies that\nmaximise communications bandwidth despite continual adversarial interference.\nUsing a public challenge for learning network resilience strategies, we propose\na state-of-the-art expert technique and study its superiority over deep\nreinforcement learning agents. Correspondingly, we identify three specific\nmethods for improving the performance of our learning-based agents: (1)\nensuring each observation contains the necessary information, (2) using expert\nagents to provide a curriculum for learning, and (3) paying close attention to\nreward. We apply our methods and present a new mixed strategy enabling expert\nand learning-based agents to work together and improve on all prior results.",
        "date": "2023-12-08T10:13:44+00:00",
        "link": "http://arxiv.org/pdf/2312.04940v1"
    },
    {
        "title": "Operationalizing Assurance Cases for Data Scientists: A Showcase of Concepts and Tooling in the Context of Test Data Quality for Machine Learning",
        "authors": [
            "Lisa Jöckel",
            "Michael Kläs",
            "Janek Groß",
            "Pascal Gerber",
            "Markus Scholz",
            "Jonathan Eberle",
            "Marc Teschner",
            "Daniel Seifert",
            "Richard Hawkins",
            "John Molloy",
            "Jens Ottnad"
        ],
        "abstract": "Assurance Cases (ACs) are an established approach in safety engineering to\nargue quality claims in a structured way. In the context of quality assurance\nfor Machine Learning (ML)-based software components, ACs are also being\ndiscussed and appear promising. Tools for operationalizing ACs do exist, yet\nmainly focus on supporting safety engineers on the system level. However,\nassuring the quality of an ML component within the system is commonly the\nresponsibility of data scientists, who are usually less familiar with these\ntools. To address this gap, we propose a framework to support the\noperationalization of ACs for ML components based on technologies that data\nscientists use on a daily basis: Python and Jupyter Notebook. Our aim is to\nmake the process of creating ML-related evidence in ACs more effective. Results\nfrom the application of the framework, documented through notebooks, can be\nintegrated into existing AC tools. We illustrate the application of the\nframework on an example excerpt concerned with the quality of the test data.",
        "date": "2023-12-08T09:34:46+00:00",
        "link": "http://arxiv.org/pdf/2312.04917v1"
    },
    {
        "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism",
        "authors": [
            "Yanxi Chen",
            "Xuchen Pan",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "abstract": "We present EE-LLM, a framework for large-scale training and inference of\nearly-exit large language models (LLMs). While recent works have shown\npreliminary evidence for the efficacy of early exiting in accelerating LLM\ninference, EE-LLM makes a foundational step towards scaling up early-exit LLMs\nby supporting their training and inference with massive 3D parallelism. Built\nupon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and\nperformance optimizations tailored to early exiting, including a lightweight\nmethod that facilitates backpropagation for the early-exit training objective\nwith pipeline parallelism, techniques of leveraging idle resources in the\noriginal pipeline schedule for computation related to early-exit layers, and\ntwo approaches of early-exit inference that are compatible with KV caching for\nautoregressive generation. Our analytical and empirical study shows that EE-LLM\nachieves great training efficiency with negligible computational overhead\ncompared to standard LLM training, as well as outstanding inference speedup\nwithout compromising output quality. To facilitate further research and\nadoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.",
        "date": "2023-12-08T09:31:50+00:00",
        "link": "http://arxiv.org/pdf/2312.04916v1"
    },
    {
        "title": "SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation",
        "authors": [
            "Bangyan He",
            "Xiaojun Jia",
            "Siyuan Liang",
            "Tianrui Lou",
            "Yang Liu",
            "Xiaochun Cao"
        ],
        "abstract": "Current Visual-Language Pre-training (VLP) models are vulnerable to\nadversarial examples. These adversarial examples present substantial security\nrisks to VLP models, as they can leverage inherent weaknesses in the models,\nresulting in incorrect predictions. In contrast to white-box adversarial\nattacks, transfer attacks (where the adversary crafts adversarial examples on a\nwhite-box model to fool another black-box model) are more reflective of\nreal-world scenarios, thus making them more meaningful for research. By\nsummarizing and analyzing existing research, we identified two factors that can\ninfluence the efficacy of transfer attacks on VLP models: inter-modal\ninteraction and data diversity. Based on these insights, we propose a\nself-augment-based transfer attack method, termed SA-Attack. Specifically,\nduring the generation of adversarial images and adversarial texts, we apply\ndifferent data augmentation methods to the image modality and text modality,\nrespectively, with the aim of improving the adversarial transferability of the\ngenerated adversarial images and texts. Experiments conducted on the FLickr30K\nand COCO datasets have validated the effectiveness of our method. Our code will\nbe available after this paper is accepted.",
        "date": "2023-12-08T09:08:50+00:00",
        "link": "http://arxiv.org/pdf/2312.04913v1"
    },
    {
        "title": "BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting",
        "authors": [
            "Huming Qiu",
            "Junjie Sun",
            "Mi Zhang",
            "Xudong Pan",
            "Min Yang"
        ],
        "abstract": "Deep neural networks (DNNs) are susceptible to backdoor attacks, where\nmalicious functionality is embedded to allow attackers to trigger incorrect\nclassifications. Old-school backdoor attacks use strong trigger features that\ncan easily be learned by victim models. Despite robustness against input\nvariation, the robustness however increases the likelihood of unintentional\ntrigger activations. This leaves traces to existing defenses, which find\napproximate replacements for the original triggers that can activate the\nbackdoor without being identical to the original trigger via, e.g., reverse\nengineering and sample overlay.\n  In this paper, we propose and investigate a new characteristic of backdoor\nattacks, namely, backdoor exclusivity, which measures the ability of backdoor\ntriggers to remain effective in the presence of input variation. Building upon\nthe concept of backdoor exclusivity, we propose Backdoor Exclusivity LifTing\n(BELT), a novel technique which suppresses the association between the backdoor\nand fuzzy triggers to enhance backdoor exclusivity for defense evasion.\nExtensive evaluation on three popular backdoor benchmarks validate, our\napproach substantially enhances the stealthiness of four old-school backdoor\nattacks, which, after backdoor exclusivity lifting, is able to evade six\nstate-of-the-art backdoor countermeasures, at almost no cost of the attack\nsuccess rate and normal utility. For example, one of the earliest backdoor\nattacks BadNet, enhanced by BELT, evades most of the state-of-the-art defenses\nincluding ABS and MOTH which would otherwise recognize the backdoored model.",
        "date": "2023-12-08T08:35:16+00:00",
        "link": "http://arxiv.org/pdf/2312.04902v1"
    },
    {
        "title": "KwaiAgents: Generalized Information-seeking Agent System with Large Language Models",
        "authors": [
            "Haojie Pan",
            "Zepeng Zhai",
            "Hao Yuan",
            "Yaojia Lv",
            "Ruiji Fu",
            "Ming Liu",
            "Zhongyuan Wang",
            "Bing Qin"
        ],
        "abstract": "Driven by curiosity, humans have continually sought to explore and understand\nthe world around them, leading to the invention of various tools to satiate\nthis inquisitiveness. Despite not having the capacity to process and memorize\nvast amounts of information in their brains, humans excel in critical thinking,\nplanning, reflection, and harnessing available tools to interact with and\ninterpret the world, enabling them to find answers efficiently. The recent\nadvancements in large language models (LLMs) suggest that machines might also\npossess the aforementioned human-like capabilities, allowing them to exhibit\npowerful abilities even with a constrained parameter count. In this paper, we\nintroduce KwaiAgents, a generalized information-seeking agent system based on\nLLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its\ncognitive core, which is capable of understanding a user's query, behavior\nguidelines, and referencing external documents. The agent can also update and\nretrieve information from its internal memory, plan and execute actions using a\ntime-aware search-browse toolkit, and ultimately provide a comprehensive\nresponse. We further investigate the system's performance when powered by LLMs\nless advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,\ndesigned to ensure even an open-sourced 7B or 13B model performs well among\nmany agent systems. We exploit both benchmark and human evaluations to\nsystematically validate these capabilities. Extensive experiments show the\nsuperiority of our agent system compared to other autonomous agents and\nhighlight the enhanced generalized agent-abilities of our fine-tuned LLMs.",
        "date": "2023-12-08T08:11:11+00:00",
        "link": "http://arxiv.org/pdf/2312.04889v1"
    },
    {
        "title": "Classification of Human- and AI-Generated Texts for English, French, German, and Spanish",
        "authors": [
            "Kristina Schaaff",
            "Tim Schlippe",
            "Lorenz Mindner"
        ],
        "abstract": "In this paper we analyze features to classify human- and AI-generated text\nfor English, French, German and Spanish and compare them across languages. We\ninvestigate two scenarios: (1) The detection of text generated by AI from\nscratch, and (2) the detection of text rephrased by AI. For training and\ntesting the classifiers in this multilingual setting, we created a new text\ncorpus covering 10 topics for each language. For the detection of AI-generated\ntext, the combination of all proposed features performs best, indicating that\nour features are portable to other related languages: The F1-scores are close\nwith 99% for Spanish, 98% for English, 97% for German and 95% for French. For\nthe detection of AI-rephrased text, the systems with all features outperform\nsystems with other features in many cases, but using only document features\nperforms best for German (72%) and Spanish (86%) and only text vector features\nleads to best results for English (78%).",
        "date": "2023-12-08T07:42:06+00:00",
        "link": "http://arxiv.org/pdf/2312.04882v1"
    },
    {
        "title": "Predictive Chemistry Augmented with Text Retrieval",
        "authors": [
            "Yujie Qian",
            "Zhening Li",
            "Zhengkai Tu",
            "Connor W. Coley",
            "Regina Barzilay"
        ],
        "abstract": "This paper focuses on using natural language descriptions to enhance\npredictive models in the chemistry field. Conventionally, chemoinformatics\nmodels are trained with extensive structured data manually extracted from the\nliterature. In this paper, we introduce TextReact, a novel method that directly\naugments predictive chemistry with texts retrieved from the literature.\nTextReact retrieves text descriptions relevant for a given chemical reaction,\nand then aligns them with the molecular representation of the reaction. This\nalignment is enhanced via an auxiliary masked LM objective incorporated in the\npredictor training. We empirically validate the framework on two chemistry\ntasks: reaction condition recommendation and one-step retrosynthesis. By\nleveraging text retrieval, TextReact significantly outperforms state-of-the-art\nchemoinformatics models trained solely on molecular data.",
        "date": "2023-12-08T07:40:59+00:00",
        "link": "http://arxiv.org/pdf/2312.04881v1"
    },
    {
        "title": "Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and UMAP Visualization and Classifying Attacks",
        "authors": [
            "Humera Ghani",
            "Shahram Salekzamankhani",
            "Bal Virdee"
        ],
        "abstract": "Networks, threat models, and malicious actors are advancing quickly. With the\nincreased deployment of the 5G networks, the security issues of the attached 5G\nphysical devices have also increased. Therefore, artificial intelligence based\nautonomous end-to-end security design is needed that can deal with incoming\nthreats by detecting network traffic anomalies. To address this requirement, in\nthis research, we used a recently published 5G traffic dataset, 5G-NIDD, to\ndetect network traffic anomalies using machine and deep learning approaches.\nFirst, we analyzed the dataset using three visualization techniques:\nt-Distributed Stochastic Neighbor Embedding (t-SNE), Uniform Manifold\nApproximation and Projection (UMAP), and Principal Component Analysis (PCA).\nSecond, we reduced the data dimensionality using mutual information and PCA\ntechniques. Third, we solve the class imbalance issue by inserting synthetic\nrecords of minority classes. Last, we performed classification using six\ndifferent classifiers and presented the evaluation metrics. We received the\nbest results when K-Nearest Neighbors classifier was used: accuracy (97.2%),\ndetection rate (96.7%), and false positive rate (2.2%).",
        "date": "2023-12-08T06:43:19+00:00",
        "link": "http://arxiv.org/pdf/2312.04864v1"
    },
    {
        "title": "Damage GAN: A Generative Model for Imbalanced Data",
        "authors": [
            "Ali Anaissi",
            "Yuanzhe Jia",
            "Ali Braytee",
            "Mohamad Naji",
            "Widad Alyassine"
        ],
        "abstract": "This study delves into the application of Generative Adversarial Networks\n(GANs) within the context of imbalanced datasets. Our primary aim is to enhance\nthe performance and stability of GANs in such datasets. In pursuit of this\nobjective, we introduce a novel network architecture known as Damage GAN,\nbuilding upon the ContraD GAN framework which seamlessly integrates GANs and\ncontrastive learning. Through the utilization of contrastive learning, the\ndiscriminator is trained to develop an unsupervised representation capable of\ndistinguishing all provided samples. Our approach draws inspiration from the\nstraightforward framework for contrastive learning of visual representations\n(SimCLR), leading to the formulation of a distinctive loss function. We also\nexplore the implementation of self-damaging contrastive learning (SDCLR) to\nfurther enhance the optimization of the ContraD GAN model. Comparative\nevaluations against baseline models including the deep convolutional GAN\n(DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed\nmodel, Damage GAN, in terms of generated image distribution, model stability,\nand image quality when applied to imbalanced datasets.",
        "date": "2023-12-08T06:36:33+00:00",
        "link": "http://arxiv.org/pdf/2312.04862v1"
    },
    {
        "title": "Radar Perception in Autonomous Driving: Exploring Different Data Representations",
        "authors": [
            "Shanliang Yao",
            "Runwei Guan",
            "Zitian Peng",
            "Chenhang Xu",
            "Yilu Shi",
            "Yong Yue",
            "Eng Gee Lim",
            "Hyungjoon Seo",
            "Ka Lok Man",
            "Xiaohui Zhu",
            "Yutao Yue"
        ],
        "abstract": "With the rapid advancements of sensor technology and deep learning,\nautonomous driving systems are providing safe and efficient access to\nintelligent vehicles as well as intelligent transportation. Among these\nequipped sensors, the radar sensor plays a crucial role in providing robust\nperception information in diverse environmental conditions. This review focuses\non exploring different radar data representations utilized in autonomous\ndriving systems. Firstly, we introduce the capabilities and limitations of the\nradar sensor by examining the working principles of radar perception and signal\nprocessing of radar measurements. Then, we delve into the generation process of\nfive radar representations, including the ADC signal, radar tensor, point\ncloud, grid map, and micro-Doppler signature. For each radar representation, we\nexamine the related datasets, methods, advantages and limitations. Furthermore,\nwe discuss the challenges faced in these data representations and propose\npotential research directions. Above all, this comprehensive review offers an\nin-depth insight into how these representations enhance autonomous system\ncapabilities, providing guidance for radar perception researchers. To\nfacilitate retrieval and comparison of different data representations, datasets\nand methods, we provide an interactive website at\nhttps://radar-camera-fusion.github.io/radar.",
        "date": "2023-12-08T06:31:19+00:00",
        "link": "http://arxiv.org/pdf/2312.04861v1"
    },
    {
        "title": "Apollo's Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates",
        "authors": [
            "Haotian Wang",
            "Xiyuan Du",
            "Weijiang Yu",
            "Qianglong Chen",
            "Kun Zhu",
            "Zheng Chu",
            "Lian Yan",
            "Yi Guan"
        ],
        "abstract": "Multi-agent debate systems are designed to derive accurate and consistent\nconclusions through adversarial interactions among agents. However, these\nsystems often encounter challenges due to cognitive constraints, manifesting as\n(1) agents' obstinate adherence to incorrect viewpoints and (2) their\npropensity to abandon correct viewpoints. These issues are primarily\nresponsible for the ineffectiveness of such debates. Addressing the challenge\nof cognitive constraints, we introduce a novel framework, the Multi-Agent\nDebate with Retrieval Augmented (MADRA). MADRA incorporates retrieval of prior\nknowledge into the debate process, effectively breaking cognitive constraints\nand enhancing the agents' reasoning capabilities. Furthermore, we have\ndeveloped a self-selection module within this framework, enabling agents to\nautonomously select pertinent evidence, thereby minimizing the impact of\nirrelevant or noisy data. We have comprehensively tested and analyzed MADRA\nacross six diverse datasets. The experimental results demonstrate that our\napproach significantly enhances performance across various tasks, proving the\neffectiveness of our proposed method.",
        "date": "2023-12-08T06:22:12+00:00",
        "link": "http://arxiv.org/pdf/2312.04854v1"
    },
    {
        "title": "FREDSum: A Dialogue Summarization Corpus for French Political Debates",
        "authors": [
            "Virgile Rennard",
            "Guokan Shang",
            "Damien Grari",
            "Julie Hunter",
            "Michalis Vazirgiannis"
        ],
        "abstract": "Recent advances in deep learning, and especially the invention of\nencoder-decoder architectures, has significantly improved the performance of\nabstractive summarization systems. The majority of research has focused on\nwritten documents, however, neglecting the problem of multi-party dialogue\nsummarization. In this paper, we present a dataset of French political debates\nfor the purpose of enhancing resources for multi-lingual dialogue\nsummarization. Our dataset consists of manually transcribed and annotated\npolitical debates, covering a range of topics and perspectives. We highlight\nthe importance of high quality transcription and annotations for training\naccurate and effective dialogue summarization models, and emphasize the need\nfor multilingual resources to support dialogue summarization in non-English\nlanguages. We also provide baseline experiments using state-of-the-art methods,\nand encourage further research in this area to advance the field of dialogue\nsummarization. Our dataset will be made publicly available for use by the\nresearch community.",
        "date": "2023-12-08T05:42:04+00:00",
        "link": "http://arxiv.org/pdf/2312.04843v1"
    },
    {
        "title": "Analysis on Effects of Fault Elements in Memristive Neuromorphic Systems",
        "authors": [
            "Hyun-Jong Lee",
            "Jae-Han Lim"
        ],
        "abstract": "Nowadays, neuromorphic systems based on Spiking Neural Networks (SNNs)\nattract attentions of many researchers. There are many studies to improve\nperformances of neuromorphic systems. These studies have been showing\nsatisfactory results. To magnify performances of neuromorphic systems,\ndeveloping actual neuromorphic systems is essential. For developing them,\nmemristors play key role due to their useful characteristics. Although\nmemristors are essential for actual neuromorphic systems, they are vulnerable\nto faults. However, there are few studies analyzing effects of fault elements\nin neuromorphic systems using memristors. To solve this problem, we analyze\nperformance of a memristive neuromorphic system with fault elements changing\nfault ratios, types, and positions. We choose neurons and synapses to inject\nfaults. We inject two types of faults to synapses: SA0 and SA1 faults. The\nfault synapses appear in random and important positions. Through our analysis,\nwe discover the following four interesting points. First, memristive\ncharacteristics increase vulnerability of neuromorphic systems to fault\nelements. Second, fault neuron ratios reducing performance sharply exist.\nThird, performance degradation by fault synapses depends on fault types.\nFinally, SA1 fault synapses improve performance when they appear in important\npositions.",
        "date": "2023-12-08T05:37:14+00:00",
        "link": "http://arxiv.org/pdf/2312.04840v1"
    },
    {
        "title": "Understanding Teacher Perspectives and Experiences after Deployment of AI Literacy Curriculum in Middle-school Classrooms",
        "authors": [
            "Prerna Ravi",
            "Annalisa Broski",
            "Glenda Stump",
            "Hal Abelson",
            "Eric Klopfer",
            "Cynthia Breazeal"
        ],
        "abstract": "Artificial Intelligence (AI) and its associated applications are ubiquitous\nin today's world, making it imperative that students and their teachers\nunderstand how it works and the ramifications arising from its usage. In this\nstudy, we investigate the experiences of seven teachers following their\nimplementation of modules from the MIT RAICA (Responsible AI for Computational\nAction) curriculum. Through semi-structured interviews, we investigated their\ninstructional strategies as they engaged with the AI curriculum in their\nclassroom, how their teaching and learning beliefs about AI evolved with the\ncurriculum as well as how those beliefs impacted their implementation of the\ncurriculum. Our analysis suggests that the AI modules not only expanded our\nteachers' knowledge in the field, but also prompted them to recognize its daily\napplications and their ethical and societal implications, so that they could\nbetter engage with the content they deliver to students. Teachers were able to\nleverage their own interdisciplinary backgrounds to creatively introduce\nfoundational AI topics to students to maximize engagement and playful learning.\nOur teachers advocated their need for better external support when navigating\ntechnological resources, additional time for preparation given the novelty of\nthe curriculum, more flexibility within curriculum timelines, and additional\naccommodations for students of determination. Our findings provide valuable\ninsights for enhancing future iterations of AI literacy curricula and teacher\nprofessional development (PD) resources.",
        "date": "2023-12-08T05:36:16+00:00",
        "link": "http://arxiv.org/pdf/2312.04839v1"
    },
    {
        "title": "Localized Symbolic Knowledge Distillation for Visual Commonsense Models",
        "authors": [
            "Jae Sung Park",
            "Jack Hessel",
            "Khyathi Raghavi Chandu",
            "Paul Pu Liang",
            "Ximing Lu",
            "Peter West",
            "Youngjae Yu",
            "Qiuyuan Huang",
            "Jianfeng Gao",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "abstract": "Instruction following vision-language (VL) models offer a flexible interface\nthat supports a broad range of multimodal tasks in a zero-shot fashion.\nHowever, interfaces that operate on full images do not directly enable the user\nto \"point to\" and access specific regions within images. This capability is\nimportant not only to support reference-grounded VL benchmarks, but also, for\npractical applications that require precise within-image reasoning. We build\nLocalized Visual Commonsense models, which allow users to specify (multiple)\nregions as input. We train our model by sampling localized commonsense\nknowledge from a large language model (LLM): specifically, we prompt an LLM to\ncollect commonsense knowledge given a global literal image description and a\nlocal literal region description automatically generated by a set of VL models.\nWith a separately trained critic model that selects high-quality examples, we\nfind that training on the localized commonsense corpus can successfully distill\nexisting VL models to support a reference-as-input interface. Empirical results\nand human evaluations in a zero-shot setup demonstrate that our distillation\nmethod results in more precise VL models of reasoning compared to a baseline of\npassing a generated referring expression to an LLM.",
        "date": "2023-12-08T05:23:50+00:00",
        "link": "http://arxiv.org/pdf/2312.04837v1"
    },
    {
        "title": "Thermodynamic Computing System for AI Applications",
        "authors": [
            "Denis Melanson",
            "Mohammad Abu Khater",
            "Maxwell Aifer",
            "Kaelan Donatella",
            "Max Hunter Gordon",
            "Thomas Ahle",
            "Gavin Crooks",
            "Antonio J. Martinez",
            "Faris Sbahi",
            "Patrick J. Coles"
        ],
        "abstract": "Recent breakthroughs in artificial intelligence (AI) algorithms have\nhighlighted the need for novel computing hardware in order to truly unlock the\npotential for AI. Physics-based hardware, such as thermodynamic computing, has\nthe potential to provide a fast, low-power means to accelerate AI primitives,\nespecially generative AI and probabilistic AI. In this work, we present the\nfirst continuous-variable thermodynamic computer, which we call the stochastic\nprocessing unit (SPU). Our SPU is composed of RLC circuits, as unit cells, on a\nprinted circuit board, with 8 unit cells that are all-to-all coupled via\nswitched capacitances. It can be used for either sampling or linear algebra\nprimitives, and we demonstrate Gaussian sampling and matrix inversion on our\nhardware. The latter represents the first thermodynamic linear algebra\nexperiment. We also illustrate the applicability of the SPU to uncertainty\nquantification for neural network classification. We envision that this\nhardware, when scaled up in size, will have significant impact on accelerating\nvarious probabilistic AI applications.",
        "date": "2023-12-08T05:22:04+00:00",
        "link": "http://arxiv.org/pdf/2312.04836v1"
    },
    {
        "title": "HuRef: HUman-REadable Fingerprint for Large Language Models",
        "authors": [
            "Boyi Zeng",
            "Chenghu Zhou",
            "Xinbing Wang",
            "Zhouhan Lin"
        ],
        "abstract": "Protecting the copyright of large language models (LLMs) has become crucial\ndue to their resource-intensive training and accompanying carefully designed\nlicenses. However, identifying the original base model of an LLM is challenging\ndue to potential parameter alterations through fine-tuning or continued\npretraining. In this study, we introduce HuRef, a human-readable fingerprint\nfor LLMs that uniquely identifies the base model without exposing model\nparameters or interfering with training. We first observe that the vector\ndirection of LLM parameters remains stable after the model has converged during\npretraining, showing negligible perturbations through subsequent training\nsteps, including continued pretraining, supervised fine-tuning (SFT), and RLHF,\nwhich makes it a sufficient condition to identify the base model. The necessity\nis validated by continuing to train an LLM with an extra term to drive away the\nmodel parameters' direction and the model becomes damaged. However, this\ndirection is vulnerable to simple attacks like dimension permutation or matrix\nrotation, which significantly change it without affecting performance. To\naddress this, leveraging the Transformer structure, we systematically analyze\npotential attacks and define three invariant terms that identify an LLM's base\nmodel. We make these invariant terms human-readable by mapping them to a\nGaussian vector using a convolutional encoder and then converting it into a\nnatural image with StyleGAN2. Our method generates a dog image as an identity\nfingerprint for an LLM, where the dog's appearance strongly indicates the LLM's\nbase model. Experimental results across various LLMs demonstrate the\neffectiveness of our method, the generated dog image remains invariant to\ndifferent training steps, including SFT, RLHF, or even continued pretraining\nwith augmented vocabulary in a new language.",
        "date": "2023-12-08T05:01:47+00:00",
        "link": "http://arxiv.org/pdf/2312.04828v1"
    },
    {
        "title": "Unify Change Point Detection and Segment Classification in a Regression Task for Transportation Mode Identification",
        "authors": [
            "Rongsong Li",
            "Xin Pei"
        ],
        "abstract": "Identifying travelers' transportation modes is important in transportation\nscience and location-based services. It's appealing for researchers to leverage\nGPS trajectory data to infer transportation modes with the popularity of\nGPS-enabled devices, e.g., smart phones. Existing studies frame this problem as\nclassification task. The dominant two-stage studies divide the trip into\nsingle-one mode segments first and then categorize these segments. The over\nsegmentation strategy and inevitable error propagation bring difficulties to\nclassification stage and make optimizing the whole system hard. The recent\none-stage works throw out trajectory segmentation entirely to avoid these by\ndirectly conducting point-wise classification for the trip, whereas leaving\npredictions dis-continuous. To solve above-mentioned problems, inspired by YOLO\nand SSD in object detection, we propose to reframe change point detection and\nsegment classification as a unified regression task instead of the existing\nclassification task. We directly regress coordinates of change points and\nclassify associated segments. In this way, our method divides the trip into\nsegments under a supervised manner and leverage more contextual information,\nobtaining predictions with high accuracy and continuity. Two frameworks,\nTrajYOLO and TrajSSD, are proposed to solve the regression task and various\nfeature extraction backbones are exploited. Exhaustive experiments on GeoLife\ndataset show that the proposed method has competitive overall identification\naccuracy of 0.853 when distinguishing five modes: walk, bike, bus, car, train.\nAs for change point detection, our method increases precision at the cost of\ndrop in recall. All codes are available at\nhttps://github.com/RadetzkyLi/TrajYOLO-SSD.",
        "date": "2023-12-08T03:59:01+00:00",
        "link": "http://arxiv.org/pdf/2312.04821v1"
    },
    {
        "title": "Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting",
        "authors": [
            "Xiaofeng Yang",
            "Yiwen Chen",
            "Cheng Chen",
            "Chi Zhang",
            "Yi Xu",
            "Xulei Yang",
            "Fayao Liu",
            "Guosheng Lin"
        ],
        "abstract": "We propose a unified framework aimed at enhancing the diffusion priors for 3D\ngeneration tasks. Despite the critical importance of these tasks, existing\nmethodologies often struggle to generate high-caliber results. We begin by\nexamining the inherent limitations in previous diffusion priors. We identify a\ndivergence between the diffusion priors and the training procedures of\ndiffusion models that substantially impairs the quality of 3D generation. To\naddress this issue, we propose a novel, unified framework that iteratively\noptimizes both the 3D model and the diffusion prior. Leveraging the different\nlearnable parameters of the diffusion prior, our approach offers multiple\nconfigurations, affording various trade-offs between performance and\nimplementation complexity. Notably, our experimental results demonstrate that\nour method markedly surpasses existing techniques, establishing new\nstate-of-the-art in the realm of text-to-3D generation. Furthermore, our\napproach exhibits impressive performance on both NeRF and the newly introduced\n3D Gaussian Splatting backbones. Additionally, our framework yields insightful\ncontributions to the understanding of recent score distillation methods, such\nas the VSD and DDS loss.",
        "date": "2023-12-08T03:55:34+00:00",
        "link": "http://arxiv.org/pdf/2312.04820v1"
    },
    {
        "title": "A Review On Table Recognition Based On Deep Learning",
        "authors": [
            "Shi Jiyuan",
            "Shi chunqi"
        ],
        "abstract": "Table recognition is using the computer to automatically understand the\ntable, to detect the position of the table from the document or picture, and to\ncorrectly extract and identify the internal structure and content of the table.\nAfter earlier mainstream approaches based on heuristic rules and machine\nlearning, the development of deep learning techniques has brought a new\nparadigm to this field. This review mainly discusses the table recognition\nproblem from five aspects. The first part introduces data sets, benchmarks, and\ncommonly used evaluation indicators. This section selects representative data\nsets, benchmarks, and evaluation indicators that are frequently used by\nresearchers. The second part introduces the table recognition model. This\nsurvey introduces the development of the table recognition model, especially\nthe table recognition model based on deep learning. It is generally accepted\nthat table recognition is divided into two stages: table detection and table\nstructure recognition. This section introduces the models that follow this\nparadigm (TD and TSR). The third part is the End-to-End method, this section\nintroduces some scholars' attempts to use an end-to-end approach to solve the\ntable recognition problem once and for all and the part are Data-centric\nmethods, such as data augmentation, aligning benchmarks, and other methods. The\nfourth part is the data-centric approach, such as data enhancement, alignment\nbenchmark, and so on. The fifth part summarizes and compares the experimental\ndata in the field of form recognition, and analyzes the mainstream and more\nadvantageous methods. Finally, this paper also discusses the possible\ndevelopment direction and trend of form processing in the future, to provide\nsome ideas for researchers in the field of table recognition. (Resource will be\nreleased at https://github.com/Wa1den-jy/Topic-on-Table-Recognition .)",
        "date": "2023-12-08T02:58:00+00:00",
        "link": "http://arxiv.org/pdf/2312.04808v1"
    },
    {
        "title": "Development and Assessment of Autonomous Vehicles in Both Fully Automated and Mixed Traffic Conditions",
        "authors": [
            "Ahmed Abdelrahman"
        ],
        "abstract": "Autonomous Vehicle (AV) technology is advancing rapidly, promising a\nsignificant shift in road transportation safety and potentially resolving\nvarious complex transportation issues. With the increasing deployment of AVs by\nvarious companies, questions emerge about how AVs interact with each other and\nwith human drivers, especially when AVs are prevalent on the roads. Ensuring\ncooperative interaction between AVs and between AVs and human drivers is\ncritical, though there are concerns about possible negative competitive\nbehaviors. This paper presents a multi-stage approach, starting with the\ndevelopment of a single AV and progressing to connected AVs, incorporating\nsharing and caring V2V communication strategy to enhance mutual coordination. A\nsurvey is conducted to validate the driving performance of the AV and will be\nutilized for a mixed traffic case study, which focuses on how the human drivers\nwill react to the AV driving alongside them on the same road. Results show that\nusing deep reinforcement learning, the AV acquired driving behavior that\nreached human driving performance. The adoption of sharing and caring based V2V\ncommunication within AV networks enhances their driving behavior, aids in more\neffective action planning, and promotes collaborative behavior amongst the AVs.\nThe survey shows that safety in mixed traffic cannot be guaranteed, as we\ncannot control human ego-driven actions if they decide to compete with AV.\nConsequently, this paper advocates for enhanced research into the safe\nincorporation of AVs on public roads.",
        "date": "2023-12-08T02:40:11+00:00",
        "link": "http://arxiv.org/pdf/2312.04805v1"
    },
    {
        "title": "An Overview of MLCommons Cloud Mask Benchmark: Related Research and Data",
        "authors": [
            "Gregor von Laszewski",
            "Ruochen Gu"
        ],
        "abstract": "Cloud masking is a crucial task that is well-motivated for meteorology and\nits applications in environmental and atmospheric sciences. Its goal is, given\nsatellite images, to accurately generate cloud masks that identify each pixel\nin image to contain either cloud or clear sky. In this paper, we summarize some\nof the ongoing research activities in cloud masking, with a focus on the\nresearch and benchmark currently conducted in MLCommons Science Working Group.\nThis overview is produced with the hope that others will have an easier time\ngetting started and collaborate on the activities related to MLCommons Cloud\nMask Benchmark.",
        "date": "2023-12-08T02:25:26+00:00",
        "link": "http://arxiv.org/pdf/2312.04799v1"
    },
    {
        "title": "AI safety by debate via regret minimization",
        "authors": [
            "Xinyi Chen",
            "Angelica Chen",
            "Dean Foster",
            "Elad Hazan"
        ],
        "abstract": "We consider the setting of AI safety by debate as a repeated game. We\nconsider the question of efficient regret minimization in this setting, when\nthe players are either AIs or humans, equipped with access to computationally\nsuperior AIs. In such a setting, we characterize when internal and external\nregret can be minimized efficiently. We conclude with conditions in which a\nsequence of strategies converges to a correlated equilibrium.",
        "date": "2023-12-08T02:06:55+00:00",
        "link": "http://arxiv.org/pdf/2312.04792v1"
    },
    {
        "title": "Fine-Tuning InstructPix2Pix for Advanced Image Colorization",
        "authors": [
            "Zifeng An",
            "Zijing Xu",
            "Eric Fan",
            "Qi Cao"
        ],
        "abstract": "This paper presents a novel approach to human image colorization by\nfine-tuning the InstructPix2Pix model, which integrates a language model\n(GPT-3) with a text-to-image model (Stable Diffusion). Despite the original\nInstructPix2Pix model's proficiency in editing images based on textual\ninstructions, it exhibits limitations in the focused domain of colorization. To\naddress this, we fine-tuned the model using the IMDB-WIKI dataset, pairing\nblack-and-white images with a diverse set of colorization prompts generated by\nChatGPT. This paper contributes by (1) applying fine-tuning techniques to\nstable diffusion models specifically for colorization tasks, and (2) employing\ngenerative models to create varied conditioning prompts. After finetuning, our\nmodel outperforms the original InstructPix2Pix model on multiple metrics\nquantitatively, and we produce more realistically colored images qualitatively.\nThe code for this project is provided on the GitHub Repository\nhttps://github.com/AllenAnZifeng/DeepLearning282.",
        "date": "2023-12-08T01:36:49+00:00",
        "link": "http://arxiv.org/pdf/2312.04780v1"
    },
    {
        "title": "Remembering to Be Fair: On Non-Markovian Fairness in Sequential DecisionMaking (Preliminary Report)",
        "authors": [
            "Parand A. Alamdari",
            "Toryn Q. Klassen",
            "Elliot Creager",
            "Sheila A. McIlraith"
        ],
        "abstract": "Fair decision making has largely been studied with respect to a single\ndecision. In this paper we investigate the notion of fairness in the context of\nsequential decision making where multiple stakeholders can be affected by the\noutcomes of decisions, and where decision making may be informed by additional\nconstraints and criteria beyond the requirement of fairness. In this setting,\nwe observe that fairness often depends on the history of the sequential\ndecision-making process and not just on the current state. To advance our\nunderstanding of this class of fairness problems, we define the notion of\nnon-Markovian fairness in the context of sequential decision making. We\nidentify properties of non-Markovian fairness, including notions of long-term,\nanytime, periodic, and bounded fairness. We further explore the interplay\nbetween non-Markovian fairness and memory, and how this can support\nconstruction of fair policies in sequential decision-making settings.",
        "date": "2023-12-08T01:04:36+00:00",
        "link": "http://arxiv.org/pdf/2312.04772v1"
    },
    {
        "title": "The Graph Lottery Ticket Hypothesis: Finding Sparse, Informative Graph Structure",
        "authors": [
            "Anton Tsitsulin",
            "Bryan Perozzi"
        ],
        "abstract": "Graph learning methods help utilize implicit relationships among data items,\nthereby reducing training label requirements and improving task performance.\nHowever, determining the optimal graph structure for a particular learning task\nremains a challenging research problem.\n  In this work, we introduce the Graph Lottery Ticket (GLT) Hypothesis - that\nthere is an extremely sparse backbone for every graph, and that graph learning\nalgorithms attain comparable performance when trained on that subgraph as on\nthe full graph. We identify and systematically study 8 key metrics of interest\nthat directly influence the performance of graph learning algorithms.\nSubsequently, we define the notion of a \"winning ticket\" for graph structure -\nan extremely sparse subset of edges that can deliver a robust approximation of\nthe entire graph's performance. We propose a straightforward and efficient\nalgorithm for finding these GLTs in arbitrary graphs. Empirically, we observe\nthat performance of different graph learning algorithms can be matched or even\nexceeded on graphs with the average degree as low as 5.",
        "date": "2023-12-08T00:24:44+00:00",
        "link": "http://arxiv.org/pdf/2312.04762v1"
    },
    {
        "title": "Reconstructing Hands in 3D with Transformers",
        "authors": [
            "Georgios Pavlakos",
            "Dandan Shan",
            "Ilija Radosavovic",
            "Angjoo Kanazawa",
            "David Fouhey",
            "Jitendra Malik"
        ],
        "abstract": "We present an approach that can reconstruct hands in 3D from monocular input.\nOur approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based\narchitecture and can analyze hands with significantly increased accuracy and\nrobustness compared to previous work. The key to HaMeR's success lies in\nscaling up both the data used for training and the capacity of the deep network\nfor hand reconstruction. For training data, we combine multiple datasets that\ncontain 2D or 3D hand annotations. For the deep model, we use a large scale\nVision Transformer architecture. Our final model consistently outperforms the\nprevious baselines on popular 3D hand pose benchmarks. To further evaluate the\neffect of our design in non-controlled settings, we annotate existing\nin-the-wild datasets with 2D hand keypoint annotations. On this newly collected\ndataset of annotations, HInt, we demonstrate significant improvements over\nexisting baselines. We make our code, data and models available on the project\nwebsite: https://geopavlakos.github.io/hamer/.",
        "date": "2023-12-08T18:59:07+00:00",
        "link": "http://arxiv.org/pdf/2312.05251v1"
    },
    {
        "title": "Dynamic LiDAR Re-simulation using Compositional Neural Fields",
        "authors": [
            "Hanfeng Wu",
            "Xingxing Zuo",
            "Stefan Leutenegger",
            "Or Litany",
            "Konrad Schindler",
            "Shengyu Huang"
        ],
        "abstract": "We introduce DyNFL, a novel neural field-based approach for high-fidelity\nre-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDAR\nmeasurements from dynamic environments, accompanied by bounding boxes of moving\nobjects, to construct an editable neural field. This field, comprising\nseparately reconstructed static backgrounds and dynamic objects, allows users\nto modify viewpoints, adjust object positions, and seamlessly add or remove\nobjects in the re-simulated scene. A key innovation of our method is the neural\nfield composition technique, which effectively integrates reconstructed neural\nassets from various scenes through a ray drop test, accounting for occlusions\nand transparent surfaces. Our evaluation with both synthetic and real-world\nenvironments demonstrates that \\ShortName substantial improves dynamic scene\nsimulation based on LiDAR scans, offering a combination of physical fidelity\nand flexible editing capabilities.",
        "date": "2023-12-08T18:55:24+00:00",
        "link": "http://arxiv.org/pdf/2312.05247v1"
    },
    {
        "title": "SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation",
        "authors": [
            "Thuan Hoang Nguyen",
            "Anh Tran"
        ],
        "abstract": "Despite their ability to generate high-resolution and diverse images from\ntext prompts, text-to-image diffusion models often suffer from slow iterative\nsampling processes. Model distillation is one of the most effective directions\nto accelerate these models. However, previous distillation methods fail to\nretain the generation quality while requiring a significant amount of images\nfor training, either from real data or synthetically generated by the teacher\nmodel. In response to this limitation, we present a novel image-free\ndistillation scheme named $\\textbf{SwiftBrush}$. Drawing inspiration from\ntext-to-3D synthesis, in which a 3D neural radiance field that aligns with the\ninput prompt can be obtained from a 2D text-to-image diffusion prior via a\nspecialized loss without the use of any 3D data ground-truth, our approach\nre-purposes that same loss for distilling a pretrained multi-step text-to-image\nmodel to a student network that can generate high-fidelity images with just a\nsingle inference step. In spite of its simplicity, our model stands as one of\nthe first one-step text-to-image generators that can produce images of\ncomparable quality to Stable Diffusion without reliance on any training image\ndata. Remarkably, SwiftBrush achieves an FID score of $\\textbf{16.67}$ and a\nCLIP score of $\\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive\nresults or even substantially surpassing existing state-of-the-art distillation\ntechniques.",
        "date": "2023-12-08T18:44:09+00:00",
        "link": "http://arxiv.org/pdf/2312.05239v1"
    },
    {
        "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
        "authors": [
            "Zhiting Hu",
            "Tianmin Shu"
        ],
        "abstract": "Despite their tremendous success in many applications, large language models\noften fall short of consistent reasoning and planning in various (language,\nembodied, and social) scenarios, due to inherent limitations in their\ninference, learning, and modeling capabilities. In this position paper, we\npresent a new perspective of machine reasoning, LAW, that connects the concepts\nof Language models, Agent models, and World models, for more robust and\nversatile reasoning capabilities. In particular, we propose that world and\nagent models are a better abstraction of reasoning, that introduces the crucial\nelements of deliberate human-like reasoning, including beliefs about the world\nand other agents, anticipation of consequences, goals/rewards, and strategic\nplanning. Crucially, language models in LAW serve as a backend to implement the\nsystem or its elements and hence provide the computational power and\nadaptability. We review the recent studies that have made relevant progress and\ndiscuss future research directions towards operationalizing the LAW framework.",
        "date": "2023-12-08T18:25:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05230v1"
    },
    {
        "title": "Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration",
        "authors": [
            "Qi-Wei Wang",
            "Da-Wei Zhou",
            "Yi-Kai Zhang",
            "De-Chuan Zhan",
            "Han-Jia Ye"
        ],
        "abstract": "Real-world scenarios are usually accompanied by continuously appearing\nclasses with scare labeled samples, which require the machine learning model to\nincrementally learn new classes and maintain the knowledge of base classes. In\nthis Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methods\neither introduce extra learnable components or rely on a frozen feature\nextractor to mitigate catastrophic forgetting and overfitting problems.\nHowever, we find a tendency for existing methods to misclassify the samples of\nnew classes into base classes, which leads to the poor performance of new\nclasses. In other words, the strong discriminability of base classes distracts\nthe classification of new classes. To figure out this intriguing phenomenon, we\nobserve that although the feature extractor is only trained on base classes, it\ncan surprisingly represent the semantic similarity between the base and unseen\nnew classes. Building upon these analyses, we propose a simple yet effective\nTraining-frEE calibratioN (TEEN) strategy to enhance the discriminability of\nnew classes by fusing the new prototypes (i.e., mean features of a class) with\nweighted base prototypes. In addition to standard benchmarks in FSCIL, TEEN\ndemonstrates remarkable performance and consistent improvements over baseline\nmethods in the few-shot learning scenario. Code is available at:\nhttps://github.com/wangkiw/TEEN",
        "date": "2023-12-08T18:24:08+00:00",
        "link": "http://arxiv.org/pdf/2312.05229v1"
    },
    {
        "title": "Shape Matters: Detecting Vertebral Fractures Using Differentiable Point-Based Shape Decoding",
        "authors": [
            "Hellena Hempe",
            "Alexander Bigalke",
            "Mattias P. Heinrich"
        ],
        "abstract": "Degenerative spinal pathologies are highly prevalent among the elderly\npopulation. Timely diagnosis of osteoporotic fractures and other degenerative\ndeformities facilitates proactive measures to mitigate the risk of severe back\npain and disability. In this study, we specifically explore the use of shape\nauto-encoders for vertebrae, taking advantage of advancements in automated\nmulti-label segmentation and the availability of large datasets for\nunsupervised learning. Our shape auto-encoders are trained on a large set of\nvertebrae surface patches, leveraging the vast amount of available data for\nvertebra segmentation. This addresses the label scarcity problem faced when\nlearning shape information of vertebrae from image intensities. Based on the\nlearned shape features we train an MLP to detect vertebral body fractures.\nUsing segmentation masks that were automatically generated using the\nTotalSegmentator, our proposed method achieves an AUC of 0.901 on the VerSe19\ntestset. This outperforms image-based and surface-based end-to-end trained\nmodels. Additionally, our results demonstrate that pre-training the models in\nan unsupervised manner enhances geometric methods like PointNet and DGCNN. Our\nfindings emphasise the advantages of explicitly learning shape features for\ndiagnosing osteoporotic vertebrae fractures. This approach improves the\nreliability of classification results and reduces the need for annotated\nlabels. This study provides novel insights into the effectiveness of various\nencoder-decoder models for shape analysis of vertebrae and proposes a new\ndecoder architecture: the point-based shape decoder.",
        "date": "2023-12-08T18:11:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05220v1"
    },
    {
        "title": "Enhancing Facial Classification and Recognition using 3D Facial Models and Deep Learning",
        "authors": [
            "Houting Li",
            "Mengxuan Dong",
            "Lok Ming Lui"
        ],
        "abstract": "Accurate analysis and classification of facial attributes are essential in\nvarious applications, from human-computer interaction to security systems. In\nthis work, a novel approach to enhance facial classification and recognition\ntasks through the integration of 3D facial models with deep learning methods\nwas proposed. We extract the most useful information for various tasks using\nthe 3D Facial Model, leading to improved classification accuracy. Combining 3D\nfacial insights with ResNet architecture, our approach achieves notable\nresults: 100% individual classification, 95.4% gender classification, and 83.5%\nexpression classification accuracy. This method holds promise for advancing\nfacial analysis and recognition research.",
        "date": "2023-12-08T18:09:29+00:00",
        "link": "http://arxiv.org/pdf/2312.05219v1"
    },
    {
        "title": "IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing",
        "authors": [
            "Shaofei Wang",
            "Božidar Antić",
            "Andreas Geiger",
            "Siyu Tang"
        ],
        "abstract": "We present IntrinsicAvatar, a novel approach to recovering the intrinsic\nproperties of clothed human avatars including geometry, albedo, material, and\nenvironment lighting from only monocular videos. Recent advancements in\nhuman-based neural rendering have enabled high-quality geometry and appearance\nreconstruction of clothed humans from just monocular videos. However, these\nmethods bake intrinsic properties such as albedo, material, and environment\nlighting into a single entangled neural representation. On the other hand, only\na handful of works tackle the problem of estimating geometry and disentangled\nappearance properties of clothed humans from monocular videos. They usually\nachieve limited quality and disentanglement due to approximations of secondary\nshading effects via learned MLPs. In this work, we propose to model secondary\nshading effects explicitly via Monte-Carlo ray tracing. We model the rendering\nprocess of clothed humans as a volumetric scattering process, and combine ray\ntracing with body articulation. Our approach can recover high-quality geometry,\nalbedo, material, and lighting properties of clothed humans from a single\nmonocular video, without requiring supervised pre-training using ground truth\nmaterials. Furthermore, since we explicitly model the volumetric scattering\nprocess and ray tracing, our model naturally generalizes to novel poses,\nenabling animation of the reconstructed avatar in novel lighting conditions.",
        "date": "2023-12-08T17:58:14+00:00",
        "link": "http://arxiv.org/pdf/2312.05210v1"
    },
    {
        "title": "ControlRoom3D: Room Generation using Semantic Proxy Rooms",
        "authors": [
            "Jonas Schult",
            "Sam Tsai",
            "Lukas Höllein",
            "Bichen Wu",
            "Jialiang Wang",
            "Chih-Yao Ma",
            "Kunpeng Li",
            "Xiaofang Wang",
            "Felix Wimbauer",
            "Zijian He",
            "Peizhao Zhang",
            "Bastian Leibe",
            "Peter Vajda",
            "Ji Hou"
        ],
        "abstract": "Manually creating 3D environments for AR/VR applications is a complex process\nrequiring expert knowledge in 3D modeling software. Pioneering works facilitate\nthis process by generating room meshes conditioned on textual style\ndescriptions. Yet, many of these automatically generated 3D meshes do not\nadhere to typical room layouts, compromising their plausibility, e.g., by\nplacing several beds in one bedroom. To address these challenges, we present\nControlRoom3D, a novel method to generate high-quality room meshes. Central to\nour approach is a user-defined 3D semantic proxy room that outlines a rough\nroom layout based on semantic bounding boxes and a textual description of the\noverall room style. Our key insight is that when rendered to 2D, this 3D\nrepresentation provides valuable geometric and semantic information to control\npowerful 2D models to generate 3D consistent textures and geometry that aligns\nwell with the proxy room. Backed up by an extensive study including\nquantitative metrics and qualitative user evaluations, our method generates\ndiverse and globally plausible 3D room meshes, thus empowering users to design\n3D rooms effortlessly without specialized knowledge.",
        "date": "2023-12-08T17:55:44+00:00",
        "link": "http://arxiv.org/pdf/2312.05208v1"
    },
    {
        "title": "Fine Dense Alignment of Image Bursts through Camera Pose and Depth Estimation",
        "authors": [
            "Bruno Lecouat",
            "Yann Dubois de Mont-Marin",
            "Théo Bodrito",
            "Julien Mairal",
            "Jean Ponce"
        ],
        "abstract": "This paper introduces a novel approach to the fine alignment of images in a\nburst captured by a handheld camera. In contrast to traditional techniques that\nestimate two-dimensional transformations between frame pairs or rely on\ndiscrete correspondences, the proposed algorithm establishes dense\ncorrespondences by optimizing both the camera motion and surface depth and\norientation at every pixel. This approach improves alignment, particularly in\nscenarios with parallax challenges. Extensive experiments with synthetic bursts\nfeaturing small and even tiny baselines demonstrate that it outperforms the\nbest optical flow methods available today in this setting, without requiring\nany training. Beyond enhanced alignment, our method opens avenues for tasks\nbeyond simple image restoration, such as depth estimation and 3D\nreconstruction, as supported by promising preliminary results. This positions\nour approach as a versatile tool for various burst image processing\napplications.",
        "date": "2023-12-08T17:22:04+00:00",
        "link": "http://arxiv.org/pdf/2312.05190v1"
    },
    {
        "title": "Video-Based Rendering Techniques: A Survey",
        "authors": [
            "Rafael Kuffner dos Anjos",
            "João Madeiras Pereira",
            "José Antonio Gaspar"
        ],
        "abstract": "Three-dimensional reconstruction of events recorded on images has been a\ncommon challenge between computer vision and computer graphics for a long time.\nEstimating the real position of objects and surfaces using vision as an input\nis no trivial task and has been approached in several different ways. Although\nhuge progress has been made so far, there are several open issues to which an\nanswer is needed. The use of videos as an input for a rendering process\n(video-based rendering, VBR) is something that recently has been started to be\nlooked upon and has added many other challenges and also solutions to the\nclassical image-based rendering issue (IBR). This article presents the state of\nart on video-based rendering and image-based techniques that can be applied on\nthis scenario, evaluating the open issues yet to be solved, indicating where\nfuture work should be focused.",
        "date": "2023-12-08T17:03:35+00:00",
        "link": "http://arxiv.org/pdf/2312.05179v1"
    },
    {
        "title": "MRI Scan Synthesis Methods based on Clustering and Pix2Pix",
        "authors": [
            "Giulia Baldini",
            "Melanie Schmidt",
            "Charlotte Zäske",
            "Liliana L. Caldeira"
        ],
        "abstract": "We consider a missing data problem in the context of automatic segmentation\nmethods for Magnetic Resonance Imaging (MRI) brain scans. Usually, automated\nMRI scan segmentation is based on multiple scans (e.g., T1-weighted,\nT2-weighted, T1CE, FLAIR). However, quite often a scan is blurry, missing or\notherwise unusable. We investigate the question whether a missing scan can be\nsynthesized. We exemplify that this is in principle possible by synthesizing a\nT2-weighted scan from a given T1-weighted scan. Our first aim is to compute a\npicture that resembles the missing scan closely, measured by average mean\nsquared error (MSE). We develop/use several methods for this, including a\nrandom baseline approach, a clustering-based method and pixel-to-pixel\ntranslation method by (Pix2Pix) which is based on conditional GANs. The lowest\nMSE is achieved by our clustering-based method. Our second aim is to compare\nthe methods with respect to the affect that using the synthesized scan has on\nthe segmentation process. For this, we use a DeepMedic model trained with the\nfour input scan modalities named above. We replace the T2-weighted scan by the\nsynthesized picture and evaluate the segmentations with respect to the tumor\nidentification, using Dice scores as numerical evaluation. The evaluation shows\nthat the segmentation works well with synthesized scans (in particular, with\nPix2Pix methods) in many cases.",
        "date": "2023-12-08T16:59:17+00:00",
        "link": "http://arxiv.org/pdf/2312.05176v1"
    },
    {
        "title": "TriHuman : A Real-time and Controllable Tri-plane Representation for Detailed Human Geometry and Appearance Synthesis",
        "authors": [
            "Heming Zhu",
            "Fangneng Zhan",
            "Christian Theobalt",
            "Marc Habermann"
        ],
        "abstract": "Creating controllable, photorealistic, and geometrically detailed digital\ndoubles of real humans solely from video data is a key challenge in Computer\nGraphics and Vision, especially when real-time performance is required. Recent\nmethods attach a neural radiance field (NeRF) to an articulated structure,\ne.g., a body model or a skeleton, to map points into a pose canonical space\nwhile conditioning the NeRF on the skeletal pose. These approaches typically\nparameterize the neural field with a multi-layer perceptron (MLP) leading to a\nslow runtime. To address this drawback, we propose TriHuman a novel\nhuman-tailored, deformable, and efficient tri-plane representation, which\nachieves real-time performance, state-of-the-art pose-controllable geometry\nsynthesis as well as photorealistic rendering quality. At the core, we\nnon-rigidly warp global ray samples into our undeformed tri-plane texture\nspace, which effectively addresses the problem of global points being mapped to\nthe same tri-plane locations. We then show how such a tri-plane feature\nrepresentation can be conditioned on the skeletal motion to account for dynamic\nappearance and geometry changes. Our results demonstrate a clear step towards\nhigher quality in terms of geometry and appearance modeling of humans as well\nas runtime performance.",
        "date": "2023-12-08T16:40:38+00:00",
        "link": "http://arxiv.org/pdf/2312.05161v1"
    },
    {
        "title": "Shape-aware Segmentation of the Placenta in BOLD Fetal MRI Time Series",
        "authors": [
            "S. Mazdak Abulnaga",
            "Neel Dey",
            "Sean I. Young",
            "Eileen Pan",
            "Katherine I. Hobgood",
            "Clinton J. Wang",
            "P. Ellen Grant",
            "Esra Abaci Turk",
            "Polina Golland"
        ],
        "abstract": "Blood oxygen level dependent (BOLD) MRI time series with maternal hyperoxia\ncan assess placental oxygenation and function. Measuring precise BOLD changes\nin the placenta requires accurate temporal placental segmentation and is\nconfounded by fetal and maternal motion, contractions, and hyperoxia-induced\nintensity changes. Current BOLD placenta segmentation methods warp a manually\nannotated subject-specific template to the entire time series. However, as the\nplacenta is a thin, elongated, and highly non-rigid organ subject to large\ndeformations and obfuscated edges, existing work cannot accurately segment the\nplacental shape, especially near boundaries. In this work, we propose a machine\nlearning segmentation framework for placental BOLD MRI and apply it to\nsegmenting each volume in a time series. We use a placental-boundary weighted\nloss formulation and perform a comprehensive evaluation across several popular\nsegmentation objectives. Our model is trained and tested on a cohort of 91\nsubjects containing healthy fetuses, fetuses with fetal growth restriction, and\nmothers with high BMI. Biomedically, our model performs reliably in segmenting\nvolumes in both normoxic and hyperoxic points in the BOLD time series. We\nfurther find that boundary-weighting increases placental segmentation\nperformance by 8.3% and 6.0% Dice coefficient for the cross-entropy and signed\ndistance transform objectives, respectively. Our code and trained model is\navailable at https://github.com/mabulnaga/automatic-placenta-segmentation.",
        "date": "2023-12-08T16:29:10+00:00",
        "link": "http://arxiv.org/pdf/2312.05148v1"
    },
    {
        "title": "Open Domain Generalization with a Single Network by Regularization Exploiting Pre-trained Features",
        "authors": [
            "Inseop Chung",
            "KiYoon Yoo",
            "Nojun Kwak"
        ],
        "abstract": "Open Domain Generalization (ODG) is a challenging task as it not only deals\nwith distribution shifts but also category shifts between the source and target\ndatasets. To handle this task, the model has to learn a generalizable\nrepresentation that can be applied to unseen domains while also identify\nunknown classes that were not present during training. Previous work has used\nmultiple source-specific networks, which involve a high computation cost.\nTherefore, this paper proposes a method that can handle ODG using only a single\nnetwork. The proposed method utilizes a head that is pre-trained by\nlinear-probing and employs two regularization terms, each targeting the\nregularization of feature extractor and the classification head, respectively.\nThe two regularization terms fully utilize the pre-trained features and\ncollaborate to modify the head of the model without excessively altering the\nfeature extractor. This ensures a smoother softmax output and prevents the\nmodel from being biased towards the source domains. The proposed method shows\nimproved adaptability to unseen domains and increased capability to detect\nunseen classes as well. Extensive experiments show that our method achieves\ncompetitive performance in several benchmarks. We also justify our method with\ncareful analysis of the effect on the logits, features, and the head.",
        "date": "2023-12-08T16:22:10+00:00",
        "link": "http://arxiv.org/pdf/2312.05141v1"
    },
    {
        "title": "GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization",
        "authors": [
            "Yahao Shi",
            "Yanmin Wu",
            "Chenming Wu",
            "Xing Liu",
            "Chen Zhao",
            "Haocheng Feng",
            "Jingtuo Liu",
            "Liangjun Zhang",
            "Jian Zhang",
            "Bin Zhou",
            "Errui Ding",
            "Jingdong Wang"
        ],
        "abstract": "This paper presents GIR, a 3D Gaussian Inverse Rendering method for\nrelightable scene factorization. Compared to existing methods leveraging\ndiscrete meshes or neural implicit fields for inverse rendering, our method\nutilizes 3D Gaussians to estimate the material properties, illumination, and\ngeometry of an object from multi-view images. Our study is motivated by the\nevidence showing that 3D Gaussian is a more promising backbone than neural\nfields in terms of performance, versatility, and efficiency. In this paper, we\naim to answer the question: ``How can 3D Gaussian be applied to improve the\nperformance of inverse rendering?'' To address the complexity of estimating\nnormals based on discrete and often in-homogeneous distributed 3D Gaussian\nrepresentations, we proposed an efficient self-regularization method that\nfacilitates the modeling of surface normals without the need for additional\nsupervision. To reconstruct indirect illumination, we propose an approach that\nsimulates ray tracing. Extensive experiments demonstrate our proposed GIR's\nsuperior performance over existing methods across multiple tasks on a variety\nof widely used datasets in inverse rendering. This substantiates its efficacy\nand broad applicability, highlighting its potential as an influential tool in\nrelighting and reconstruction. Project page: https://3dgir.github.io",
        "date": "2023-12-08T16:05:15+00:00",
        "link": "http://arxiv.org/pdf/2312.05133v1"
    },
    {
        "title": "Quantifying white matter hyperintensity and brain volumes in heterogeneous clinical and low-field portable MRI",
        "authors": [
            "Pablo Laso",
            "Stefano Cerri",
            "Annabel Sorby-Adams",
            "Jennifer Guo",
            "Farrah Mateen",
            "Philipp Goebl",
            "Jiaming Wu",
            "Peirong Liu",
            "Hongwei Li",
            "Sean I. Young",
            "Benjamin Billot",
            "Oula Puonti",
            "Gordon Sze",
            "Sam Payabavash",
            "Adam DeHavenon",
            "Kevin N. Sheth",
            "Matthew S. Rosen",
            "John Kirsch",
            "Nicola Strisciuglio",
            "Jelmer M. Wolterink",
            "Arman Eshaghi",
            "Frederik Barkhof",
            "W. Taylor Kimberly",
            "Juan Eugenio Iglesias"
        ],
        "abstract": "Brain atrophy and white matter hyperintensity (WMH) are critical neuroimaging\nfeatures for ascertaining brain injury in cerebrovascular disease and multiple\nsclerosis. Automated segmentation and quantification is desirable but existing\nmethods require high-resolution MRI with good signal-to-noise ratio (SNR). This\nprecludes application to clinical and low-field portable MRI (pMRI) scans, thus\nhampering large-scale tracking of atrophy and WMH progression, especially in\nunderserved areas where pMRI has huge potential. Here we present a method that\nsegments white matter hyperintensity and 36 brain regions from scans of any\nresolution and contrast (including pMRI) without retraining. We show results on\nsix public datasets and on a private dataset with paired high- and low-field\nscans (3T and 64mT), where we attain strong correlation between the WMH\n($\\rho$=.85) and hippocampal volumes (r=.89) estimated at both fields. Our\nmethod is publicly available as part of FreeSurfer, at:\nhttp://surfer.nmr.mgh.harvard.edu/fswiki/WMH-SynthSeg.",
        "date": "2023-12-08T15:47:50+00:00",
        "link": "http://arxiv.org/pdf/2312.05119v1"
    },
    {
        "title": "DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models",
        "authors": [
            "Mengyang Feng",
            "Jinlin Liu",
            "Kai Yu",
            "Yuan Yao",
            "Zheng Hui",
            "Xiefan Guo",
            "Xianhui Lin",
            "Haolan Xue",
            "Chen Shi",
            "Xiaowen Li",
            "Aojie Li",
            "Miaomiao Cui",
            "Peiran Ren",
            "Xuansong Xie"
        ],
        "abstract": "In this paper, we present DreaMoving, a diffusion-based controllable video\ngeneration framework to produce high-quality customized human dance videos.\nSpecifically, given target identity and posture sequences, DreaMoving can\ngenerate a video of the target identity dancing anywhere driven by the posture\nsequences. To this end, we propose a Video ControlNet for motion-controlling\nand a Content Guider for identity preserving. The proposed model is easy to use\nand can be adapted to most stylized diffusion models to generate diverse\nresults. The project page is available at\nhttps://dreamoving.github.io/dreamoving.",
        "date": "2023-12-08T15:37:17+00:00",
        "link": "http://arxiv.org/pdf/2312.05107v1"
    },
    {
        "title": "Continual learning for surface defect segmentation by subnetwork creation and selection",
        "authors": [
            "Aleksandr Dekhovich",
            "Miguel A. Bessa"
        ],
        "abstract": "We introduce a new continual (or lifelong) learning algorithm called LDA-CP&S\nthat performs segmentation tasks without undergoing catastrophic forgetting.\nThe method is applied to two different surface defect segmentation problems\nthat are learned incrementally, i.e. providing data about one type of defect at\na time, while still being capable of predicting every defect that was seen\npreviously. Our method creates a defect-related subnetwork for each defect type\nvia iterative pruning and trains a classifier based on linear discriminant\nanalysis (LDA). At the inference stage, we first predict the defect type with\nLDA and then predict the surface defects using the selected subnetwork. We\ncompare our method with other continual learning methods showing a significant\nimprovement -- mean Intersection over Union better by a factor of two when\ncompared to existing methods on both datasets. Importantly, our approach shows\ncomparable results with joint training when all the training data (all defects)\nare seen simultaneously",
        "date": "2023-12-08T15:28:50+00:00",
        "link": "http://arxiv.org/pdf/2312.05100v1"
    },
    {
        "title": "I Can't Believe It's Not Better: In-air Movement For Alzheimer Handwriting Synthetic Generation",
        "authors": [
            "Asma Bensalah",
            "Antonio Parziale",
            "Giuseppe De Gregorio",
            "Angelo Marcelli",
            "Alicia Fornés",
            "Lladós"
        ],
        "abstract": "During recent years, there here has been a boom in terms of deep learning use\nfor handwriting analysis and recognition. One main application for handwriting\nanalysis is early detection and diagnosis in the health field. Unfortunately,\nmost real case problems still suffer a scarcity of data, which makes difficult\nthe use of deep learning-based models. To alleviate this problem, some works\nresort to synthetic data generation. Lately, more works are directed towards\nguided data synthetic generation, a generation that uses the domain and data\nknowledge to generate realistic data that can be useful to train deep learning\nmodels. In this work, we combine the domain knowledge about the Alzheimer's\ndisease for handwriting and use it for a more guided data generation.\nConcretely, we have explored the use of in-air movements for synthetic data\ngeneration.",
        "date": "2023-12-08T15:14:41+00:00",
        "link": "http://arxiv.org/pdf/2312.05086v1"
    },
    {
        "title": "MuVieCAST: Multi-View Consistent Artistic Style Transfer",
        "authors": [
            "Nail Ibrahimli",
            "Julian F. P. Kooij",
            "Liangliang Nan"
        ],
        "abstract": "We introduce MuVieCAST, a modular multi-view consistent style transfer\nnetwork architecture that enables consistent style transfer between multiple\nviewpoints of the same scene. This network architecture supports both sparse\nand dense views, making it versatile enough to handle a wide range of\nmulti-view image datasets. The approach consists of three modules that perform\nspecific tasks related to style transfer, namely content preservation, image\ntransformation, and multi-view consistency enforcement. We extensively evaluate\nour approach across multiple application domains including depth-map-based\npoint cloud fusion, mesh reconstruction, and novel-view synthesis. Our\nexperiments reveal that the proposed framework achieves an exceptional\ngeneration of stylized images, exhibiting consistent outcomes across\nperspectives. A user study focusing on novel-view synthesis further confirms\nthese results, with approximately 68\\% of cases participants expressing a\npreference for our generated outputs compared to the recent state-of-the-art\nmethod. Our modular framework is extensible and can easily be integrated with\nvarious backbone architectures, making it a flexible solution for multi-view\nstyle transfer. More results are demonstrated on our project page:\nmuviecast.github.io",
        "date": "2023-12-08T14:01:03+00:00",
        "link": "http://arxiv.org/pdf/2312.05046v1"
    },
    {
        "title": "SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control",
        "authors": [
            "Jaskirat Singh",
            "Jianming Zhang",
            "Qing Liu",
            "Cameron Smith",
            "Zhe Lin",
            "Liang Zheng"
        ],
        "abstract": "The field of generative image inpainting and object insertion has made\nsignificant progress with the recent advent of latent diffusion models.\nUtilizing a precise object mask can greatly enhance these applications.\nHowever, due to the challenges users encounter in creating high-fidelity masks,\nthere is a tendency for these methods to rely on more coarse masks (e.g.,\nbounding box) for these applications. This results in limited control and\ncompromised background content preservation. To overcome these limitations, we\nintroduce SmartMask, which allows any novice user to create detailed masks for\nprecise object insertion. Combined with a ControlNet-Inpaint model, our\nexperiments demonstrate that SmartMask achieves superior object insertion\nquality, preserving the background content more effectively than previous\nmethods. Notably, unlike prior works the proposed approach can also be used\neven without user-mask guidance, which allows it to perform mask-free object\ninsertion at diverse positions and scales. Furthermore, we find that when used\niteratively with a novel instruction-tuning based planning model, SmartMask can\nbe used to design detailed layouts from scratch. As compared with user-scribble\nbased layout design, we observe that SmartMask allows for better quality\noutputs with layout-to-image generation methods. Project page is available at\nhttps://smartmask-gen.github.io",
        "date": "2023-12-08T13:38:22+00:00",
        "link": "http://arxiv.org/pdf/2312.05039v1"
    },
    {
        "title": "Prompt-In-Prompt Learning for Universal Image Restoration",
        "authors": [
            "Zilong Li",
            "Yiming Lei",
            "Chenglong Ma",
            "Junping Zhang",
            "Hongming Shan"
        ],
        "abstract": "Image restoration, which aims to retrieve and enhance degraded images, is\nfundamental across a wide range of applications. While conventional deep\nlearning approaches have notably improved the image quality across various\ntasks, they still suffer from (i) the high storage cost needed for various\ntask-specific models and (ii) the lack of interactivity and flexibility,\nhindering their wider application. Drawing inspiration from the pronounced\nsuccess of prompts in both linguistic and visual domains, we propose novel\nPrompt-In-Prompt learning for universal image restoration, named PIP. First, we\npresent two novel prompts, a degradation-aware prompt to encode high-level\ndegradation knowledge and a basic restoration prompt to provide essential\nlow-level information. Second, we devise a novel prompt-to-prompt interaction\nmodule to fuse these two prompts into a universal restoration prompt. Third, we\nintroduce a selective prompt-to-feature interaction module to modulate the\ndegradation-related feature. By doing so, the resultant PIP works as a\nplug-and-play module to enhance existing restoration models for universal image\nrestoration. Extensive experimental results demonstrate the superior\nperformance of PIP on multiple restoration tasks, including image denoising,\nderaining, dehazing, deblurring, and low-light enhancement. Remarkably, PIP is\ninterpretable, flexible, efficient, and easy-to-use, showing promising\npotential for real-world applications. The code is available at\nhttps://github.com/longzilicart/pip_universal.",
        "date": "2023-12-08T13:36:01+00:00",
        "link": "http://arxiv.org/pdf/2312.05038v1"
    },
    {
        "title": "Synthesizing Traffic Datasets using Graph Neural Networks",
        "authors": [
            "Daniel Rodriguez-Criado",
            "Maria Chli",
            "Luis J. Manso",
            "George Vogiatzis"
        ],
        "abstract": "Traffic congestion in urban areas presents significant challenges, and\nIntelligent Transportation Systems (ITS) have sought to address these via\nautomated and adaptive controls. However, these systems often struggle to\ntransfer simulated experiences to real-world scenarios. This paper introduces a\nnovel methodology for bridging this `sim-real' gap by creating photorealistic\nimages from 2D traffic simulations and recorded junction footage. We propose a\nnovel image generation approach, integrating a Conditional Generative\nAdversarial Network with a Graph Neural Network (GNN) to facilitate the\ncreation of realistic urban traffic images. We harness GNNs' ability to process\ninformation at different levels of abstraction alongside segmented images for\npreserving locality data. The presented architecture leverages the power of\nSPADE and Graph ATtention (GAT) network models to create images based on\nsimulated traffic scenarios. These images are conditioned by factors such as\nentity positions, colors, and time of day. The uniqueness of our approach lies\nin its ability to effectively translate structured and human-readable\nconditions, encoded as graphs, into realistic images. This advancement\ncontributes to applications requiring rich traffic image datasets, from data\naugmentation to urban traffic solutions. We further provide an application to\ntest the model's capabilities, including generating images with manually\ndefined positions for various entities.",
        "date": "2023-12-08T13:24:19+00:00",
        "link": "http://arxiv.org/pdf/2312.05031v1"
    },
    {
        "title": "Cluster images with AntClust: a clustering algorithm based on the chemical recognition system of ants",
        "authors": [
            "Winfried Gero Oed",
            "Parisa Memarmoshrefi"
        ],
        "abstract": "We implement AntClust, a clustering algorithm based on the chemical\nrecognition system of ants and use it to cluster images of cars. We will give a\nshort recap summary of the main working principles of the algorithm as devised\nby the original paper [1]. Further, we will describe how to define a similarity\nfunction for images and how the implementation is used to cluster images of\ncars from the vehicle re-identification data set. We then test the clustering\nperformance of AntClust against DBSCAN, HDBSCAN and OPTICS. Finally one of the\ncore parts in AntClust, the rule set can be easily redefined with our\nimplementation, enabling a way for other bio-inspired algorithms to find rules\nin an automated process. The implementation can be found on GitLab [9].",
        "date": "2023-12-08T13:14:03+00:00",
        "link": "http://arxiv.org/pdf/2312.05028v1"
    },
    {
        "title": "A Unified Framework for Unsupervised Domain Adaptation based on Instance Weighting",
        "authors": [
            "Jinjing Zhu",
            "Feiyang Ye",
            "Qiao Xiao",
            "Pengxin Guo",
            "Yu Zhang",
            "Qiang Yang"
        ],
        "abstract": "Despite the progress made in domain adaptation, solving Unsupervised Domain\nAdaptation (UDA) problems with a general method under complex conditions caused\nby label shifts between domains remains a formidable task. In this work, we\ncomprehensively investigate four distinct UDA settings including closed set\ndomain adaptation, partial domain adaptation, open set domain adaptation, and\nuniversal domain adaptation, where shared common classes between source and\ntarget domains coexist alongside domain-specific private classes. The prominent\nchallenges inherent in diverse UDA settings center around the discrimination of\ncommon/private classes and the precise measurement of domain discrepancy. To\nsurmount these challenges effectively, we propose a novel yet effective method\ncalled Learning Instance Weighting for Unsupervised Domain Adaptation (LIWUDA),\nwhich caters to various UDA settings. Specifically, the proposed LIWUDA method\nconstructs a weight network to assign weights to each instance based on its\nprobability of belonging to common classes, and designs Weighted Optimal\nTransport (WOT) for domain alignment by leveraging instance weights.\nAdditionally, the proposed LIWUDA method devises a Separate and Align (SA) loss\nto separate instances with low similarities and align instances with high\nsimilarities. To guide the learning of the weight network, Intra-domain Optimal\nTransport (IOT) is proposed to enforce the weights of instances in common\nclasses to follow a uniform distribution. Through the integration of those\nthree components, the proposed LIWUDA method demonstrates its capability to\naddress all four UDA settings in a unified manner. Experimental evaluations\nconducted on three benchmark datasets substantiate the effectiveness of the\nproposed LIWUDA method.",
        "date": "2023-12-08T13:04:55+00:00",
        "link": "http://arxiv.org/pdf/2312.05024v1"
    },
    {
        "title": "Decoupling Degradation and Content Processing for Adverse Weather Image Restoration",
        "authors": [
            "Xi Wang",
            "Xueyang Fu",
            "Peng-Tao Jiang",
            "Jie Huang",
            "Mi Zhou",
            "Bo Li",
            "Zheng-Jun Zha"
        ],
        "abstract": "Adverse weather image restoration strives to recover clear images from those\naffected by various weather types, such as rain, haze, and snow. Each weather\ntype calls for a tailored degradation removal approach due to its unique impact\non images. Conversely, content reconstruction can employ a uniform approach, as\nthe underlying image content remains consistent. Although previous techniques\ncan handle multiple weather types within a single network, they neglect the\ncrucial distinction between these two processes, limiting the quality of\nrestored images. This work introduces a novel adverse weather image restoration\nmethod, called DDCNet, which decouples the degradation removal and content\nreconstruction process at the feature level based on their channel statistics.\nSpecifically, we exploit the unique advantages of the Fourier transform in both\nthese two processes: (1) the degradation information is mainly located in the\namplitude component of the Fourier domain, and (2) the Fourier domain contains\nglobal information. The former facilitates channel-dependent degradation\nremoval operation, allowing the network to tailor responses to various adverse\nweather types; the latter, by integrating Fourier's global properties into\nchannel-independent content features, enhances network capacity for consistent\nglobal content reconstruction. We further augment the degradation removal\nprocess with a degradation mapping loss function. Extensive experiments\ndemonstrate our method achieves state-of-the-art performance in multiple\nadverse weather removal benchmarks.",
        "date": "2023-12-08T12:26:38+00:00",
        "link": "http://arxiv.org/pdf/2312.05006v1"
    },
    {
        "title": "MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness",
        "authors": [
            "Xiaoyun Xu",
            "Shujian Yu",
            "Jingzheng Wu",
            "Stjepan Picek"
        ],
        "abstract": "Vision Transformers (ViTs) achieve superior performance on various tasks\ncompared to convolutional neural networks (CNNs), but ViTs are also vulnerable\nto adversarial attacks. Adversarial training is one of the most successful\nmethods to build robust CNN models. Thus, recent works explored new\nmethodologies for adversarial training of ViTs based on the differences between\nViTs and CNNs, such as better training strategies, preventing attention from\nfocusing on a single block, or discarding low-attention embeddings. However,\nthese methods still follow the design of traditional supervised adversarial\ntraining, limiting the potential of adversarial training on ViTs. This paper\nproposes a novel defense method, MIMIR, which aims to build a different\nadversarial training methodology by utilizing Masked Image Modeling at\npre-training. We create an autoencoder that accepts adversarial examples as\ninput but takes the clean examples as the modeling target. Then, we create a\nmutual information (MI) penalty following the idea of the Information\nBottleneck. Among the two information source inputs and corresponding\nadversarial perturbation, the perturbation information is eliminated due to the\nconstraint of the modeling target. Next, we provide a theoretical analysis of\nMIMIR using the bounds of the MI penalty. We also design two adaptive attacks\nwhen the adversary is aware of the MIMIR defense and show that MIMIR still\nperforms well. The experimental results show that MIMIR improves (natural and\nadversarial) accuracy on average by 4.19\\% on CIFAR-10 and 5.52\\% on\nImageNet-1K, compared to baselines. On Tiny-ImageNet, we obtained improved\nnatural accuracy of 2.99\\% on average and comparable adversarial accuracy. Our\ncode and trained models are publicly\navailable\\footnote{\\url{https://anonymous.4open.science/r/MIMIR-5444/README.md}}.",
        "date": "2023-12-08T10:50:02+00:00",
        "link": "http://arxiv.org/pdf/2312.04960v1"
    },
    {
        "title": "Scientific Preparation for CSST: Classification of Galaxy and Nebula/Star Cluster Based on Deep Learning",
        "authors": [
            "Yuquan Zhang",
            "Zhong Cao",
            "Feng Wang",
            "Lam",
            "Man I",
            "Hui Deng",
            "Ying Mei",
            "Lei Tan"
        ],
        "abstract": "The Chinese Space Station Telescope (abbreviated as CSST) is a future\nadvanced space telescope. Real-time identification of galaxy and nebula/star\ncluster (abbreviated as NSC) images is of great value during CSST survey. While\nrecent research on celestial object recognition has progressed, the rapid and\nefficient identification of high-resolution local celestial images remains\nchallenging. In this study, we conducted galaxy and NSC image classification\nresearch using deep learning methods based on data from the Hubble Space\nTelescope. We built a Local Celestial Image Dataset and designed a deep\nlearning model named HR-CelestialNet for classifying images of the galaxy and\nNSC. HR-CelestialNet achieved an accuracy of 89.09% on the testing set,\noutperforming models such as AlexNet, VGGNet and ResNet, while demonstrating\nfaster recognition speeds. Furthermore, we investigated the factors influencing\nCSST image quality and evaluated the generalization ability of HR-CelestialNet\non the blurry image dataset, demonstrating its robustness to low image quality.\nThe proposed method can enable real-time identification of celestial images\nduring CSST survey mission.",
        "date": "2023-12-08T10:27:40+00:00",
        "link": "http://arxiv.org/pdf/2312.04948v1"
    },
    {
        "title": "Benchmarking and Analysis of Unsupervised Object Segmentation from Real-world Single Images",
        "authors": [
            "Yafei Yang",
            "Bo Yang"
        ],
        "abstract": "In this paper, we study the problem of unsupervised object segmentation from\nsingle images. We do not introduce a new algorithm, but systematically\ninvestigate the effectiveness of existing unsupervised models on challenging\nreal-world images. We first introduce seven complexity factors to\nquantitatively measure the distributions of background and foreground object\nbiases in appearance and geometry for datasets with human annotations. With the\naid of these factors, we empirically find that, not surprisingly, existing\nunsupervised models fail to segment generic objects in real-world images,\nalthough they can easily achieve excellent performance on numerous simple\nsynthetic datasets, due to the vast gap in objectness biases between synthetic\nand real images. By conducting extensive experiments on multiple groups of\nablated real-world datasets, we ultimately find that the key factors underlying\nthe failure of existing unsupervised models on real-world images are the\nchallenging distributions of background and foreground object biases in\nappearance and geometry. Because of this, the inductive biases introduced in\nexisting unsupervised models can hardly capture the diverse object\ndistributions. Our research results suggest that future work should exploit\nmore explicit objectness biases in the network design.",
        "date": "2023-12-08T10:25:59+00:00",
        "link": "http://arxiv.org/pdf/2312.04947v1"
    },
    {
        "title": "Retrieval-based Video Language Model for Efficient Long Video Question Answering",
        "authors": [
            "Jiaqi Xu",
            "Cuiling Lan",
            "Wenxuan Xie",
            "Xuejin Chen",
            "Yan Lu"
        ],
        "abstract": "The remarkable natural language understanding, reasoning, and generation\ncapabilities of large language models (LLMs) have made them attractive for\napplication to video question answering (Video QA) tasks, utilizing video\ntokens as contextual input. However, employing LLMs for long video\nunderstanding presents significant challenges and remains under-explored. The\nextensive number of video tokens leads to considerable computational costs for\nLLMs while using aggregated tokens results in loss of vision details. Moreover,\nthe presence of abundant question-irrelevant tokens introduces noise to the\nvideo QA process. To address these issues, we introduce a simple yet effective\nretrieval-based video language model (R-VLM) for efficient and interpretable\nlong video QA. Specifically, given a question (query) and a long video, our\nmodel identifies and selects the most relevant $K$ video chunks and uses their\nassociated visual tokens to serve as context for the LLM inference. This\neffectively reduces the number of video tokens, eliminates noise interference,\nand enhances system performance. Our experimental results validate the\neffectiveness of our framework for comprehending long videos. Furthermore,\nbased on the retrieved chunks, our model is interpretable that provides the\njustifications on where we get the answers.",
        "date": "2023-12-08T09:48:36+00:00",
        "link": "http://arxiv.org/pdf/2312.04931v1"
    },
    {
        "title": "Accelerating Convolutional Neural Network Pruning via Spatial Aura Entropy",
        "authors": [
            "Bogdan Musat",
            "Razvan Andonie"
        ],
        "abstract": "In recent years, pruning has emerged as a popular technique to reduce the\ncomputational complexity and memory footprint of Convolutional Neural Network\n(CNN) models. Mutual Information (MI) has been widely used as a criterion for\nidentifying unimportant filters to prune. However, existing methods for MI\ncomputation suffer from high computational cost and sensitivity to noise,\nleading to suboptimal pruning performance. We propose a novel method to improve\nMI computation for CNN pruning, using the spatial aura entropy. The spatial\naura entropy is useful for evaluating the heterogeneity in the distribution of\nthe neural activations over a neighborhood, providing information about local\nfeatures. Our method effectively improves the MI computation for CNN pruning,\nleading to more robust and efficient pruning. Experimental results on the\nCIFAR-10 benchmark dataset demonstrate the superiority of our approach in terms\nof pruning performance and computational efficiency.",
        "date": "2023-12-08T09:43:49+00:00",
        "link": "http://arxiv.org/pdf/2312.04926v1"
    },
    {
        "title": "SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation",
        "authors": [
            "Bangyan He",
            "Xiaojun Jia",
            "Siyuan Liang",
            "Tianrui Lou",
            "Yang Liu",
            "Xiaochun Cao"
        ],
        "abstract": "Current Visual-Language Pre-training (VLP) models are vulnerable to\nadversarial examples. These adversarial examples present substantial security\nrisks to VLP models, as they can leverage inherent weaknesses in the models,\nresulting in incorrect predictions. In contrast to white-box adversarial\nattacks, transfer attacks (where the adversary crafts adversarial examples on a\nwhite-box model to fool another black-box model) are more reflective of\nreal-world scenarios, thus making them more meaningful for research. By\nsummarizing and analyzing existing research, we identified two factors that can\ninfluence the efficacy of transfer attacks on VLP models: inter-modal\ninteraction and data diversity. Based on these insights, we propose a\nself-augment-based transfer attack method, termed SA-Attack. Specifically,\nduring the generation of adversarial images and adversarial texts, we apply\ndifferent data augmentation methods to the image modality and text modality,\nrespectively, with the aim of improving the adversarial transferability of the\ngenerated adversarial images and texts. Experiments conducted on the FLickr30K\nand COCO datasets have validated the effectiveness of our method. Our code will\nbe available after this paper is accepted.",
        "date": "2023-12-08T09:08:50+00:00",
        "link": "http://arxiv.org/pdf/2312.04913v1"
    },
    {
        "title": "Annotation-Free Group Robustness via Loss-Based Resampling",
        "authors": [
            "Mahdi Ghaznavi",
            "Hesam Asadollahzadeh",
            "HamidReza Yaghoubi Araghi",
            "Fahimeh Hosseini Noohdani",
            "Mohammad Hossein Rohban",
            "Mahdieh Soleymani Baghshah"
        ],
        "abstract": "It is well-known that training neural networks for image classification with\nempirical risk minimization (ERM) makes them vulnerable to relying on spurious\nattributes instead of causal ones for prediction. Previously, deep feature\nre-weighting (DFR) has proposed retraining the last layer of a pre-trained\nnetwork on balanced data concerning spurious attributes, making it robust to\nspurious correlation. However, spurious attribute annotations are not always\navailable. In order to provide group robustness without such annotations, we\npropose a new method, called loss-based feature re-weighting (LFR), in which we\ninfer a grouping of the data by evaluating an ERM-pre-trained model on a small\nleft-out split of the training data. Then, a balanced number of samples is\nchosen by selecting high-loss samples from misclassified data points and\nlow-loss samples from correctly-classified ones. Finally, we retrain the last\nlayer on the selected balanced groups to make the model robust to spurious\ncorrelation. For a complete assessment, we evaluate LFR on various versions of\nWaterbirds and CelebA datasets with different spurious correlations, which is a\nnovel technique for observing the model's performance in a wide range of\nspuriosity rates. While LFR is extremely fast and straightforward, it\noutperforms the previous methods that do not assume group label availability,\nas well as the DFR with group annotations provided, in cases of high spurious\ncorrelation in the training data.",
        "date": "2023-12-08T08:22:02+00:00",
        "link": "http://arxiv.org/pdf/2312.04893v1"
    },
    {
        "title": "Cross-BERT for Point Cloud Pretraining",
        "authors": [
            "Xin Li",
            "Peng Li",
            "Zeyong Wei",
            "Zhe Zhu",
            "Mingqiang Wei",
            "Junhui Hou",
            "Liangliang Nan",
            "Jing Qin",
            "Haoran Xie",
            "Fu Lee Wang"
        ],
        "abstract": "Introducing BERT into cross-modal settings raises difficulties in its\noptimization for handling multiple modalities. Both the BERT architecture and\ntraining objective need to be adapted to incorporate and model information from\ndifferent modalities. In this paper, we address these challenges by exploring\nthe implicit semantic and geometric correlations between 2D and 3D data of the\nsame objects/scenes. We propose a new cross-modal BERT-style self-supervised\nlearning paradigm, called Cross-BERT. To facilitate pretraining for irregular\nand sparse point clouds, we design two self-supervised tasks to boost\ncross-modal interaction. The first task, referred to as Point-Image Alignment,\naims to align features between unimodal and cross-modal representations to\ncapture the correspondences between the 2D and 3D modalities. The second task,\ntermed Masked Cross-modal Modeling, further improves mask modeling of BERT by\nincorporating high-dimensional semantic information obtained by cross-modal\ninteraction. By performing cross-modal interaction, Cross-BERT can smoothly\nreconstruct the masked tokens during pretraining, leading to notable\nperformance enhancements for downstream tasks. Through empirical evaluation, we\ndemonstrate that Cross-BERT outperforms existing state-of-the-art methods in 3D\ndownstream applications. Our work highlights the effectiveness of leveraging\ncross-modal 2D knowledge to strengthen 3D point cloud representation and the\ntransferable capability of BERT across modalities.",
        "date": "2023-12-08T08:18:12+00:00",
        "link": "http://arxiv.org/pdf/2312.04891v1"
    },
    {
        "title": "VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement",
        "authors": [
            "Hanjung Kim",
            "Jaehyun Kang",
            "Miran Heo",
            "Sukjun Hwang",
            "Seoung Wug Oh",
            "Seon Joo Kim"
        ],
        "abstract": "In recent years, online Video Instance Segmentation (VIS) methods have shown\nremarkable advancement with their powerful query-based detectors. Utilizing the\noutput queries of the detector at the frame level, these methods achieve high\naccuracy on challenging benchmarks. However, we observe the heavy reliance of\nthese methods on the location information that leads to incorrect matching when\npositional cues are insufficient for resolving ambiguities. Addressing this\nissue, we present VISAGE that enhances instance association by explicitly\nleveraging appearance information. Our method involves a generation of queries\nthat embed appearances from backbone feature maps, which in turn get used in\nour suggested simple tracker for robust associations. Finally, enabling\naccurate matching in complex scenarios by resolving the issue of over-reliance\non location information, we achieve competitive performance on multiple VIS\nbenchmarks. For instance, on YTVIS19 and YTVIS21, our method achieves 54.5 AP\nand 50.8 AP. Furthermore, to highlight appearance-awareness not fully addressed\nby existing benchmarks, we generate a synthetic dataset where our method\noutperforms others significantly by leveraging the appearance cue. Code will be\nmade available at https://github.com/KimHanjung/VISAGE.",
        "date": "2023-12-08T07:48:03+00:00",
        "link": "http://arxiv.org/pdf/2312.04885v1"
    },
    {
        "title": "UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models",
        "authors": [
            "Yiming Zhao",
            "Zhouhui Lian"
        ],
        "abstract": "Text-to-Image (T2I) generation methods based on diffusion model have garnered\nsignificant attention in the last few years. Although these image synthesis\nmethods produce visually appealing results, they frequently exhibit spelling\nerrors when rendering text within the generated images. Such errors manifest as\nmissing, incorrect or extraneous characters, thereby severely constraining the\nperformance of text image generation based on diffusion models. To address the\naforementioned issue, this paper proposes a novel approach for text image\ngeneration, utilizing a pre-trained diffusion model (i.e., Stable Diffusion\n[27]). Our approach involves the design and training of a light-weight\ncharacter-level text encoder, which replaces the original CLIP encoder and\nprovides more robust text embeddings as conditional guidance. Then, we\nfine-tune the diffusion model using a large-scale dataset, incorporating local\nattention control under the supervision of character-level segmentation maps.\nFinally, by employing an inference stage refinement process, we achieve a\nnotably high sequence accuracy when synthesizing text in arbitrarily given\nimages. Both qualitative and quantitative results demonstrate the superiority\nof our method to the state of the art. Furthermore, we showcase several\npotential applications of the proposed UDiffText, including text-centric image\nsynthesis, scene text editing, etc. Code and model will be available at\nhttps://github.com/ZYM-PKU/UDiffText .",
        "date": "2023-12-08T07:47:46+00:00",
        "link": "http://arxiv.org/pdf/2312.04884v1"
    },
    {
        "title": "MVDD: Multi-View Depth Diffusion Models",
        "authors": [
            "Zhen Wang",
            "Qiangeng Xu",
            "Feitong Tan",
            "Menglei Chai",
            "Shichen Liu",
            "Rohit Pandey",
            "Sean Fanello",
            "Achuta Kadambi",
            "Yinda Zhang"
        ],
        "abstract": "Denoising diffusion models have demonstrated outstanding results in 2D image\ngeneration, yet it remains a challenge to replicate its success in 3D shape\ngeneration. In this paper, we propose leveraging multi-view depth, which\nrepresents complex 3D shapes in a 2D data format that is easy to denoise. We\npair this representation with a diffusion model, MVDD, that is capable of\ngenerating high-quality dense point clouds with 20K+ points with fine-grained\ndetails. To enforce 3D consistency in multi-view depth, we introduce an\nepipolar line segment attention that conditions the denoising step for a view\non its neighboring views. Additionally, a depth fusion module is incorporated\ninto diffusion steps to further ensure the alignment of depth maps. When\naugmented with surface reconstruction, MVDD can also produce high-quality 3D\nmeshes. Furthermore, MVDD stands out in other tasks such as depth completion,\nand can serve as a 3D prior, significantly boosting many downstream tasks, such\nas GAN inversion. State-of-the-art results from extensive experiments\ndemonstrate MVDD's excellent ability in 3D shape generation, depth completion,\nand its potential as a 3D prior for downstream tasks.",
        "date": "2023-12-08T07:16:09+00:00",
        "link": "http://arxiv.org/pdf/2312.04875v1"
    },
    {
        "title": "Interpretable Underwater Diver Gesture Recognition",
        "authors": [
            "Sudeep Mangalvedhekar",
            "Shreyas Nahar",
            "Sudarshan Maskare",
            "Kaushal Mahajan",
            "Dr. Anant Bagade"
        ],
        "abstract": "In recent years, usage and applications of Autonomous Underwater Vehicles has\ngrown rapidly. Interaction of divers with the AUVs remains an integral part of\nthe usage of AUVs for various applications and makes building robust and\nefficient underwater gesture recognition systems extremely important. In this\npaper, we propose an Underwater Gesture Recognition system trained on the\nCognitive Autonomous Diving Buddy Underwater gesture dataset using deep\nlearning that achieves 98.01\\% accuracy on the dataset, which to the best of\nour knowledge is the best performance achieved on this dataset at the time of\nwriting this paper. We also improve the Gesture Recognition System\nInterpretability by using XAI techniques to visualize the model's predictions.",
        "date": "2023-12-08T07:14:52+00:00",
        "link": "http://arxiv.org/pdf/2312.04874v1"
    },
    {
        "title": "Adapting Vision Transformer for Efficient Change Detection",
        "authors": [
            "Yang Zhao",
            "Yuxiang Zhang",
            "Yanni Dong",
            "Bo Du"
        ],
        "abstract": "Most change detection models based on vision transformers currently follow a\n\"pretraining then fine-tuning\" strategy. This involves initializing the model\nweights using large scale classification datasets, which can be either natural\nimages or remote sensing images. However, fully tuning such a model requires\nsignificant time and resources. In this paper, we propose an efficient tuning\napproach that involves freezing the parameters of the pretrained image encoder\nand introducing additional training parameters. Through this approach, we have\nachieved competitive or even better results while maintaining extremely low\nresource consumption across six change detection benchmarks. For example,\ntraining time on LEVIR-CD, a change detection benchmark, is only half an hour\nwith 9 GB memory usage, which could be very convenient for most researchers.\nAdditionally, the decoupled tuning framework can be extended to any pretrained\nmodel for semantic change detection and multi temporal change detection as\nwell. We hope that our proposed approach will serve as a part of foundational\nmodel to inspire more unified training approaches on change detection in the\nfuture.",
        "date": "2023-12-08T07:09:03+00:00",
        "link": "http://arxiv.org/pdf/2312.04869v1"
    },
    {
        "title": "HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models",
        "authors": [
            "Pei Lin",
            "Sihang Xu",
            "Hongdi Yang",
            "Yiran Liu",
            "Xin Chen",
            "Jingya Wang",
            "Jingyi Yu",
            "Lan Xu"
        ],
        "abstract": "Existing hands datasets are largely short-range and the interaction is weak\ndue to the self-occlusion and self-similarity of hands, which can not yet fit\nthe need for interacting hands motion generation. To rescue the data scarcity,\nwe propose HandDiffuse12.5M, a novel dataset that consists of temporal\nsequences with strong two-hand interactions. HandDiffuse12.5M has the largest\nscale and richest interactions among the existing two-hand datasets. We further\npresent a strong baseline method HandDiffuse for the controllable motion\ngeneration of interacting hands using various controllers. Specifically, we\napply the diffusion model as the backbone and design two motion representations\nfor different controllers. To reduce artifacts, we also propose Interaction\nLoss which explicitly quantifies the dynamic interaction process. Our\nHandDiffuse enables various applications with vivid two-hand interactions,\ni.e., motion in-betweening and trajectory control. Experiments show that our\nmethod outperforms the state-of-the-art techniques in motion generation and can\nalso contribute to data augmentation for other datasets. Our dataset,\ncorresponding codes, and pre-trained models will be disseminated to the\ncommunity for future research towards two-hand interaction modeling.",
        "date": "2023-12-08T07:07:13+00:00",
        "link": "http://arxiv.org/pdf/2312.04867v1"
    },
    {
        "title": "Damage GAN: A Generative Model for Imbalanced Data",
        "authors": [
            "Ali Anaissi",
            "Yuanzhe Jia",
            "Ali Braytee",
            "Mohamad Naji",
            "Widad Alyassine"
        ],
        "abstract": "This study delves into the application of Generative Adversarial Networks\n(GANs) within the context of imbalanced datasets. Our primary aim is to enhance\nthe performance and stability of GANs in such datasets. In pursuit of this\nobjective, we introduce a novel network architecture known as Damage GAN,\nbuilding upon the ContraD GAN framework which seamlessly integrates GANs and\ncontrastive learning. Through the utilization of contrastive learning, the\ndiscriminator is trained to develop an unsupervised representation capable of\ndistinguishing all provided samples. Our approach draws inspiration from the\nstraightforward framework for contrastive learning of visual representations\n(SimCLR), leading to the formulation of a distinctive loss function. We also\nexplore the implementation of self-damaging contrastive learning (SDCLR) to\nfurther enhance the optimization of the ContraD GAN model. Comparative\nevaluations against baseline models including the deep convolutional GAN\n(DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed\nmodel, Damage GAN, in terms of generated image distribution, model stability,\nand image quality when applied to imbalanced datasets.",
        "date": "2023-12-08T06:36:33+00:00",
        "link": "http://arxiv.org/pdf/2312.04862v1"
    },
    {
        "title": "Radar Perception in Autonomous Driving: Exploring Different Data Representations",
        "authors": [
            "Shanliang Yao",
            "Runwei Guan",
            "Zitian Peng",
            "Chenhang Xu",
            "Yilu Shi",
            "Yong Yue",
            "Eng Gee Lim",
            "Hyungjoon Seo",
            "Ka Lok Man",
            "Xiaohui Zhu",
            "Yutao Yue"
        ],
        "abstract": "With the rapid advancements of sensor technology and deep learning,\nautonomous driving systems are providing safe and efficient access to\nintelligent vehicles as well as intelligent transportation. Among these\nequipped sensors, the radar sensor plays a crucial role in providing robust\nperception information in diverse environmental conditions. This review focuses\non exploring different radar data representations utilized in autonomous\ndriving systems. Firstly, we introduce the capabilities and limitations of the\nradar sensor by examining the working principles of radar perception and signal\nprocessing of radar measurements. Then, we delve into the generation process of\nfive radar representations, including the ADC signal, radar tensor, point\ncloud, grid map, and micro-Doppler signature. For each radar representation, we\nexamine the related datasets, methods, advantages and limitations. Furthermore,\nwe discuss the challenges faced in these data representations and propose\npotential research directions. Above all, this comprehensive review offers an\nin-depth insight into how these representations enhance autonomous system\ncapabilities, providing guidance for radar perception researchers. To\nfacilitate retrieval and comparison of different data representations, datasets\nand methods, we provide an interactive website at\nhttps://radar-camera-fusion.github.io/radar.",
        "date": "2023-12-08T06:31:19+00:00",
        "link": "http://arxiv.org/pdf/2312.04861v1"
    },
    {
        "title": "DiffCMR: Fast Cardiac MRI Reconstruction with Diffusion Probabilistic Models",
        "authors": [
            "Tianqi Xiang",
            "Wenjun Yue",
            "Yiqun Lin",
            "Jiewen Yang",
            "Zhenkun Wang",
            "Xiaomeng Li"
        ],
        "abstract": "Performing magnetic resonance imaging (MRI) reconstruction from under-sampled\nk-space data can accelerate the procedure to acquire MRI scans and reduce\npatients' discomfort. The reconstruction problem is usually formulated as a\ndenoising task that removes the noise in under-sampled MRI image slices.\nAlthough previous GAN-based methods have achieved good performance in image\ndenoising, they are difficult to train and require careful tuning of\nhyperparameters. In this paper, we propose a novel MRI denoising framework\nDiffCMR by leveraging conditional denoising diffusion probabilistic models.\nSpecifically, DiffCMR perceives conditioning signals from the under-sampled MRI\nimage slice and generates its corresponding fully-sampled MRI image slice.\nDuring inference, we adopt a multi-round ensembling strategy to stabilize the\nperformance. We validate DiffCMR with cine reconstruction and T1/T2 mapping\ntasks on MICCAI 2023 Cardiac MRI Reconstruction Challenge (CMRxRecon) dataset.\nResults show that our method achieves state-of-the-art performance, exceeding\nprevious methods by a significant margin. Code is available at\nhttps://github.com/xmed-lab/DiffCMR.",
        "date": "2023-12-08T06:11:21+00:00",
        "link": "http://arxiv.org/pdf/2312.04853v1"
    },
    {
        "title": "Learning Generalizable Perceptual Representations for Data-Efficient No-Reference Image Quality Assessment",
        "authors": [
            "Suhas Srinath",
            "Shankhanil Mitra",
            "Shika Rao",
            "Rajiv Soundararajan"
        ],
        "abstract": "No-reference (NR) image quality assessment (IQA) is an important tool in\nenhancing the user experience in diverse visual applications. A major drawback\nof state-of-the-art NR-IQA techniques is their reliance on a large number of\nhuman annotations to train models for a target IQA application. To mitigate\nthis requirement, there is a need for unsupervised learning of generalizable\nquality representations that capture diverse distortions. We enable the\nlearning of low-level quality features agnostic to distortion types by\nintroducing a novel quality-aware contrastive loss. Further, we leverage the\ngeneralizability of vision-language models by fine-tuning one such model to\nextract high-level image quality information through relevant text prompts. The\ntwo sets of features are combined to effectively predict quality by training a\nsimple regressor with very few samples on a target dataset. Additionally, we\ndesign zero-shot quality predictions from both pathways in a completely blind\nsetting. Our experiments on diverse datasets encompassing various distortions\nshow the generalizability of the features and their superior performance in the\ndata-efficient and zero-shot settings. Code will be made available at\nhttps://github.com/suhas-srinath/GRepQ.",
        "date": "2023-12-08T05:24:21+00:00",
        "link": "http://arxiv.org/pdf/2312.04838v1"
    },
    {
        "title": "Localized Symbolic Knowledge Distillation for Visual Commonsense Models",
        "authors": [
            "Jae Sung Park",
            "Jack Hessel",
            "Khyathi Raghavi Chandu",
            "Paul Pu Liang",
            "Ximing Lu",
            "Peter West",
            "Youngjae Yu",
            "Qiuyuan Huang",
            "Jianfeng Gao",
            "Ali Farhadi",
            "Yejin Choi"
        ],
        "abstract": "Instruction following vision-language (VL) models offer a flexible interface\nthat supports a broad range of multimodal tasks in a zero-shot fashion.\nHowever, interfaces that operate on full images do not directly enable the user\nto \"point to\" and access specific regions within images. This capability is\nimportant not only to support reference-grounded VL benchmarks, but also, for\npractical applications that require precise within-image reasoning. We build\nLocalized Visual Commonsense models, which allow users to specify (multiple)\nregions as input. We train our model by sampling localized commonsense\nknowledge from a large language model (LLM): specifically, we prompt an LLM to\ncollect commonsense knowledge given a global literal image description and a\nlocal literal region description automatically generated by a set of VL models.\nWith a separately trained critic model that selects high-quality examples, we\nfind that training on the localized commonsense corpus can successfully distill\nexisting VL models to support a reference-as-input interface. Empirical results\nand human evaluations in a zero-shot setup demonstrate that our distillation\nmethod results in more precise VL models of reasoning compared to a baseline of\npassing a generated referring expression to an LLM.",
        "date": "2023-12-08T05:23:50+00:00",
        "link": "http://arxiv.org/pdf/2312.04837v1"
    },
    {
        "title": "Towards Stable and Faithful Inpainting",
        "authors": [
            "Yikai Wang",
            "Chenjie Cao",
            "Yanwei Fu"
        ],
        "abstract": "Recent progress in inpainting increasingly relies on generative models,\nleveraging their strong generation capabilities for addressing ill-conditioned\nproblems. However, this enhanced generation often introduces instability,\nleading to arbitrary object generation within masked regions. This paper\nproposes a balanced solution, emphasizing the importance of unmasked regions in\nguiding inpainting while preserving generative capacity. Our approach, Aligned\nStable Inpainting with UnKnown Areas Prior (ASUKA), employs a\nreconstruction-based masked auto-encoder (MAE) as a stable prior. Aligned with\nthe robust Stable Diffusion inpainting model (SD), ASUKA significantly improves\ninpainting stability. ASUKA further aligns masked and unmasked regions through\nan inpainting-specialized decoder, ensuring more faithful inpainting. To\nvalidate effectiveness across domains and masking scenarios, we evaluate on\nMISATO, a collection of several existing dataset. Results confirm ASUKA's\nefficacy in both stability and fidelity compared to SD and other inpainting\nalgorithms.",
        "date": "2023-12-08T05:08:06+00:00",
        "link": "http://arxiv.org/pdf/2312.04831v1"
    },
    {
        "title": "SiCP: Simultaneous Individual and Cooperative Perception for 3D Object Detection in Connected and Automated Vehicles",
        "authors": [
            "Deyuan Qu",
            "Qi Chen",
            "Tianyu Bai",
            "Andy Qin",
            "Hongsheng Lu",
            "Heng Fan",
            "Song Fu",
            "Qing Yang"
        ],
        "abstract": "Cooperative perception for connected and automated vehicles is traditionally\nachieved through the fusion of feature maps from two or more vehicles. However,\nthe absence of feature maps shared from other vehicles can lead to a\nsignificant decline in object detection performance for cooperative perception\nmodels compared to standalone 3D detection models. This drawback impedes the\nadoption of cooperative perception as vehicle resources are often insufficient\nto concurrently employ two perception models. To tackle this issue, we present\nSimultaneous Individual and Cooperative Perception (SiCP), a generic framework\nthat supports a wide range of the state-of-the-art standalone perception\nbackbones and enhances them with a novel Dual-Perception Network (DP-Net)\ndesigned to facilitate both individual and cooperative perception. In addition\nto its lightweight nature with only 0.13M parameters, DP-Net is robust and\nretains crucial gradient information during feature map fusion. As demonstrated\nin a comprehensive evaluation on the OPV2V dataset, thanks to DP-Net, SiCP\nsurpasses state-of-the-art cooperative perception solutions while preserving\nthe performance of standalone perception solutions.",
        "date": "2023-12-08T04:12:26+00:00",
        "link": "http://arxiv.org/pdf/2312.04822v1"
    },
    {
        "title": "Unify Change Point Detection and Segment Classification in a Regression Task for Transportation Mode Identification",
        "authors": [
            "Rongsong Li",
            "Xin Pei"
        ],
        "abstract": "Identifying travelers' transportation modes is important in transportation\nscience and location-based services. It's appealing for researchers to leverage\nGPS trajectory data to infer transportation modes with the popularity of\nGPS-enabled devices, e.g., smart phones. Existing studies frame this problem as\nclassification task. The dominant two-stage studies divide the trip into\nsingle-one mode segments first and then categorize these segments. The over\nsegmentation strategy and inevitable error propagation bring difficulties to\nclassification stage and make optimizing the whole system hard. The recent\none-stage works throw out trajectory segmentation entirely to avoid these by\ndirectly conducting point-wise classification for the trip, whereas leaving\npredictions dis-continuous. To solve above-mentioned problems, inspired by YOLO\nand SSD in object detection, we propose to reframe change point detection and\nsegment classification as a unified regression task instead of the existing\nclassification task. We directly regress coordinates of change points and\nclassify associated segments. In this way, our method divides the trip into\nsegments under a supervised manner and leverage more contextual information,\nobtaining predictions with high accuracy and continuity. Two frameworks,\nTrajYOLO and TrajSSD, are proposed to solve the regression task and various\nfeature extraction backbones are exploited. Exhaustive experiments on GeoLife\ndataset show that the proposed method has competitive overall identification\naccuracy of 0.853 when distinguishing five modes: walk, bike, bus, car, train.\nAs for change point detection, our method increases precision at the cost of\ndrop in recall. All codes are available at\nhttps://github.com/RadetzkyLi/TrajYOLO-SSD.",
        "date": "2023-12-08T03:59:01+00:00",
        "link": "http://arxiv.org/pdf/2312.04821v1"
    },
    {
        "title": "Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting",
        "authors": [
            "Xiaofeng Yang",
            "Yiwen Chen",
            "Cheng Chen",
            "Chi Zhang",
            "Yi Xu",
            "Xulei Yang",
            "Fayao Liu",
            "Guosheng Lin"
        ],
        "abstract": "We propose a unified framework aimed at enhancing the diffusion priors for 3D\ngeneration tasks. Despite the critical importance of these tasks, existing\nmethodologies often struggle to generate high-caliber results. We begin by\nexamining the inherent limitations in previous diffusion priors. We identify a\ndivergence between the diffusion priors and the training procedures of\ndiffusion models that substantially impairs the quality of 3D generation. To\naddress this issue, we propose a novel, unified framework that iteratively\noptimizes both the 3D model and the diffusion prior. Leveraging the different\nlearnable parameters of the diffusion prior, our approach offers multiple\nconfigurations, affording various trade-offs between performance and\nimplementation complexity. Notably, our experimental results demonstrate that\nour method markedly surpasses existing techniques, establishing new\nstate-of-the-art in the realm of text-to-3D generation. Furthermore, our\napproach exhibits impressive performance on both NeRF and the newly introduced\n3D Gaussian Splatting backbones. Additionally, our framework yields insightful\ncontributions to the understanding of recent score distillation methods, such\nas the VSD and DDS loss.",
        "date": "2023-12-08T03:55:34+00:00",
        "link": "http://arxiv.org/pdf/2312.04820v1"
    },
    {
        "title": "MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding",
        "authors": [
            "Hongjie Zhang",
            "Yi Liu",
            "Lu Dong",
            "Yifei Huang",
            "Zhen-Hua Ling",
            "Yali Wang",
            "Limin Wang",
            "Yu Qiao"
        ],
        "abstract": "While several long-form VideoQA datasets have been introduced, the length of\nboth videos used to curate questions and sub-clips of clues leveraged to answer\nthose questions have not yet reached the criteria for genuine long-form video\nunderstanding. Moreover, their QAs are unduly narrow and modality-biased,\nlacking a wider view of understanding long-term video content with rich\ndynamics and complex narratives. To remedy this, we introduce MoVQA, a\nlong-form movie question-answering dataset, and benchmark to assess the diverse\ncognitive capabilities of multimodal systems rely on multi-level temporal\nlengths, with considering both video length and clue length. Additionally, to\ntake a step towards human-level understanding in long-form video, versatile and\nmultimodal question-answering is designed from the moviegoer-perspective to\nassess the model capabilities on various perceptual and cognitive axes.Through\nanalysis involving various baselines reveals a consistent trend: the\nperformance of all methods significantly deteriorate with increasing video and\nclue length. Meanwhile, our established baseline method has shown some\nimprovements, but there is still ample scope for enhancement on our challenging\nMoVQA dataset. We expect our MoVQA to provide a new perspective and encourage\ninspiring works on long-form video understanding research.",
        "date": "2023-12-08T03:33:38+00:00",
        "link": "http://arxiv.org/pdf/2312.04817v1"
    },
    {
        "title": "DARNet: Bridging Domain Gaps in Cross-Domain Few-Shot Segmentation with Dynamic Adaptation",
        "authors": [
            "Haoran Fan",
            "Qi Fan",
            "Maurice Pagnucco",
            "Yang Song"
        ],
        "abstract": "Few-shot segmentation (FSS) aims to segment novel classes in a query image by\nusing only a small number of supporting images from base classes. However, in\ncross-domain few-shot segmentation (CD-FSS), leveraging features from\nlabel-rich domains for resource-constrained domains poses challenges due to\ndomain discrepancies. This work presents a Dynamically Adaptive Refine (DARNet)\nmethod that aims to balance generalization and specificity for CD-FSS. Our\nmethod includes the Channel Statistics Disruption (CSD) strategy, which\nperturbs feature channel statistics in the source domain, bolstering\ngeneralization to unknown target domains. Moreover, recognizing the variability\nacross target domains, an Adaptive Refine Self-Matching (ARSM) method is also\nproposed to adjust the matching threshold and dynamically refine the prediction\nresult with the self-matching method, enhancing accuracy. We also present a\nTest-Time Adaptation (TTA) method to refine the model's adaptability to diverse\nfeature distributions. Our approach demonstrates superior performance against\nstate-of-the-art methods in CD-FSS tasks.",
        "date": "2023-12-08T03:03:22+00:00",
        "link": "http://arxiv.org/pdf/2312.04813v1"
    },
    {
        "title": "RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion Models",
        "authors": [
            "Yue Jiang",
            "Yueming Lyu",
            "Tianxiang Ma",
            "Bo Peng",
            "Jing Dong"
        ],
        "abstract": "Recent text-conditioned image generation models have demonstrated an\nexceptional capacity to produce diverse and creative imagery with high visual\nquality. However, when pre-trained on billion-sized datasets randomly collected\nfrom the Internet, where potential biased human preferences exist, these models\ntend to produce images with common and recurring stereotypes, particularly for\ncertain racial groups. In this paper, we conduct an initial analysis of the\npublicly available Stable Diffusion model and its derivatives, highlighting the\npresence of racial stereotypes. These models often generate distorted or biased\nimages for certain racial groups, emphasizing stereotypical characteristics. To\naddress these issues, we propose a framework called \"RS-Corrector\", designed to\nestablish an anti-stereotypical preference in the latent space and update the\nlatent code for refined generated results. The correction process occurs during\nthe inference stage without requiring fine-tuning of the original model.\nExtensive empirical evaluations demonstrate that the introduced \\themodel\neffectively corrects the racial stereotypes of the well-trained Stable\nDiffusion model while leaving the original model unchanged.",
        "date": "2023-12-08T02:59:29+00:00",
        "link": "http://arxiv.org/pdf/2312.04810v1"
    },
    {
        "title": "A Review On Table Recognition Based On Deep Learning",
        "authors": [
            "Shi Jiyuan",
            "Shi chunqi"
        ],
        "abstract": "Table recognition is using the computer to automatically understand the\ntable, to detect the position of the table from the document or picture, and to\ncorrectly extract and identify the internal structure and content of the table.\nAfter earlier mainstream approaches based on heuristic rules and machine\nlearning, the development of deep learning techniques has brought a new\nparadigm to this field. This review mainly discusses the table recognition\nproblem from five aspects. The first part introduces data sets, benchmarks, and\ncommonly used evaluation indicators. This section selects representative data\nsets, benchmarks, and evaluation indicators that are frequently used by\nresearchers. The second part introduces the table recognition model. This\nsurvey introduces the development of the table recognition model, especially\nthe table recognition model based on deep learning. It is generally accepted\nthat table recognition is divided into two stages: table detection and table\nstructure recognition. This section introduces the models that follow this\nparadigm (TD and TSR). The third part is the End-to-End method, this section\nintroduces some scholars' attempts to use an end-to-end approach to solve the\ntable recognition problem once and for all and the part are Data-centric\nmethods, such as data augmentation, aligning benchmarks, and other methods. The\nfourth part is the data-centric approach, such as data enhancement, alignment\nbenchmark, and so on. The fifth part summarizes and compares the experimental\ndata in the field of form recognition, and analyzes the mainstream and more\nadvantageous methods. Finally, this paper also discusses the possible\ndevelopment direction and trend of form processing in the future, to provide\nsome ideas for researchers in the field of table recognition. (Resource will be\nreleased at https://github.com/Wa1den-jy/Topic-on-Table-Recognition .)",
        "date": "2023-12-08T02:58:00+00:00",
        "link": "http://arxiv.org/pdf/2312.04808v1"
    },
    {
        "title": "RL Dreams: Policy Gradient Optimization for Score Distillation based 3D Generation",
        "authors": [
            "Aradhya N. Mathur",
            "Phu Pham",
            "Aniket Bera",
            "Ojaswa Sharma"
        ],
        "abstract": "3D generation has rapidly accelerated in the past decade owing to the\nprogress in the field of generative modeling. Score Distillation Sampling (SDS)\nbased rendering has improved 3D asset generation to a great extent. Further,\nthe recent work of Denoising Diffusion Policy Optimization (DDPO) demonstrates\nthat the diffusion process is compatible with policy gradient methods and has\nbeen demonstrated to improve the 2D diffusion models using an aesthetic scoring\nfunction. We first show that this aesthetic scorer acts as a strong guide for a\nvariety of SDS-based methods and demonstrates its effectiveness in text-to-3D\nsynthesis. Further, we leverage the DDPO approach to improve the quality of the\n3D rendering obtained from 2D diffusion models. Our approach, DDPO3D, employs\nthe policy gradient method in tandem with aesthetic scoring. To the best of our\nknowledge, this is the first method that extends policy gradient methods to 3D\nscore-based rendering and shows improvement across SDS-based methods such as\nDreamGaussian, which are currently driving research in text-to-3D synthesis.\nOur approach is compatible with score distillation-based methods, which would\nfacilitate the integration of diverse reward functions into the generative\nprocess. Our project page can be accessed via https://ddpo3d.github.io.",
        "date": "2023-12-08T02:41:04+00:00",
        "link": "http://arxiv.org/pdf/2312.04806v1"
    },
    {
        "title": "SuperNormal: Neural Surface Reconstruction via Multi-View Normal Integration",
        "authors": [
            "Xu Cao",
            "Takafumi Taketomi"
        ],
        "abstract": "We present SuperNormal, a fast, high-fidelity approach to multi-view 3D\nreconstruction using surface normal maps. With a few minutes, SuperNormal\nproduces detailed surfaces on par with 3D scanners. We harness volume rendering\nto optimize a neural signed distance function (SDF) powered by multi-resolution\nhash encoding. To accelerate training, we propose directional finite difference\nand patch-based ray marching to approximate the SDF gradients numerically.\nWhile not compromising reconstruction quality, this strategy is nearly twice as\nefficient as analytical gradients and about three times faster than\naxis-aligned finite difference. Experiments on the benchmark dataset\ndemonstrate the superiority of SuperNormal in efficiency and accuracy compared\nto existing multi-view photometric stereo methods. On our captured objects,\nSuperNormal produces more fine-grained geometry than recent neural 3D\nreconstruction methods.",
        "date": "2023-12-08T02:34:30+00:00",
        "link": "http://arxiv.org/pdf/2312.04803v1"
    },
    {
        "title": "MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model",
        "authors": [
            "Kaiyu Song",
            "Hanjiang Lai"
        ],
        "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial perturbation, where\nan imperceptible perturbation is added to the image that can fool the DNNs.\nDiffusion-based adversarial purification focuses on using the diffusion model\nto generate a clean image against such adversarial attacks. Unfortunately, the\ngenerative process of the diffusion model is also inevitably affected by\nadversarial perturbation since the diffusion model is also a deep network where\nits input has adversarial perturbation. In this work, we propose\nMimicDiffusion, a new diffusion-based adversarial purification technique, that\ndirectly approximates the generative process of the diffusion model with the\nclean image as input. Concretely, we analyze the differences between the guided\nterms using the clean image and the adversarial sample. After that, we first\nimplement MimicDiffusion based on Manhattan distance. Then, we propose two\nguidance to purify the adversarial perturbation and approximate the clean\ndiffusion model. Extensive experiments on three image datasets including\nCIFAR-10, CIFAR-100, and ImageNet with three classifier backbones including\nWideResNet-70-16, WideResNet-28-10, and ResNet50 demonstrate that\nMimicDiffusion significantly performs better than the state-of-the-art\nbaselines. On CIFAR-10, CIFAR-100, and ImageNet, it achieves 92.67\\%, 61.35\\%,\nand 61.53\\% average robust accuracy, which are 18.49\\%, 13.23\\%, and 17.64\\%\nhigher, respectively. The code is available in the supplementary material.",
        "date": "2023-12-08T02:32:47+00:00",
        "link": "http://arxiv.org/pdf/2312.04802v1"
    },
    {
        "title": "Segmentation of Kidney Tumors on Non-Contrast CT Images using Protuberance Detection Network",
        "authors": [
            "Taro Hatsutani",
            "Akimichi Ichinose",
            "Keigo Nakamura",
            "Yoshiro Kitamura"
        ],
        "abstract": "Many renal cancers are incidentally found on non-contrast CT (NCCT) images.\nOn contrast-enhanced CT (CECT) images, most kidney tumors, especially renal\ncancers, have different intensity values compared to normal tissues. However,\non NCCT images, some tumors called isodensity tumors, have similar intensity\nvalues to the surrounding normal tissues, and can only be detected through a\nchange in organ shape. Several deep learning methods which segment kidney\ntumors from CECT images have been proposed and showed promising results.\nHowever, these methods fail to capture such changes in organ shape on NCCT\nimages. In this paper, we present a novel framework, which can explicitly\ncapture protruded regions in kidneys to enable a better segmentation of kidney\ntumors. We created a synthetic mask dataset that simulates a protuberance, and\ntrained a segmentation network to separate the protruded regions from the\nnormal kidney regions. To achieve the segmentation of whole tumors, our\nframework consists of three networks. The first network is a conventional\nsemantic segmentation network which extracts a kidney region mask and an\ninitial tumor region mask. The second network, which we name protuberance\ndetection network, identifies the protruded regions from the kidney region\nmask. Given the initial tumor region mask and the protruded region mask, the\nlast network fuses them and predicts the final kidney tumor mask accurately.\nThe proposed method was evaluated on a publicly available KiTS19 dataset, which\ncontains 108 NCCT images, and showed that our method achieved a higher dice\nscore of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared to 3D-UNet.\nTo the best of our knowledge, this is the first deep learning method that is\nspecifically designed for kidney tumor segmentation on NCCT images.",
        "date": "2023-12-08T02:12:15+00:00",
        "link": "http://arxiv.org/pdf/2312.04796v1"
    },
    {
        "title": "Visual Grounding of Whole Radiology Reports for 3D CT Images",
        "authors": [
            "Akimichi Ichinose",
            "Taro Hatsutani",
            "Keigo Nakamura",
            "Yoshiro Kitamura",
            "Satoshi Iizuka",
            "Edgar Simo-Serra",
            "Shoji Kido",
            "Noriyuki Tomiyama"
        ],
        "abstract": "Building a large-scale training dataset is an essential problem in the\ndevelopment of medical image recognition systems. Visual grounding techniques,\nwhich automatically associate objects in images with corresponding\ndescriptions, can facilitate labeling of large number of images. However,\nvisual grounding of radiology reports for CT images remains challenging,\nbecause so many kinds of anomalies are detectable via CT imaging, and resulting\nreport descriptions are long and complex. In this paper, we present the first\nvisual grounding framework designed for CT image and report pairs covering\nvarious body parts and diverse anomaly types. Our framework combines two\ncomponents of 1) anatomical segmentation of images, and 2) report structuring.\nThe anatomical segmentation provides multiple organ masks of given CT images,\nand helps the grounding model recognize detailed anatomies. The report\nstructuring helps to accurately extract information regarding the presence,\nlocation, and type of each anomaly described in corresponding reports. Given\nthe two additional image/report features, the grounding model can achieve\nbetter localization. In the verification process, we constructed a large-scale\ndataset with region-description correspondence annotations for 10,410 studies\nof 7,321 unique patients. We evaluated our framework using grounding accuracy,\nthe percentage of correctly localized anomalies, as a metric and demonstrated\nthat the combination of the anatomical segmentation and the report structuring\nimproves the performance with a large margin over the baseline model (66.0% vs\n77.8%). Comparison with the prior techniques also showed higher performance of\nour method.",
        "date": "2023-12-08T02:09:17+00:00",
        "link": "http://arxiv.org/pdf/2312.04794v1"
    },
    {
        "title": "User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning",
        "authors": [
            "Xuan Wang",
            "Guanhong Wang",
            "Wenhao Chai",
            "Jiayu Zhou",
            "Gaoang Wang"
        ],
        "abstract": "Image captioning bridges the gap between vision and language by automatically\ngenerating natural language descriptions for images. Traditional image\ncaptioning methods often overlook the preferences and characteristics of users.\nPersonalized image captioning solves this problem by incorporating user prior\nknowledge into the model, such as writing styles and preferred vocabularies.\nMost existing methods emphasize the user context fusion process by memory\nnetworks or transformers. However, these methods ignore the distinct domains of\neach dataset. Therefore, they need to update the entire caption model\nparameters when meeting new samples, which is time-consuming and\ncalculation-intensive. To address this challenge, we propose a novel\npersonalized image captioning framework that leverages user context to consider\npersonality factors. Additionally, our framework utilizes the prefix-tuning\nparadigm to extract knowledge from a frozen large language model, reducing the\ngap between different language domains. Specifically, we employ CLIP to extract\nthe visual features of an image and align the semantic space using a\nquery-guided mapping network. By incorporating the transformer layer, we merge\nthe visual features with the user's contextual prior knowledge to generate\ninformative prefixes. Moreover, we employ GPT-2 as the frozen large language\nmodel. With a small number of parameters to be trained, our model performs\nefficiently and effectively. Our model outperforms existing baseline models on\nInstagram and YFCC100M datasets across five evaluation metrics, demonstrating\nits superiority, including twofold improvements in metrics such as BLEU-4 and\nCIDEr.",
        "date": "2023-12-08T02:08:00+00:00",
        "link": "http://arxiv.org/pdf/2312.04793v1"
    },
    {
        "title": "Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video",
        "authors": [
            "Yuchen Rao",
            "Eduardo Perez Pellitero",
            "Benjamin Busam",
            "Yiren Zhou",
            "Jifei Song"
        ],
        "abstract": "Recent advancements in 3D avatar generation excel with multi-view supervision\nfor photorealistic models. However, monocular counterparts lag in quality\ndespite broader applicability. We propose ReCaLab to close this gap. ReCaLab is\na fully-differentiable pipeline that learns high-fidelity 3D human avatars from\njust a single RGB video. A pose-conditioned deformable NeRF is optimized to\nvolumetrically represent a human subject in canonical T-pose. The canonical\nrepresentation is then leveraged to efficiently associate viewpoint-agnostic\ntextures using 2D-3D correspondences. This enables to separately generate\nalbedo and shading which jointly compose an RGB prediction. The design allows\nto control intermediate results for human pose, body shape, texture, and\nlighting with text prompts. An image-conditioned diffusion model thereby helps\nto animate appearance and pose of the 3D avatar to create video sequences with\npreviously unseen human motion. Extensive experiments show that ReCaLab\noutperforms previous monocular approaches in terms of image quality for image\nsynthesis tasks. ReCaLab even outperforms multi-view methods that leverage up\nto 19x more synchronized videos for the task of novel pose rendering. Moreover,\nnatural language offers an intuitive user interface for creative manipulation\nof 3D human avatars.",
        "date": "2023-12-08T01:53:06+00:00",
        "link": "http://arxiv.org/pdf/2312.04784v1"
    },
    {
        "title": "Fine-Tuning InstructPix2Pix for Advanced Image Colorization",
        "authors": [
            "Zifeng An",
            "Zijing Xu",
            "Eric Fan",
            "Qi Cao"
        ],
        "abstract": "This paper presents a novel approach to human image colorization by\nfine-tuning the InstructPix2Pix model, which integrates a language model\n(GPT-3) with a text-to-image model (Stable Diffusion). Despite the original\nInstructPix2Pix model's proficiency in editing images based on textual\ninstructions, it exhibits limitations in the focused domain of colorization. To\naddress this, we fine-tuned the model using the IMDB-WIKI dataset, pairing\nblack-and-white images with a diverse set of colorization prompts generated by\nChatGPT. This paper contributes by (1) applying fine-tuning techniques to\nstable diffusion models specifically for colorization tasks, and (2) employing\ngenerative models to create varied conditioning prompts. After finetuning, our\nmodel outperforms the original InstructPix2Pix model on multiple metrics\nquantitatively, and we produce more realistically colored images qualitatively.\nThe code for this project is provided on the GitHub Repository\nhttps://github.com/AllenAnZifeng/DeepLearning282.",
        "date": "2023-12-08T01:36:49+00:00",
        "link": "http://arxiv.org/pdf/2312.04780v1"
    },
    {
        "title": "Image Synthesis-based Late Stage Cancer Augmentation and Semi-Supervised Segmentation for MRI Rectal Cancer Staging",
        "authors": [
            "Saeko Sasuga",
            "Akira Kudo",
            "Yoshiro Kitamura",
            "Satoshi Iizuka",
            "Edgar Simo-Serra",
            "Atsushi Hamabe",
            "Masayuki Ishii",
            "Ichiro Takemasa"
        ],
        "abstract": "Rectal cancer is one of the most common diseases and a major cause of\nmortality. For deciding rectal cancer treatment plans, T-staging is important.\nHowever, evaluating the index from preoperative MRI images requires high\nradiologists' skill and experience. Therefore, the aim of this study is to\nsegment the mesorectum, rectum, and rectal cancer region so that the system can\npredict T-stage from segmentation results. Generally, shortage of large and\ndiverse dataset and high quality annotation are known to be the bottlenecks in\ncomputer aided diagnostics development. Regarding rectal cancer, advanced\ncancer images are very rare, and per-pixel annotation requires high\nradiologists' skill and time. Therefore, it is not feasible to collect\ncomprehensive disease patterns in a training dataset. To tackle this, we\npropose two kinds of approaches of image synthesis-based late stage cancer\naugmentation and semi-supervised learning which is designed for T-stage\nprediction. In the image synthesis data augmentation approach, we generated\nadvanced cancer images from labels. The real cancer labels were deformed to\nresemble advanced cancer labels by artificial cancer progress simulation. Next,\nwe introduce a T-staging loss which enables us to train segmentation models\nfrom per-image T-stage labels. The loss works to keep inclusion/invasion\nrelationships between rectum and cancer region consistent to the ground truth\nT-stage. The verification tests show that the proposed method obtains the best\nsensitivity (0.76) and specificity (0.80) in distinguishing between over T3\nstage and underT2. In the ablation studies, our semi-supervised learning\napproach with the T-staging loss improved specificity by 0.13. Adding the image\nsynthesis-based data augmentation improved the DICE score of invasion cancer\narea by 0.08 from baseline.",
        "date": "2023-12-08T01:36:24+00:00",
        "link": "http://arxiv.org/pdf/2312.04779v1"
    },
    {
        "title": "Seamless: Multilingual Expressive and Streaming Speech Translation",
        "authors": [
            "Seamless Communication",
            "Loïc Barrault",
            "Yu-An Chung",
            "Mariano Coria Meglioli",
            "David Dale",
            "Ning Dong",
            "Mark Duppenthaler",
            "Paul-Ambroise Duquenne",
            "Brian Ellis",
            "Hady Elsahar",
            "Justin Haaheim",
            "John Hoffman",
            "Min-Jae Hwang",
            "Hirofumi Inaguma",
            "Christopher Klaiber",
            "Ilia Kulikov",
            "Pengwei Li",
            "Daniel Licht",
            "Jean Maillard",
            "Ruslan Mavlyutov",
            "Alice Rakotoarison",
            "Kaushik Ram Sadagopan",
            "Abinesh Ramakrishnan",
            "Tuan Tran",
            "Guillaume Wenzek",
            "Yilin Yang",
            "Ethan Ye",
            "Ivan Evtimov",
            "Pierre Fernandez",
            "Cynthia Gao",
            "Prangthip Hansanti",
            "Elahe Kalbassi",
            "Amanda Kallet",
            "Artyom Kozhevnikov",
            "Gabriel Mejia Gonzalez",
            "Robin San Roman",
            "Christophe Touret",
            "Corinne Wong",
            "Carleigh Wood",
            "Bokai Yu",
            "Pierre Andrews",
            "Can Balioglu",
            "Peng-Jen Chen",
            "Marta R. Costa-jussà",
            "Maha Elbayad",
            "Hongyu Gong",
            "Francisco Guzmán",
            "Kevin Heffernan",
            "Somya Jain",
            "Justine Kao",
            "Ann Lee",
            "Xutai Ma",
            "Alex Mourachko",
            "Benjamin Peloquin",
            "Juan Pino",
            "Sravya Popuri",
            "Christophe Ropers",
            "Safiyyah Saleem",
            "Holger Schwenk",
            "Anna Sun",
            "Paden Tomasello",
            "Changhan Wang",
            "Jeff Wang",
            "Skyler Wang",
            "Mary Williamson"
        ],
        "abstract": "Large-scale automatic speech translation systems today lack key features that\nhelp machine-mediated communication feel seamless when compared to\nhuman-to-human dialogue. In this work, we introduce a family of models that\nenable end-to-end expressive and multilingual translations in a streaming\nfashion. First, we contribute an improved version of the massively multilingual\nand multimodal SeamlessM4T model-SeamlessM4T v2. This newer model,\nincorporating an updated UnitY2 framework, was trained on more low-resource\nlanguage data. SeamlessM4T v2 provides the foundation on which our next two\nmodels are initiated. SeamlessExpressive enables translation that preserves\nvocal styles and prosody. Compared to previous efforts in expressive speech\nresearch, our work addresses certain underexplored aspects of prosody, such as\nspeech rate and pauses, while also preserving the style of one's voice. As for\nSeamlessStreaming, our model leverages the Efficient Monotonic Multihead\nAttention mechanism to generate low-latency target translations without waiting\nfor complete source utterances. As the first of its kind, SeamlessStreaming\nenables simultaneous speech-to-speech/text translation for multiple source and\ntarget languages. To ensure that our models can be used safely and responsibly,\nwe implemented the first known red-teaming effort for multimodal machine\ntranslation, a system for the detection and mitigation of added toxicity, a\nsystematic evaluation of gender bias, and an inaudible localized watermarking\nmechanism designed to dampen the impact of deepfakes. Consequently, we bring\nmajor components from SeamlessExpressive and SeamlessStreaming together to form\nSeamless, the first publicly available system that unlocks expressive\ncross-lingual communication in real-time. The contributions to this work are\npublicly released and accessible at\nhttps://github.com/facebookresearch/seamless_communication",
        "date": "2023-12-08T17:18:42+00:00",
        "link": "http://arxiv.org/pdf/2312.05187v1"
    },
    {
        "title": "Binaural multichannel blind speaker separation with a causal low-latency and low-complexity approach",
        "authors": [
            "Nils L. Westhausen",
            "Bernd T. Meyer"
        ],
        "abstract": "In this paper, we introduce a causal low-latency low-complexity approach for\nbinaural multichannel blind speaker separation in noisy reverberant conditions.\n  The model, referred to as Group Communication Binaural Filter and Sum Network\n(GCBFSnet) predicts complex filters for filter-and-sum beamforming in the\ntime-frequency domain.\n  We apply Group Communication (GC),\n  i.e., latent model variables are split into groups and processed with a\nshared sequence model with the aim of reducing the complexity of a simple model\nonly containing one convolutional and one recurrent module.\n  With GC we are able to reduce the size of the model by up to 83 % and the\ncomplexity up to 73 % compared to the model without GC, while mostly retaining\nperformance.\n  Even for the smallest model configuration, GCBFSnet matches the performance\nof a low-complexity TasNet baseline in most metrics despite the larger size and\nhigher number of required operations of the baseline.",
        "date": "2023-12-08T16:53:04+00:00",
        "link": "http://arxiv.org/pdf/2312.05173v1"
    },
    {
        "title": "neural concatenative singing voice conversion: rethinking concatenation-based approach for one-shot singing voice conversion",
        "authors": [
            "Binzhu Sha",
            "Xu Li",
            "Zhiyong Wu",
            "Ying Shan",
            "Helen Meng"
        ],
        "abstract": "Any-to-any singing voice conversion is confronted with a significant\nchallenge of ``timbre leakage'' issue caused by inadequate disentanglement\nbetween the content and the speaker timbre. To address this issue, this study\nintroduces a novel neural concatenative singing voice conversion (NeuCoSVC)\nframework. The NeuCoSVC framework comprises a self-supervised learning (SSL)\nrepresentation extractor, a neural harmonic signal generator, and a waveform\nsynthesizer. Specifically, the SSL extractor condenses the audio into a\nsequence of fixed-dimensional SSL features. The harmonic signal generator\nproduces both raw and filtered harmonic signals as the pitch information by\nleveraging a linear time-varying (LTV) filter. Finally, the audio generator\nreconstructs the audio waveform based on the SSL features, as well as the\nharmonic signals and the loudness information. During inference, the system\nperforms voice conversion by substituting source SSL features with their\nnearest counterparts from a matching pool, which comprises SSL representations\nextracted from the target audio, while the raw harmonic signals and the\nloudness are extracted from the source audio and are kept unchanged. Since the\nutilized SSL features in the conversion stage are directly from the target\naudio, the proposed framework has great potential to address the ``timbre\nleakage'' issue caused by previous disentanglement-based approaches.\nExperimental results confirm that the proposed system delivers much better\nperformance than the speaker embedding approach (disentanglement-based) in the\ncontext of one-shot SVC across intra-language, cross-language, and cross-domain\nevaluations.",
        "date": "2023-12-08T09:35:08+00:00",
        "link": "http://arxiv.org/pdf/2312.04919v1"
    },
    {
        "title": "Sound Source Localization for a Source inside a Structure using Ac-CycleGAN",
        "authors": [
            "Shunsuke Kita",
            "Choong Sik Park",
            "Yoshinobu Kajikawa"
        ],
        "abstract": "We propose a method for sound source localization (SSL) for a source inside a\nstructure using Ac-CycleGAN under unpaired data conditions. The proposed method\nutilizes a large amount of simulated data and a small amount of actual\nexperimental data to locate a sound source inside a structure in a real\nenvironment. An Ac-CycleGAN generator contributes to the transformation of\nsimulated data into real data, or vice versa, using unpaired data from both\ndomains. The discriminator of an Ac-CycleGAN model is designed to differentiate\nbetween the transformed data generated by the generator and real data, while\nalso predicting the location of the sound source. Vectors representing the\nfrequency spectrum of the accelerometers (FSAs) measured at three points\noutside the structure are used as input data and the source areas inside the\nstructure are used as labels. The input data vectors are concatenated\nvertically to form an image. Labels are defined by dividing the interior of the\nstructure into eight areas with one-hot encoding for each area. Thus, the SSL\nproblem is redefined as an image-classification problem to stochastically\nestimate the location of the sound source. We show that it is possible to\nestimate the sound source location using the Ac-CycleGAN discriminator for\nunpaired data across domains. Furthermore, we analyze the discriminative\nfactors for distinguishing the data. The proposed model exhibited an accuracy\nexceeding 90\\% when trained on 80\\% of actual data (12.5\\% of simulated data).\nDespite potential imperfections in the domain transformation process carried\nout by the Ac-CycleGAN generator, the discriminator can effectively distinguish\nbetween transferred and real data by selectively utilizing only those features\nthat generate a relatively small transformation error.",
        "date": "2023-12-08T05:50:07+00:00",
        "link": "http://arxiv.org/pdf/2312.04846v1"
    }
]